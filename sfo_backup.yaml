apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-01-19T21:27:59Z"
    generateName: anythingllm-647f966459-
    generation: 1
    labels:
      app.kubernetes.io/component: ai-assistant
      app.kubernetes.io/instance: anythingllm
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: anythingllm
      app.kubernetes.io/part-of: weown-mvp
      app.kubernetes.io/version: 1.9.0
      helm.sh/chart: anythingllm-2.0.2
      pod-template-hash: 647f966459
    name: anythingllm-647f966459-7fzv5
    namespace: anything-llm
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: anythingllm-647f966459
      uid: d4083395-0d02-4215-8dcb-167598248593
    resourceVersion: "10206333"
    uid: 3986c91d-a1cc-4d83-95dc-6129e0544a2d
  spec:
    automountServiceAccountToken: false
    containers:
    - env:
      - name: DISABLE_TELEMETRY
        value: "true"
      - name: EMBEDDING_ENGINE
        value: native
      - name: LLM_PROVIDER
        value: openai
      - name: SERVER_PORT
        value: "3001"
      - name: STORAGE_DIR
        value: /app/server/storage
      - name: VECTOR_DB
        value: lancedb
      envFrom:
      - secretRef:
          name: anythingllm-secrets
      image: mintplexlabs/anythingllm:1.9.0
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /api/ping
          port: 3001
          scheme: HTTP
        initialDelaySeconds: 120
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: anythingllm
      ports:
      - containerPort: 3001
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /api/ping
          port: 3001
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /app/server/storage
        name: anythingllm-storage
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /app/.cache
        name: cache-dir
      - mountPath: /collector
        name: anythingllm-storage
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k0
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: anythingllm
    serviceAccountName: anythingllm
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: anythingllm-storage
      persistentVolumeClaim:
        claimName: anythingllm-storage
    - emptyDir: {}
      name: tmp-dir
    - emptyDir: {}
      name: cache-dir
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-19T21:28:01Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-19T21:27:59Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-19T21:29:02Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-19T21:29:02Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-19T21:27:59Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 200m
        memory: 512Mi
      containerID: containerd://f184ee1268f7719f6402e506867bd114f36fdb3bb8229fd6dcc26b7db26f500c
      image: docker.io/mintplexlabs/anythingllm:1.9.0
      imageID: docker.io/mintplexlabs/anythingllm@sha256:2a5f3ac234a25c54fbe6bc6a8ef37ecb1eea9539c095d23cd4231f879c505dbd
      lastState: {}
      name: anythingllm
      ready: true
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 512Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-19T21:28:01Z"
      volumeMounts:
      - mountPath: /app/server/storage
        name: anythingllm-storage
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /app/.cache
        name: cache-dir
      - mountPath: /collector
        name: anythingllm-storage
    hostIP: 10.120.0.4
    hostIPs:
    - ip: 10.120.0.4
    observedGeneration: 1
    phase: Running
    podIP: 10.108.0.28
    podIPs:
    - ip: 10.108.0.28
    qosClass: Burstable
    startTime: "2026-01-19T21:27:59Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-18T16:33:11+05:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "9402"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-12-18T11:33:14Z"
    generateName: cert-manager-84f97659c5-
    generation: 1
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.13.2
      helm.sh/chart: cert-manager-v1.13.2
      pod-template-hash: 84f97659c5
    name: cert-manager-84f97659c5-7wzt8
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-84f97659c5
      uid: 1e783221-38ae-423d-8e9c-9bc51b6f67db
    resourceVersion: "1802853"
    uid: 29e076e5-15dd-4b86-aa6b-d1f9c9731064
  spec:
    containers:
    - args:
      - --v=2
      - --cluster-resource-namespace=$(POD_NAMESPACE)
      - --leader-election-namespace=kube-system
      - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.13.2
      - --max-concurrent-challenges=60
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-controller:v1.13.2
      imagePullPolicy: IfNotPresent
      name: cert-manager-controller
      ports:
      - containerPort: 9402
        name: http-metrics
        protocol: TCP
      - containerPort: 9403
        name: http-healthz
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xt22w
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: ptoken-assets--pool-525qi3gh9-s44k1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cert-manager
    serviceAccountName: cert-manager
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-xt22w
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T11:33:16Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T11:33:14Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-24T02:01:33Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-24T02:01:33Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T11:33:14Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://0b0a57b46ca3ba9382afd53390db803503c10325e01b57ee2baf1b82d63d47ec
      image: quay.io/jetstack/cert-manager-controller:v1.13.2
      imageID: quay.io/jetstack/cert-manager-controller@sha256:9c67cf8c92d8693f9b726bec79c2a84d2cebeb217af6947355601dec4acfa966
      lastState:
        terminated:
          containerID: containerd://5a0a3357456913aa3e127db4ed732b58aa99c1eda6690bd2347e5642f3cf9be9
          exitCode: 1
          finishedAt: "2025-12-24T02:01:32Z"
          reason: Error
          startedAt: "2025-12-18T11:33:15Z"
      name: cert-manager-controller
      ready: true
      resources: {}
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-12-24T02:01:33Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xt22w
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.3
    hostIPs:
    - ip: 10.120.0.3
    observedGeneration: 1
    phase: Running
    podIP: 10.108.0.201
    podIPs:
    - ip: 10.108.0.201
    qosClass: BestEffort
    startTime: "2025-12-18T11:33:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-18T16:33:13+05:00"
    creationTimestamp: "2025-12-18T11:33:16Z"
    generateName: cert-manager-cainjector-8499fb697f-
    generation: 1
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.13.2
      helm.sh/chart: cert-manager-v1.13.2
      pod-template-hash: 8499fb697f
    name: cert-manager-cainjector-8499fb697f-h6pvm
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-cainjector-8499fb697f
      uid: ada12946-dc49-4fdf-baa5-010463d6df62
    resourceVersion: "1802854"
    uid: b4dc5e85-2195-4d0c-ae00-786a95a90900
  spec:
    containers:
    - args:
      - --v=2
      - --leader-election-namespace=kube-system
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-cainjector:v1.13.2
      imagePullPolicy: IfNotPresent
      name: cert-manager-cainjector
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5d96g
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: ptoken-assets--pool-525qi3gh9-s44k1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cert-manager-cainjector
    serviceAccountName: cert-manager-cainjector
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-5d96g
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T11:33:20Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T11:33:16Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-24T02:01:33Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-24T02:01:33Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T11:33:16Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://90108a012cfa55dfda31f6497198317e5f234c624fbd89dab9b76c7bb5c19472
      image: quay.io/jetstack/cert-manager-cainjector:v1.13.2
      imageID: quay.io/jetstack/cert-manager-cainjector@sha256:858fee0c4af069d0e87c08fd0943f0091434e05f945d222875fc1f3d36c41616
      lastState:
        terminated:
          containerID: containerd://bcca3eb0d50d250f22a45c91ddc698cc2fcf5782c474331b83718c06787e09fd
          exitCode: 1
          finishedAt: "2025-12-24T02:01:32Z"
          reason: Error
          startedAt: "2025-12-18T11:33:19Z"
      name: cert-manager-cainjector
      ready: true
      resources: {}
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-12-24T02:01:33Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5d96g
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.3
    hostIPs:
    - ip: 10.120.0.3
    observedGeneration: 1
    phase: Running
    podIP: 10.108.0.198
    podIPs:
    - ip: 10.108.0.198
    qosClass: BestEffort
    startTime: "2025-12-18T11:33:16Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-18T16:33:15+05:00"
    creationTimestamp: "2025-12-18T11:33:18Z"
    generateName: cert-manager-webhook-59ff465c5c-
    generation: 1
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.13.2
      helm.sh/chart: cert-manager-v1.13.2
      pod-template-hash: 59ff465c5c
    name: cert-manager-webhook-59ff465c5c-9jxpd
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-webhook-59ff465c5c
      uid: 6474f8b3-243c-427f-9006-da2bcf5eb229
    resourceVersion: "47017"
    uid: 43f534aa-edc3-46e7-8e66-0bcf041c98f8
  spec:
    containers:
    - args:
      - --v=2
      - --secure-port=10250
      - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
      - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
      - --dynamic-serving-dns-names=cert-manager-webhook
      - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
      - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-webhook:v1.13.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: 6080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: cert-manager-webhook
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      - containerPort: 6080
        name: healthcheck
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 6080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nmkxt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: ptoken-assets--pool-525qi3gh9-s44k1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cert-manager-webhook
    serviceAccountName: cert-manager-webhook
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-nmkxt
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T11:33:22Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T11:33:18Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T11:33:28Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T11:33:28Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T11:33:18Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ec667f93bb412530be0b3550af3a241e9e78108796d754fa86699beeda6529d5
      image: quay.io/jetstack/cert-manager-webhook:v1.13.2
      imageID: quay.io/jetstack/cert-manager-webhook@sha256:0a9470447ebf1d3ff1c172e19268be12dc26125ff83320d456f6826c677c0ed2
      lastState: {}
      name: cert-manager-webhook
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T11:33:21Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nmkxt
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.3
    hostIPs:
    - ip: 10.120.0.3
    observedGeneration: 1
    phase: Running
    podIP: 10.108.0.229
    podIPs:
    - ip: 10.108.0.229
    qosClass: BestEffort
    startTime: "2025-12-18T11:33:18Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/port: "10254"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-12-18T10:51:35Z"
    generateName: ingress-nginx-controller-8878d8bfd-
    generation: 1
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.1
      helm.sh/chart: ingress-nginx-4.14.1
      pod-template-hash: 8878d8bfd
    name: ingress-nginx-controller-8878d8bfd-cdnwr
    namespace: ingress-nginx
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: ingress-nginx-controller-8878d8bfd
      uid: dfecd7b7-a3a6-4fed-8b2b-42373885e6fa
    resourceVersion: "36547"
    uid: 7e87f884-bae2-48d9-9d7b-88dd22555a75
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - /nginx-ingress-controller
      - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
      - --election-id=ingress-nginx-leader
      - --controller-class=k8s.io/ingress-nginx
      - --ingress-class=nginx
      - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
      - --validating-webhook=:8443
      - --validating-webhook-certificate=/usr/local/certificates/cert
      - --validating-webhook-key=/usr/local/certificates/key
      - --enable-metrics=true
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: LD_PRELOAD
        value: /usr/local/lib/libmimalloc.so
      image: registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /wait-shutdown
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: controller
      ports:
      - containerPort: 80
        name: http
        protocol: TCP
      - containerPort: 443
        name: https
        protocol: TCP
      - containerPort: 10254
        name: metrics
        protocol: TCP
      - containerPort: 8443
        name: webhook
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 90Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsGroup: 82
        runAsNonRoot: true
        runAsUser: 101
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /usr/local/certificates/
        name: webhook-cert
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7dt26
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k0
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: ingress-nginx
    serviceAccountName: ingress-nginx
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: webhook-cert
      secret:
        defaultMode: 420
        secretName: ingress-nginx-admission
    - name: kube-api-access-7dt26
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T10:51:44Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T10:51:35Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T10:51:56Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T10:51:56Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T10:51:35Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 90Mi
      containerID: containerd://c16a9fa47f377a4d7cfd85553e70bce1ccdb6e32da94815195d6f507738cefec
      image: sha256:8043403e50094a07ba382a116497ecfb317f3196e9c8063c89be209f4f654810
      imageID: registry.k8s.io/ingress-nginx/controller@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47
      lastState: {}
      name: controller
      ready: true
      resources:
        requests:
          cpu: 100m
          memory: 90Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T10:51:43Z"
      volumeMounts:
      - mountPath: /usr/local/certificates/
        name: webhook-cert
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7dt26
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.4
    hostIPs:
    - ip: 10.120.0.4
    observedGeneration: 1
    phase: Running
    podIP: 10.108.0.84
    podIPs:
    - ip: 10.108.0.84
    qosClass: Burstable
    startTime: "2025-12-18T10:51:35Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: privileged-containers,non-root-user,resource-requirements,hostpath-volume
      container.apparmor.security.beta.kubernetes.io/apply-sysctl-overwrites: unconfined
      container.apparmor.security.beta.kubernetes.io/cilium-agent: unconfined
      container.apparmor.security.beta.kubernetes.io/clean-cilium-state: unconfined
      container.apparmor.security.beta.kubernetes.io/mount-cgroup: unconfined
      kubectl.kubernetes.io/default-container: cilium-agent
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-12-18T07:55:04Z"
    generateName: cilium-
    generation: 1
    labels:
      app.kubernetes.io/name: cilium-agent
      app.kubernetes.io/part-of: cilium
      controller-revision-hash: 55ccfcf75c
      doks.digitalocean.com/managed: "true"
      k8s-app: cilium
      kubernetes.io/cluster-service: "true"
      pod-template-generation: "1"
    name: cilium-6vmps
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: cilium
      uid: b4e0f534-88c0-4aca-aeef-4ef4b29adb49
    resourceVersion: "1143"
    uid: da36d717-11b4-493a-bbc3-4a16b5a99b68
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ptoken-assets--pool-525qi3gh9-s44k1
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              k8s-app: cilium
          topologyKey: kubernetes.io/hostname
    automountServiceAccountToken: true
    containers:
    - args:
      - --config-dir=/tmp/cilium/config-map
      - --k8s-api-server=https://ca54c9b1-1f4a-498d-9a4d-af31964c74a9.k8s.ondigitalocean.com
      - --ipv4-native-routing-cidr=10.108.0.0/16
      command:
      - cilium-agent
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CILIUM_CLUSTERMESH_CONFIG
        value: /var/lib/cilium/clustermesh/
      - name: GOMEMLIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.memory
      - name: KUBE_CLIENT_BACKOFF_BASE
        value: "1"
      - name: KUBE_CLIENT_BACKOFF_DURATION
        value: "120"
      - name: KUBERNETES_SERVICE_HOST
        value: ca54c9b1-1f4a-498d-9a4d-af31964c74a9.k8s.ondigitalocean.com
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imagePullPolicy: IfNotPresent
      lifecycle:
        postStart:
          exec:
            command:
            - bash
            - -c
            - |
              set -o errexit
              set -o pipefail
              set -o nounset

              # When running in AWS ENI mode, it's likely that 'aws-node' has
              # had a chance to install SNAT iptables rules. These can result
              # in dropped traffic, so we should attempt to remove them.
              # We do it using a 'postStart' hook since this may need to run
              # for nodes which might have already been init'ed but may still
              # have dangling rules. This is safe because there are no
              # dependencies on anything that is part of the startup script
              # itself, and can be safely run multiple times per node (e.g. in
              # case of a restart).
              if [[ "$(iptables-save | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')" != "0" ]];
              then
                  echo 'Deleting iptables rules created by the AWS CNI VPC plugin'
                  iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN' | iptables-restore
              fi
              echo 'Done!'
        preStop:
          exec:
            command:
            - /cni-uninstall.sh
      livenessProbe:
        failureThreshold: 10
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          - name: require-k8s-connectivity
            value: "false"
          path: /healthz
          port: 9879
          scheme: HTTP
        initialDelaySeconds: 120
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      name: cilium-agent
      ports:
      - containerPort: 4244
        hostPort: 4244
        name: peer-service
        protocol: TCP
      - containerPort: 9090
        hostPort: 9090
        name: prometheus
        protocol: TCP
      - containerPort: 9964
        hostPort: 9964
        name: envoy-metrics
        protocol: TCP
      - containerPort: 9965
        hostPort: 9965
        name: hubble-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        requests:
          cpu: 300m
          memory: 300Mi
      securityContext:
        capabilities:
          add:
          - CHOWN
          - KILL
          - NET_ADMIN
          - NET_RAW
          - IPC_LOCK
          - SYS_MODULE
          - SYS_ADMIN
          - SYS_RESOURCE
          - DAC_OVERRIDE
          - FOWNER
          - SETGID
          - SETUID
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      startupProbe:
        failureThreshold: 300
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /host/proc/sys/net
        name: host-proc-sys-net
      - mountPath: /host/proc/sys/kernel
        name: host-proc-sys-kernel
      - mountPath: /sys/fs/bpf
        mountPropagation: HostToContainer
        name: bpf-maps
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /var/run/cilium/netns
        mountPropagation: HostToContainer
        name: cilium-netns
      - mountPath: /host/etc/cni/net.d
        name: etc-cni-netd
      - mountPath: /var/lib/cilium/clustermesh
        name: clustermesh-secrets
        readOnly: true
      - mountPath: /etc/config
        name: ip-masq-agent
        readOnly: true
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/lib/cilium/tls/hubble
        name: hubble-tls
        readOnly: true
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-67fwj
        readOnly: true
    - args:
      - -c
      - while true; do for IP in $( ip -4 neigh show dev eth1 | grep STALE | awk '{print
        $1}'); do if ping ${IP} -c 1 -4 -W 1 -q 2>&1 > /dev/null; then echo "$IP reachable";
        else echo "$IP unreachable"; fi; done; sleep 10; done
      command:
      - /bin/sh
      image: ghcr.io/digitalocean-packages/busybox:1.36
      imagePullPolicy: IfNotPresent
      name: arp-fix
      resources:
        limits:
          memory: 50Mi
        requests:
          cpu: 20m
          memory: 50Mi
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-67fwj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - chroot
      - /host
      - bash
      - -e
      - -c
      - |
        set -euo pipefail

        error(){
            echo "$(date +'%Y-%m-%d %H:%M:%S') ERROR: $*" >&2
        }

        log() {
            echo "$(date +'%Y-%m-%d %H:%M:%S') - $1"
        }

        wait_for_cloud_init_status() {
            local status_file="/var/lib/cloud/data/status.json"
            while true; do
                if [ ! -f "$status_file" ]; then
                    log "Waiting for cloud-init status file..."
                    sleep 2
                    continue
                fi

                local start finish
                start=$(jq -r '.v1."modules-final".start // "null" | if . != "null" then (.|floor) else "null" end' "$status_file")
                finish=$(jq -r '.v1."modules-final".finished // "null" | if . != "null" then (.|floor) else "null" end' "$status_file")

                if [ "$start" = "null" ] || [ "$finish" = "null" ]; then
                    log "Waiting for cloud-init to complete..."
                    sleep 1
                    continue
                fi

                if [ "$finish" -ge "$start" ]; then
                    log "Cloud-init has completed successfully"
                    break
                else
                    log "Waiting for cloud-init to complete..."
                    sleep 1
                fi
            done
        }

        fetch_anchor_ip() {
            local url="http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address"
            curl -s --fail "$url" || { error "Failed to fetch anchor IP"; exit 1; }
        }

        fetch_netmask() {
            local url="http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/netmask"
            curl -s --fail "$url" || { error "Failed to fetch anchor netmask"; exit 1; }
        }

        netmask_to_prefix() {
            local netmask=$1
            local IFS=.
            local -a octets=($netmask)
            local prefix=0
            for octet in "${octets[@]}"; do
                while [ $octet -gt 0 ]; do
                    prefix=$((prefix + (octet & 1)))
                    octet=$((octet >> 1))
                done
            done
            echo $prefix
        }

        calculate_network() {
            local ip=$1
            local prefix=$2
            local IFS=.
            local -a ip_octets=($ip)
            local -a netmask_octets
            local -a network_octets
            local netmask=$((0xFFFFFFFF << (32 - prefix) & 0xFFFFFFFF))
            for ((i=3; i>=0; i--)); do
                netmask_octets[$i]=$((netmask & 0xFF))
                netmask=$((netmask >> 8))
            done
            for ((i=0; i<4; i++)); do
                network_octets[$i]=$((ip_octets[i] & netmask_octets[i]))
            done
            echo "${network_octets[0]}.${network_octets[1]}.${network_octets[2]}.${network_octets[3]}/$prefix"
        }

        remove_ip_from_eth0() {
            local ip=$1
            local prefix=$2
            local eth0_links=$(ip addr show eth0)
            if grep -q "$ip" <<< "$eth0_links"; then
                ip addr del "$ip/$prefix" dev eth0 || { error "Failed to remove IP $ip from eth0"; exit 1; }
                log "Removed IP $ip from eth0"
            else
                log "IP $ip not found on eth0"
            fi
        }

        configure_anchor_interface() {
            local ip=$1
            local prefix=$2
            if ! ip link show anchor &>/dev/null; then
                ip link add anchor type dummy || { error "Failed to create anchor interface"; exit 1; }
                log "Created anchor interface"
            else
                log "anchor interface already exists"
            fi

            local anchor_links=$(ip addr show anchor)
            if ! grep -q "$ip" <<< "$anchor_links"; then
                ip addr add "$ip/$prefix" dev anchor || { error "Failed to assign IP $ip/$prefix to anchor interface"; exit 1; }
                log "Assigned IP $ip to anchor interface"
            else
                log "IP $ip/$prefix already assigned to anchor interface"
            fi

            ip link set anchor up || { log "Failed to set anchor interface up"; exit 1; }
            log "Set anchor interface up"
        }

        configure_anchor_route() {
            local network=$1
            ip route replace "$network" dev eth0
            log "Configure anchor $network eth0 route"
        }

        main() {
            log "Starting anchor fix script"

            wait_for_cloud_init_status

            anchor_ip=$(fetch_anchor_ip)
            log "Fetched anchor IP: $anchor_ip"

            netmask=$(fetch_netmask)
            log "Fetched netmask: $netmask"

            ip_prefix=$(netmask_to_prefix "$netmask")
            log "Calculated IP prefix: $ip_prefix"

            network=$(calculate_network "$anchor_ip" "$ip_prefix")
            log "Calculated network: $network"

            remove_ip_from_eth0 "$anchor_ip" "$ip_prefix"
            configure_anchor_interface "$anchor_ip" "$ip_prefix"

            configure_anchor_route "$network"

            log "Init script completed successfully"
        }
        main "$@"
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imagePullPolicy: IfNotPresent
      name: anchor-ip-fix
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-67fwj
        readOnly: true
    - command:
      - cilium-dbg
      - build-config
      - --source=config-map:cilium-config
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: KUBERNETES_SERVICE_HOST
        value: ca54c9b1-1f4a-498d-9a4d-af31964c74a9.k8s.ondigitalocean.com
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imagePullPolicy: IfNotPresent
      name: config
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-67fwj
        readOnly: true
    - command:
      - sh
      - -ec
      - |
        cp /usr/bin/cilium-mount /hostbin/cilium-mount;
        nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-mount" $CGROUP_ROOT;
        rm /hostbin/cilium-mount
      env:
      - name: CGROUP_ROOT
        value: /run/cilium/cgroupv2
      - name: BIN_PATH
        value: /opt/cni/bin
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imagePullPolicy: IfNotPresent
      name: mount-cgroup
      resources: {}
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
          - SYS_CHROOT
          - SYS_PTRACE
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /hostproc
        name: hostproc
      - mountPath: /hostbin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-67fwj
        readOnly: true
    - command:
      - sh
      - -ec
      - |
        cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;
        nsenter --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-sysctlfix";
        rm /hostbin/cilium-sysctlfix
      env:
      - name: BIN_PATH
        value: /opt/cni/bin
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imagePullPolicy: IfNotPresent
      name: apply-sysctl-overwrites
      resources: {}
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
          - SYS_CHROOT
          - SYS_PTRACE
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /hostproc
        name: hostproc
      - mountPath: /hostbin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-67fwj
        readOnly: true
    - args:
      - mount | grep "/sys/fs/bpf type bpf" || mount -t bpf bpf /sys/fs/bpf
      command:
      - /bin/bash
      - -c
      - --
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imagePullPolicy: IfNotPresent
      name: mount-bpf-fs
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /sys/fs/bpf
        mountPropagation: Bidirectional
        name: bpf-maps
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-67fwj
        readOnly: true
    - command:
      - /init-container.sh
      env:
      - name: CILIUM_ALL_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-state
            name: cilium-config
            optional: true
      - name: CILIUM_BPF_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-bpf-state
            name: cilium-config
            optional: true
      - name: WRITE_CNI_CONF_WHEN_READY
        valueFrom:
          configMapKeyRef:
            key: write-cni-conf-when-ready
            name: cilium-config
            optional: true
      - name: KUBERNETES_SERVICE_HOST
        value: ca54c9b1-1f4a-498d-9a4d-af31964c74a9.k8s.ondigitalocean.com
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imagePullPolicy: IfNotPresent
      name: clean-cilium-state
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - SYS_MODULE
          - SYS_ADMIN
          - SYS_RESOURCE
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /sys/fs/bpf
        name: bpf-maps
      - mountPath: /run/cilium/cgroupv2
        mountPropagation: HostToContainer
        name: cilium-cgroup
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-67fwj
        readOnly: true
    - command:
      - /install-plugin.sh
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imagePullPolicy: IfNotPresent
      name: install-cni-binaries
      resources:
        requests:
          cpu: 100m
          memory: 10Mi
      securityContext:
        capabilities:
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-67fwj
        readOnly: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      appArmorProfile:
        type: Unconfined
      seccompProfile:
        type: Unconfined
    serviceAccount: cilium
    serviceAccountName: cilium
    terminationGracePeriodSeconds: 1
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host
    - emptyDir: {}
      name: tmp
    - hostPath:
        path: /var/run/cilium
        type: DirectoryOrCreate
      name: cilium-run
    - hostPath:
        path: /var/run/netns
        type: DirectoryOrCreate
      name: cilium-netns
    - hostPath:
        path: /sys/fs/bpf
        type: DirectoryOrCreate
      name: bpf-maps
    - hostPath:
        path: /proc
        type: Directory
      name: hostproc
    - hostPath:
        path: /run/cilium/cgroupv2
        type: DirectoryOrCreate
      name: cilium-cgroup
    - hostPath:
        path: /opt/cni/bin
        type: DirectoryOrCreate
      name: cni-path
    - hostPath:
        path: /etc/cni/net.d
        type: DirectoryOrCreate
      name: etc-cni-netd
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: clustermesh-secrets
      projected:
        defaultMode: 256
        sources:
        - secret:
            name: cilium-clustermesh
            optional: true
        - secret:
            items:
            - key: tls.key
              path: common-etcd-client.key
            - key: tls.crt
              path: common-etcd-client.crt
            - key: ca.crt
              path: common-etcd-client-ca.crt
            name: clustermesh-apiserver-remote-cert
            optional: true
        - secret:
            items:
            - key: tls.key
              path: local-etcd-client.key
            - key: tls.crt
              path: local-etcd-client.crt
            - key: ca.crt
              path: local-etcd-client-ca.crt
            name: clustermesh-apiserver-local-cert
            optional: true
    - configMap:
        defaultMode: 420
        items:
        - key: config
          path: ip-masq-agent
        name: ip-masq-agent
        optional: true
      name: ip-masq-agent
    - hostPath:
        path: /proc/sys/net
        type: Directory
      name: host-proc-sys-net
    - hostPath:
        path: /proc/sys/kernel
        type: Directory
      name: host-proc-sys-kernel
    - name: hubble-tls
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: tls.crt
              path: server.crt
            - key: tls.key
              path: server.key
            - key: ca.crt
              path: client-ca.crt
            name: hubble-server-certs
            optional: true
    - name: kube-api-access-67fwj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:22Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:29Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:35Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:35Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:04Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 20m
        memory: 50Mi
      containerID: containerd://9b7d052bf4fefbc6ff257d926c6aa36a603b465b3d13363a55c8e61420df749a
      image: ghcr.io/digitalocean-packages/busybox:1.36
      imageID: ghcr.io/digitalocean-packages/busybox@sha256:907ca53d7e2947e849b839b1cd258c98fd3916c60f2e6e70c30edbf741ab6754
      lastState: {}
      name: arp-fix
      ready: true
      resources:
        limits:
          memory: 50Mi
        requests:
          cpu: 20m
          memory: 50Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T07:55:31Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-67fwj
        readOnly: true
        recursiveReadOnly: Disabled
    - allocatedResources:
        cpu: 300m
        memory: 300Mi
      containerID: containerd://73866c7f4897c285ca54f030603f42dd4bc6f99154a21a7f773c44fc3ad03d62
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imageID: ghcr.io/digitalocean-packages/cilium@sha256:70f0c668c27a8efa64ecc923ae40472db101ab3cbd88e048f8070339f1b601cf
      lastState: {}
      name: cilium-agent
      ready: true
      resources:
        requests:
          cpu: 300m
          memory: 300Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T07:55:29Z"
      volumeMounts:
      - mountPath: /host/proc/sys/net
        name: host-proc-sys-net
      - mountPath: /host/proc/sys/kernel
        name: host-proc-sys-kernel
      - mountPath: /sys/fs/bpf
        name: bpf-maps
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /var/run/cilium/netns
        name: cilium-netns
      - mountPath: /host/etc/cni/net.d
        name: etc-cni-netd
      - mountPath: /var/lib/cilium/clustermesh
        name: clustermesh-secrets
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/config
        name: ip-masq-agent
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/lib/cilium/tls/hubble
        name: hubble-tls
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-67fwj
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.3
    hostIPs:
    - ip: 10.120.0.3
    initContainerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 100Mi
      containerID: containerd://55de735da4374d93eb428b4c0900ed389f347b8e12251c9815cba8c6e3b2c84e
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imageID: ghcr.io/digitalocean-packages/cilium@sha256:70f0c668c27a8efa64ecc923ae40472db101ab3cbd88e048f8070339f1b601cf
      lastState: {}
      name: anchor-ip-fix
      ready: true
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://55de735da4374d93eb428b4c0900ed389f347b8e12251c9815cba8c6e3b2c84e
          exitCode: 0
          finishedAt: "2025-12-18T07:55:22Z"
          reason: Completed
          startedAt: "2025-12-18T07:55:21Z"
      volumeMounts:
      - mountPath: /host
        name: host
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-67fwj
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://27b16898f6733c2c6269e54affb8aa6b5ee9cf5ad745bd15c88c534a8e3fbae8
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imageID: ghcr.io/digitalocean-packages/cilium@sha256:70f0c668c27a8efa64ecc923ae40472db101ab3cbd88e048f8070339f1b601cf
      lastState: {}
      name: config
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://27b16898f6733c2c6269e54affb8aa6b5ee9cf5ad745bd15c88c534a8e3fbae8
          exitCode: 0
          finishedAt: "2025-12-18T07:55:23Z"
          reason: Completed
          startedAt: "2025-12-18T07:55:23Z"
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-67fwj
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://6f807e6f2d8c27ec3fe04a079db2d58a2bf78c0ad10e24c0a75f3b0824888c38
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imageID: ghcr.io/digitalocean-packages/cilium@sha256:70f0c668c27a8efa64ecc923ae40472db101ab3cbd88e048f8070339f1b601cf
      lastState: {}
      name: mount-cgroup
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://6f807e6f2d8c27ec3fe04a079db2d58a2bf78c0ad10e24c0a75f3b0824888c38
          exitCode: 0
          finishedAt: "2025-12-18T07:55:24Z"
          reason: Completed
          startedAt: "2025-12-18T07:55:24Z"
      volumeMounts:
      - mountPath: /hostproc
        name: hostproc
      - mountPath: /hostbin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-67fwj
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://035b2f6ea67ed915f6382cf1c8997b6878fc4fabda54c923a189b55493ab3287
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imageID: ghcr.io/digitalocean-packages/cilium@sha256:70f0c668c27a8efa64ecc923ae40472db101ab3cbd88e048f8070339f1b601cf
      lastState: {}
      name: apply-sysctl-overwrites
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://035b2f6ea67ed915f6382cf1c8997b6878fc4fabda54c923a189b55493ab3287
          exitCode: 0
          finishedAt: "2025-12-18T07:55:25Z"
          reason: Completed
          startedAt: "2025-12-18T07:55:25Z"
      volumeMounts:
      - mountPath: /hostproc
        name: hostproc
      - mountPath: /hostbin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-67fwj
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://990492bcb58f8a917eaa0301b6c6b9ce57d0efa5a21bfb976df00d1ad189c362
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imageID: ghcr.io/digitalocean-packages/cilium@sha256:70f0c668c27a8efa64ecc923ae40472db101ab3cbd88e048f8070339f1b601cf
      lastState: {}
      name: mount-bpf-fs
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://990492bcb58f8a917eaa0301b6c6b9ce57d0efa5a21bfb976df00d1ad189c362
          exitCode: 0
          finishedAt: "2025-12-18T07:55:26Z"
          reason: Completed
          startedAt: "2025-12-18T07:55:26Z"
      volumeMounts:
      - mountPath: /sys/fs/bpf
        name: bpf-maps
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-67fwj
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://2ea1b1d7bac70831367be118b9c2514919dc17fe938376cce129d5d5774cc6d9
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imageID: ghcr.io/digitalocean-packages/cilium@sha256:70f0c668c27a8efa64ecc923ae40472db101ab3cbd88e048f8070339f1b601cf
      lastState: {}
      name: clean-cilium-state
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://2ea1b1d7bac70831367be118b9c2514919dc17fe938376cce129d5d5774cc6d9
          exitCode: 0
          finishedAt: "2025-12-18T07:55:27Z"
          reason: Completed
          startedAt: "2025-12-18T07:55:27Z"
      volumeMounts:
      - mountPath: /sys/fs/bpf
        name: bpf-maps
      - mountPath: /run/cilium/cgroupv2
        name: cilium-cgroup
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-67fwj
        readOnly: true
        recursiveReadOnly: Disabled
    - allocatedResources:
        cpu: 100m
        memory: 10Mi
      containerID: containerd://d9ac088a99074c7d0a598abc3e69e6377775a9da20b81a8bc4e84dd945d0221d
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imageID: ghcr.io/digitalocean-packages/cilium@sha256:70f0c668c27a8efa64ecc923ae40472db101ab3cbd88e048f8070339f1b601cf
      lastState: {}
      name: install-cni-binaries
      ready: true
      resources:
        requests:
          cpu: 100m
          memory: 10Mi
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://d9ac088a99074c7d0a598abc3e69e6377775a9da20b81a8bc4e84dd945d0221d
          exitCode: 0
          finishedAt: "2025-12-18T07:55:28Z"
          reason: Completed
          startedAt: "2025-12-18T07:55:28Z"
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-67fwj
        readOnly: true
        recursiveReadOnly: Disabled
    observedGeneration: 1
    phase: Running
    podIP: 10.120.0.3
    podIPs:
    - ip: 10.120.0.3
    qosClass: Burstable
    startTime: "2025-12-18T07:55:05Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: privileged-containers,non-root-user,resource-requirements,hostpath-volume
      container.apparmor.security.beta.kubernetes.io/apply-sysctl-overwrites: unconfined
      container.apparmor.security.beta.kubernetes.io/cilium-agent: unconfined
      container.apparmor.security.beta.kubernetes.io/clean-cilium-state: unconfined
      container.apparmor.security.beta.kubernetes.io/mount-cgroup: unconfined
      kubectl.kubernetes.io/default-container: cilium-agent
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-12-18T07:55:04Z"
    generateName: cilium-
    generation: 1
    labels:
      app.kubernetes.io/name: cilium-agent
      app.kubernetes.io/part-of: cilium
      controller-revision-hash: 55ccfcf75c
      doks.digitalocean.com/managed: "true"
      k8s-app: cilium
      kubernetes.io/cluster-service: "true"
      pod-template-generation: "1"
    name: cilium-hzt8h
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: cilium
      uid: b4e0f534-88c0-4aca-aeef-4ef4b29adb49
    resourceVersion: "1115"
    uid: e85e29f0-0293-4280-bc53-d58be784f81f
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ptoken-assets--pool-525qi3gh9-s44k0
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              k8s-app: cilium
          topologyKey: kubernetes.io/hostname
    automountServiceAccountToken: true
    containers:
    - args:
      - --config-dir=/tmp/cilium/config-map
      - --k8s-api-server=https://ca54c9b1-1f4a-498d-9a4d-af31964c74a9.k8s.ondigitalocean.com
      - --ipv4-native-routing-cidr=10.108.0.0/16
      command:
      - cilium-agent
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CILIUM_CLUSTERMESH_CONFIG
        value: /var/lib/cilium/clustermesh/
      - name: GOMEMLIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.memory
      - name: KUBE_CLIENT_BACKOFF_BASE
        value: "1"
      - name: KUBE_CLIENT_BACKOFF_DURATION
        value: "120"
      - name: KUBERNETES_SERVICE_HOST
        value: ca54c9b1-1f4a-498d-9a4d-af31964c74a9.k8s.ondigitalocean.com
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imagePullPolicy: IfNotPresent
      lifecycle:
        postStart:
          exec:
            command:
            - bash
            - -c
            - |
              set -o errexit
              set -o pipefail
              set -o nounset

              # When running in AWS ENI mode, it's likely that 'aws-node' has
              # had a chance to install SNAT iptables rules. These can result
              # in dropped traffic, so we should attempt to remove them.
              # We do it using a 'postStart' hook since this may need to run
              # for nodes which might have already been init'ed but may still
              # have dangling rules. This is safe because there are no
              # dependencies on anything that is part of the startup script
              # itself, and can be safely run multiple times per node (e.g. in
              # case of a restart).
              if [[ "$(iptables-save | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')" != "0" ]];
              then
                  echo 'Deleting iptables rules created by the AWS CNI VPC plugin'
                  iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN' | iptables-restore
              fi
              echo 'Done!'
        preStop:
          exec:
            command:
            - /cni-uninstall.sh
      livenessProbe:
        failureThreshold: 10
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          - name: require-k8s-connectivity
            value: "false"
          path: /healthz
          port: 9879
          scheme: HTTP
        initialDelaySeconds: 120
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      name: cilium-agent
      ports:
      - containerPort: 4244
        hostPort: 4244
        name: peer-service
        protocol: TCP
      - containerPort: 9090
        hostPort: 9090
        name: prometheus
        protocol: TCP
      - containerPort: 9964
        hostPort: 9964
        name: envoy-metrics
        protocol: TCP
      - containerPort: 9965
        hostPort: 9965
        name: hubble-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        requests:
          cpu: 300m
          memory: 300Mi
      securityContext:
        capabilities:
          add:
          - CHOWN
          - KILL
          - NET_ADMIN
          - NET_RAW
          - IPC_LOCK
          - SYS_MODULE
          - SYS_ADMIN
          - SYS_RESOURCE
          - DAC_OVERRIDE
          - FOWNER
          - SETGID
          - SETUID
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      startupProbe:
        failureThreshold: 300
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /host/proc/sys/net
        name: host-proc-sys-net
      - mountPath: /host/proc/sys/kernel
        name: host-proc-sys-kernel
      - mountPath: /sys/fs/bpf
        mountPropagation: HostToContainer
        name: bpf-maps
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /var/run/cilium/netns
        mountPropagation: HostToContainer
        name: cilium-netns
      - mountPath: /host/etc/cni/net.d
        name: etc-cni-netd
      - mountPath: /var/lib/cilium/clustermesh
        name: clustermesh-secrets
        readOnly: true
      - mountPath: /etc/config
        name: ip-masq-agent
        readOnly: true
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/lib/cilium/tls/hubble
        name: hubble-tls
        readOnly: true
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8687
        readOnly: true
    - args:
      - -c
      - while true; do for IP in $( ip -4 neigh show dev eth1 | grep STALE | awk '{print
        $1}'); do if ping ${IP} -c 1 -4 -W 1 -q 2>&1 > /dev/null; then echo "$IP reachable";
        else echo "$IP unreachable"; fi; done; sleep 10; done
      command:
      - /bin/sh
      image: ghcr.io/digitalocean-packages/busybox:1.36
      imagePullPolicy: IfNotPresent
      name: arp-fix
      resources:
        limits:
          memory: 50Mi
        requests:
          cpu: 20m
          memory: 50Mi
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8687
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - chroot
      - /host
      - bash
      - -e
      - -c
      - |
        set -euo pipefail

        error(){
            echo "$(date +'%Y-%m-%d %H:%M:%S') ERROR: $*" >&2
        }

        log() {
            echo "$(date +'%Y-%m-%d %H:%M:%S') - $1"
        }

        wait_for_cloud_init_status() {
            local status_file="/var/lib/cloud/data/status.json"
            while true; do
                if [ ! -f "$status_file" ]; then
                    log "Waiting for cloud-init status file..."
                    sleep 2
                    continue
                fi

                local start finish
                start=$(jq -r '.v1."modules-final".start // "null" | if . != "null" then (.|floor) else "null" end' "$status_file")
                finish=$(jq -r '.v1."modules-final".finished // "null" | if . != "null" then (.|floor) else "null" end' "$status_file")

                if [ "$start" = "null" ] || [ "$finish" = "null" ]; then
                    log "Waiting for cloud-init to complete..."
                    sleep 1
                    continue
                fi

                if [ "$finish" -ge "$start" ]; then
                    log "Cloud-init has completed successfully"
                    break
                else
                    log "Waiting for cloud-init to complete..."
                    sleep 1
                fi
            done
        }

        fetch_anchor_ip() {
            local url="http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address"
            curl -s --fail "$url" || { error "Failed to fetch anchor IP"; exit 1; }
        }

        fetch_netmask() {
            local url="http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/netmask"
            curl -s --fail "$url" || { error "Failed to fetch anchor netmask"; exit 1; }
        }

        netmask_to_prefix() {
            local netmask=$1
            local IFS=.
            local -a octets=($netmask)
            local prefix=0
            for octet in "${octets[@]}"; do
                while [ $octet -gt 0 ]; do
                    prefix=$((prefix + (octet & 1)))
                    octet=$((octet >> 1))
                done
            done
            echo $prefix
        }

        calculate_network() {
            local ip=$1
            local prefix=$2
            local IFS=.
            local -a ip_octets=($ip)
            local -a netmask_octets
            local -a network_octets
            local netmask=$((0xFFFFFFFF << (32 - prefix) & 0xFFFFFFFF))
            for ((i=3; i>=0; i--)); do
                netmask_octets[$i]=$((netmask & 0xFF))
                netmask=$((netmask >> 8))
            done
            for ((i=0; i<4; i++)); do
                network_octets[$i]=$((ip_octets[i] & netmask_octets[i]))
            done
            echo "${network_octets[0]}.${network_octets[1]}.${network_octets[2]}.${network_octets[3]}/$prefix"
        }

        remove_ip_from_eth0() {
            local ip=$1
            local prefix=$2
            local eth0_links=$(ip addr show eth0)
            if grep -q "$ip" <<< "$eth0_links"; then
                ip addr del "$ip/$prefix" dev eth0 || { error "Failed to remove IP $ip from eth0"; exit 1; }
                log "Removed IP $ip from eth0"
            else
                log "IP $ip not found on eth0"
            fi
        }

        configure_anchor_interface() {
            local ip=$1
            local prefix=$2
            if ! ip link show anchor &>/dev/null; then
                ip link add anchor type dummy || { error "Failed to create anchor interface"; exit 1; }
                log "Created anchor interface"
            else
                log "anchor interface already exists"
            fi

            local anchor_links=$(ip addr show anchor)
            if ! grep -q "$ip" <<< "$anchor_links"; then
                ip addr add "$ip/$prefix" dev anchor || { error "Failed to assign IP $ip/$prefix to anchor interface"; exit 1; }
                log "Assigned IP $ip to anchor interface"
            else
                log "IP $ip/$prefix already assigned to anchor interface"
            fi

            ip link set anchor up || { log "Failed to set anchor interface up"; exit 1; }
            log "Set anchor interface up"
        }

        configure_anchor_route() {
            local network=$1
            ip route replace "$network" dev eth0
            log "Configure anchor $network eth0 route"
        }

        main() {
            log "Starting anchor fix script"

            wait_for_cloud_init_status

            anchor_ip=$(fetch_anchor_ip)
            log "Fetched anchor IP: $anchor_ip"

            netmask=$(fetch_netmask)
            log "Fetched netmask: $netmask"

            ip_prefix=$(netmask_to_prefix "$netmask")
            log "Calculated IP prefix: $ip_prefix"

            network=$(calculate_network "$anchor_ip" "$ip_prefix")
            log "Calculated network: $network"

            remove_ip_from_eth0 "$anchor_ip" "$ip_prefix"
            configure_anchor_interface "$anchor_ip" "$ip_prefix"

            configure_anchor_route "$network"

            log "Init script completed successfully"
        }
        main "$@"
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imagePullPolicy: IfNotPresent
      name: anchor-ip-fix
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8687
        readOnly: true
    - command:
      - cilium-dbg
      - build-config
      - --source=config-map:cilium-config
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: KUBERNETES_SERVICE_HOST
        value: ca54c9b1-1f4a-498d-9a4d-af31964c74a9.k8s.ondigitalocean.com
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imagePullPolicy: IfNotPresent
      name: config
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8687
        readOnly: true
    - command:
      - sh
      - -ec
      - |
        cp /usr/bin/cilium-mount /hostbin/cilium-mount;
        nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-mount" $CGROUP_ROOT;
        rm /hostbin/cilium-mount
      env:
      - name: CGROUP_ROOT
        value: /run/cilium/cgroupv2
      - name: BIN_PATH
        value: /opt/cni/bin
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imagePullPolicy: IfNotPresent
      name: mount-cgroup
      resources: {}
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
          - SYS_CHROOT
          - SYS_PTRACE
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /hostproc
        name: hostproc
      - mountPath: /hostbin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8687
        readOnly: true
    - command:
      - sh
      - -ec
      - |
        cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;
        nsenter --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-sysctlfix";
        rm /hostbin/cilium-sysctlfix
      env:
      - name: BIN_PATH
        value: /opt/cni/bin
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imagePullPolicy: IfNotPresent
      name: apply-sysctl-overwrites
      resources: {}
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
          - SYS_CHROOT
          - SYS_PTRACE
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /hostproc
        name: hostproc
      - mountPath: /hostbin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8687
        readOnly: true
    - args:
      - mount | grep "/sys/fs/bpf type bpf" || mount -t bpf bpf /sys/fs/bpf
      command:
      - /bin/bash
      - -c
      - --
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imagePullPolicy: IfNotPresent
      name: mount-bpf-fs
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /sys/fs/bpf
        mountPropagation: Bidirectional
        name: bpf-maps
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8687
        readOnly: true
    - command:
      - /init-container.sh
      env:
      - name: CILIUM_ALL_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-state
            name: cilium-config
            optional: true
      - name: CILIUM_BPF_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-bpf-state
            name: cilium-config
            optional: true
      - name: WRITE_CNI_CONF_WHEN_READY
        valueFrom:
          configMapKeyRef:
            key: write-cni-conf-when-ready
            name: cilium-config
            optional: true
      - name: KUBERNETES_SERVICE_HOST
        value: ca54c9b1-1f4a-498d-9a4d-af31964c74a9.k8s.ondigitalocean.com
      - name: KUBERNETES_SERVICE_PORT
        value: "443"
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imagePullPolicy: IfNotPresent
      name: clean-cilium-state
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - SYS_MODULE
          - SYS_ADMIN
          - SYS_RESOURCE
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /sys/fs/bpf
        name: bpf-maps
      - mountPath: /run/cilium/cgroupv2
        mountPropagation: HostToContainer
        name: cilium-cgroup
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8687
        readOnly: true
    - command:
      - /install-plugin.sh
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imagePullPolicy: IfNotPresent
      name: install-cni-binaries
      resources:
        requests:
          cpu: 100m
          memory: 10Mi
      securityContext:
        capabilities:
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8687
        readOnly: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k0
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      appArmorProfile:
        type: Unconfined
      seccompProfile:
        type: Unconfined
    serviceAccount: cilium
    serviceAccountName: cilium
    terminationGracePeriodSeconds: 1
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host
    - emptyDir: {}
      name: tmp
    - hostPath:
        path: /var/run/cilium
        type: DirectoryOrCreate
      name: cilium-run
    - hostPath:
        path: /var/run/netns
        type: DirectoryOrCreate
      name: cilium-netns
    - hostPath:
        path: /sys/fs/bpf
        type: DirectoryOrCreate
      name: bpf-maps
    - hostPath:
        path: /proc
        type: Directory
      name: hostproc
    - hostPath:
        path: /run/cilium/cgroupv2
        type: DirectoryOrCreate
      name: cilium-cgroup
    - hostPath:
        path: /opt/cni/bin
        type: DirectoryOrCreate
      name: cni-path
    - hostPath:
        path: /etc/cni/net.d
        type: DirectoryOrCreate
      name: etc-cni-netd
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: clustermesh-secrets
      projected:
        defaultMode: 256
        sources:
        - secret:
            name: cilium-clustermesh
            optional: true
        - secret:
            items:
            - key: tls.key
              path: common-etcd-client.key
            - key: tls.crt
              path: common-etcd-client.crt
            - key: ca.crt
              path: common-etcd-client-ca.crt
            name: clustermesh-apiserver-remote-cert
            optional: true
        - secret:
            items:
            - key: tls.key
              path: local-etcd-client.key
            - key: tls.crt
              path: local-etcd-client.crt
            - key: ca.crt
              path: local-etcd-client-ca.crt
            name: clustermesh-apiserver-local-cert
            optional: true
    - configMap:
        defaultMode: 420
        items:
        - key: config
          path: ip-masq-agent
        name: ip-masq-agent
        optional: true
      name: ip-masq-agent
    - hostPath:
        path: /proc/sys/net
        type: Directory
      name: host-proc-sys-net
    - hostPath:
        path: /proc/sys/kernel
        type: Directory
      name: host-proc-sys-kernel
    - name: hubble-tls
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: tls.crt
              path: server.crt
            - key: tls.key
              path: server.key
            - key: ca.crt
              path: client-ca.crt
            name: hubble-server-certs
            optional: true
    - name: kube-api-access-t8687
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:21Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:28Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:33Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:33Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:04Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 20m
        memory: 50Mi
      containerID: containerd://f5e74eb30533388423a09fc7fa4054a96beb2f1e9e1e7614086db192f79a5252
      image: ghcr.io/digitalocean-packages/busybox:1.36
      imageID: ghcr.io/digitalocean-packages/busybox@sha256:907ca53d7e2947e849b839b1cd258c98fd3916c60f2e6e70c30edbf741ab6754
      lastState: {}
      name: arp-fix
      ready: true
      resources:
        limits:
          memory: 50Mi
        requests:
          cpu: 20m
          memory: 50Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T07:55:30Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8687
        readOnly: true
        recursiveReadOnly: Disabled
    - allocatedResources:
        cpu: 300m
        memory: 300Mi
      containerID: containerd://df2fea859ce3f585edb4e78cba8d3a49320e4a19b98981d5e11f7e05e3509aaa
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imageID: ghcr.io/digitalocean-packages/cilium@sha256:70f0c668c27a8efa64ecc923ae40472db101ab3cbd88e048f8070339f1b601cf
      lastState: {}
      name: cilium-agent
      ready: true
      resources:
        requests:
          cpu: 300m
          memory: 300Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T07:55:28Z"
      volumeMounts:
      - mountPath: /host/proc/sys/net
        name: host-proc-sys-net
      - mountPath: /host/proc/sys/kernel
        name: host-proc-sys-kernel
      - mountPath: /sys/fs/bpf
        name: bpf-maps
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /var/run/cilium/netns
        name: cilium-netns
      - mountPath: /host/etc/cni/net.d
        name: etc-cni-netd
      - mountPath: /var/lib/cilium/clustermesh
        name: clustermesh-secrets
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/config
        name: ip-masq-agent
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/lib/cilium/tls/hubble
        name: hubble-tls
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8687
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.4
    hostIPs:
    - ip: 10.120.0.4
    initContainerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 100Mi
      containerID: containerd://32f759de7c3c457f7e28c7d46e33f774c22e522d5ee420a14c9b590d6a36b807
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imageID: ghcr.io/digitalocean-packages/cilium@sha256:70f0c668c27a8efa64ecc923ae40472db101ab3cbd88e048f8070339f1b601cf
      lastState: {}
      name: anchor-ip-fix
      ready: true
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://32f759de7c3c457f7e28c7d46e33f774c22e522d5ee420a14c9b590d6a36b807
          exitCode: 0
          finishedAt: "2025-12-18T07:55:20Z"
          reason: Completed
          startedAt: "2025-12-18T07:55:20Z"
      volumeMounts:
      - mountPath: /host
        name: host
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8687
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://9c14794433578aefba77fab12e9fa133db35e9a75a40a631112ae6a5c6c5f680
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imageID: ghcr.io/digitalocean-packages/cilium@sha256:70f0c668c27a8efa64ecc923ae40472db101ab3cbd88e048f8070339f1b601cf
      lastState: {}
      name: config
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://9c14794433578aefba77fab12e9fa133db35e9a75a40a631112ae6a5c6c5f680
          exitCode: 0
          finishedAt: "2025-12-18T07:55:22Z"
          reason: Completed
          startedAt: "2025-12-18T07:55:22Z"
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8687
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://446028388a4b8b31fd6bed7a1b0039b46567bc6707662f3cf90425d0957ef638
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imageID: ghcr.io/digitalocean-packages/cilium@sha256:70f0c668c27a8efa64ecc923ae40472db101ab3cbd88e048f8070339f1b601cf
      lastState: {}
      name: mount-cgroup
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://446028388a4b8b31fd6bed7a1b0039b46567bc6707662f3cf90425d0957ef638
          exitCode: 0
          finishedAt: "2025-12-18T07:55:23Z"
          reason: Completed
          startedAt: "2025-12-18T07:55:23Z"
      volumeMounts:
      - mountPath: /hostproc
        name: hostproc
      - mountPath: /hostbin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8687
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://d55b7b3e090515523f83318ca2e67dabb00af57f487d710476b3541dae479705
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imageID: ghcr.io/digitalocean-packages/cilium@sha256:70f0c668c27a8efa64ecc923ae40472db101ab3cbd88e048f8070339f1b601cf
      lastState: {}
      name: apply-sysctl-overwrites
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://d55b7b3e090515523f83318ca2e67dabb00af57f487d710476b3541dae479705
          exitCode: 0
          finishedAt: "2025-12-18T07:55:24Z"
          reason: Completed
          startedAt: "2025-12-18T07:55:24Z"
      volumeMounts:
      - mountPath: /hostproc
        name: hostproc
      - mountPath: /hostbin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8687
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://356819755e417c9c5f9e0cd3ed88457668c6d01e52f5b7d2f38e4afb575317d0
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imageID: ghcr.io/digitalocean-packages/cilium@sha256:70f0c668c27a8efa64ecc923ae40472db101ab3cbd88e048f8070339f1b601cf
      lastState: {}
      name: mount-bpf-fs
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://356819755e417c9c5f9e0cd3ed88457668c6d01e52f5b7d2f38e4afb575317d0
          exitCode: 0
          finishedAt: "2025-12-18T07:55:25Z"
          reason: Completed
          startedAt: "2025-12-18T07:55:25Z"
      volumeMounts:
      - mountPath: /sys/fs/bpf
        name: bpf-maps
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8687
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://afc9260b02d0a349a7b8078043a0474c036d66038c8a603a12f1f11b59bfe0b8
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imageID: ghcr.io/digitalocean-packages/cilium@sha256:70f0c668c27a8efa64ecc923ae40472db101ab3cbd88e048f8070339f1b601cf
      lastState: {}
      name: clean-cilium-state
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://afc9260b02d0a349a7b8078043a0474c036d66038c8a603a12f1f11b59bfe0b8
          exitCode: 0
          finishedAt: "2025-12-18T07:55:26Z"
          reason: Completed
          startedAt: "2025-12-18T07:55:26Z"
      volumeMounts:
      - mountPath: /sys/fs/bpf
        name: bpf-maps
      - mountPath: /run/cilium/cgroupv2
        name: cilium-cgroup
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8687
        readOnly: true
        recursiveReadOnly: Disabled
    - allocatedResources:
        cpu: 100m
        memory: 10Mi
      containerID: containerd://fa0b90136c28e8d6b8f4d0b24606ddc90210be08b3a3640365e5afe12f6ee8cf
      image: ghcr.io/digitalocean-packages/cilium:v1.18.3
      imageID: ghcr.io/digitalocean-packages/cilium@sha256:70f0c668c27a8efa64ecc923ae40472db101ab3cbd88e048f8070339f1b601cf
      lastState: {}
      name: install-cni-binaries
      ready: true
      resources:
        requests:
          cpu: 100m
          memory: 10Mi
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://fa0b90136c28e8d6b8f4d0b24606ddc90210be08b3a3640365e5afe12f6ee8cf
          exitCode: 0
          finishedAt: "2025-12-18T07:55:27Z"
          reason: Completed
          startedAt: "2025-12-18T07:55:27Z"
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8687
        readOnly: true
        recursiveReadOnly: Disabled
    observedGeneration: 1
    phase: Running
    podIP: 10.120.0.4
    podIPs:
    - ip: 10.120.0.4
    qosClass: Burstable
    startTime: "2025-12-18T07:55:05Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-12-18T07:56:07Z"
    generateName: coredns-7c475d69-
    generation: 1
    labels:
      doks.digitalocean.com/managed: "true"
      k8s-app: kube-dns
      pod-template-hash: 7c475d69
    name: coredns-7c475d69-hcxjm
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-7c475d69
      uid: 9c459965-ff5f-4bd7-b00d-ad8363dc000d
    resourceVersion: "1423"
    uid: b1f34524-bbb9-42cd-844e-98d1b26ef3b9
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: doks.digitalocean.com/gpu-brand
              operator: DoesNotExist
          weight: 50
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: ghcr.io/digitalocean-packages/coredns/coredns:1.12.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 170Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7b84x
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k0
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - configMap:
        defaultMode: 420
        name: coredns-custom
        optional: true
      name: custom-config-volume
    - name: kube-api-access-7b84x
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:11Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:07Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:11Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:11Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:07Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 170Mi
      containerID: containerd://af5087191959277ac9a5561b6742b7e5a3ccf6f78a9802ccd2056be4333f0f3c
      image: ghcr.io/digitalocean-packages/coredns/coredns:1.12.1
      imageID: ghcr.io/digitalocean-packages/coredns/coredns@sha256:4f7a57135719628cf2070c5e3cbde64b013e90d4c560c5ecbf14004181f91998
      lastState: {}
      name: coredns
      ready: true
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 170Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T07:56:11Z"
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7b84x
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.4
    hostIPs:
    - ip: 10.120.0.4
    observedGeneration: 1
    phase: Running
    podIP: 10.108.0.26
    podIPs:
    - ip: 10.108.0.26
    qosClass: Burstable
    startTime: "2025-12-18T07:56:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-12-18T07:56:07Z"
    generateName: coredns-7c475d69-
    generation: 1
    labels:
      doks.digitalocean.com/managed: "true"
      k8s-app: kube-dns
      pod-template-hash: 7c475d69
    name: coredns-7c475d69-vvkcm
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-7c475d69
      uid: 9c459965-ff5f-4bd7-b00d-ad8363dc000d
    resourceVersion: "1434"
    uid: ef93fd7a-d341-4d72-aa1e-317baa8c1317
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: doks.digitalocean.com/gpu-brand
              operator: DoesNotExist
          weight: 50
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: ghcr.io/digitalocean-packages/coredns/coredns:1.12.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 170Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-k5nmr
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - configMap:
        defaultMode: 420
        name: coredns-custom
        optional: true
      name: custom-config-volume
    - name: kube-api-access-k5nmr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:12Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:07Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:12Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:12Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:07Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 170Mi
      containerID: containerd://5827e2e8014b10d181f9769db2c102cd1ad6d3d6a0dde61d58f2cd92ce396c2b
      image: ghcr.io/digitalocean-packages/coredns/coredns:1.12.1
      imageID: ghcr.io/digitalocean-packages/coredns/coredns@sha256:4f7a57135719628cf2070c5e3cbde64b013e90d4c560c5ecbf14004181f91998
      lastState: {}
      name: coredns
      ready: true
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 170Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T07:56:12Z"
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-k5nmr
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.3
    hostIPs:
    - ip: 10.120.0.3
    observedGeneration: 1
    phase: Running
    podIP: 10.108.0.160
    podIPs:
    - ip: 10.108.0.160
    qosClass: Burstable
    startTime: "2025-12-18T07:56:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: resource-requirements
    creationTimestamp: "2025-12-18T07:55:56Z"
    generateName: cpc-bridge-proxy-ebpf-
    generation: 1
    labels:
      app: cpc-bridge-proxy
      controller-revision-hash: 5896bc9d67
      doks.digitalocean.com/managed: "true"
      pod-template-generation: "1"
    name: cpc-bridge-proxy-ebpf-fj89c
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: cpc-bridge-proxy-ebpf
      uid: e8d6ecbd-94a8-4bc0-942c-ef57e3646d2c
    resourceVersion: "1354"
    uid: 994726cf-588f-4da0-bd7e-535c98b79648
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ptoken-assets--pool-525qi3gh9-s44k0
    automountServiceAccountToken: false
    containers:
    - image: ghcr.io/digitalocean-packages/cpbridge:1.29.3
      imagePullPolicy: IfNotPresent
      name: cpc-bridge-proxy
      resources:
        requests:
          cpu: 100m
          memory: 75Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/nginx
        name: cpc-bridge-proxy-config
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - /bin/bash
      - -c
      - |
        set -o errexit
        set -o pipefail
        set -o nounset
        # Wait until the cpbridge interface is ready
        until ip addr show dev cpbridge | grep -q "inet 100.65.22.7"; do echo "waiting for cpbridge interface to be up"; sleep 3; done
        ip addr del 100.65.22.7/32 dev cpbridge
        ip addr add 100.65.22.7/32 dev cpbridge
        ipt_nat="iptables-legacy -t nat"
        ipt_output_args="OUTPUT -p tcp -d 10.109.0.1/32 --dport 443 -j DNAT --to-destination 100.65.22.7:16443"
        ipt_prerouting_args="PREROUTING -p tcp -d 100.65.22.7 --dport 443 -j DNAT --to-destination 100.65.22.7:16443"
        ipt_output_cpbridgeip_args="OUTPUT -p tcp -d 100.65.22.7 --dport 443 -j DNAT --to-destination 100.65.22.7:16443"
        ${ipt_nat} --check ${ipt_output_args} || ${ipt_nat} --insert ${ipt_output_args}
        ${ipt_nat} --check ${ipt_prerouting_args} || ${ipt_nat} --insert ${ipt_prerouting_args}
        ${ipt_nat} --check ${ipt_output_cpbridgeip_args} || ${ipt_nat} --insert ${ipt_output_cpbridgeip_args}
      image: ghcr.io/digitalocean-packages/cpbridge:1.29.3
      imagePullPolicy: IfNotPresent
      name: init-cpbridge
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    nodeName: ptoken-assets--pool-525qi3gh9-s44k0
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: cpc-bridge-proxy-config
      name: cpc-bridge-proxy-config
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:03Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:03Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:04Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:04Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:56Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 75Mi
      containerID: containerd://a72166081530187a31513bf336681b711875ed0ab654ee65dbf6b961fc5a2b85
      image: ghcr.io/digitalocean-packages/cpbridge:1.29.3
      imageID: ghcr.io/digitalocean-packages/cpbridge@sha256:ef7027c591b53233d342cc252648bfc43d0b91ac2f616889afd997f80830ede2
      lastState: {}
      name: cpc-bridge-proxy
      ready: true
      resources:
        requests:
          cpu: 100m
          memory: 75Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T07:56:03Z"
      volumeMounts:
      - mountPath: /etc/nginx
        name: cpc-bridge-proxy-config
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.4
    hostIPs:
    - ip: 10.120.0.4
    initContainerStatuses:
    - containerID: containerd://a130b1b6b78bbbb85472373ff2712af15a380bb41172cb997a7be8ecb2b4b62b
      image: ghcr.io/digitalocean-packages/cpbridge:1.29.3
      imageID: ghcr.io/digitalocean-packages/cpbridge@sha256:ef7027c591b53233d342cc252648bfc43d0b91ac2f616889afd997f80830ede2
      lastState: {}
      name: init-cpbridge
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://a130b1b6b78bbbb85472373ff2712af15a380bb41172cb997a7be8ecb2b4b62b
          exitCode: 0
          finishedAt: "2025-12-18T07:56:02Z"
          reason: Completed
          startedAt: "2025-12-18T07:56:02Z"
    observedGeneration: 1
    phase: Running
    podIP: 10.120.0.4
    podIPs:
    - ip: 10.120.0.4
    qosClass: Burstable
    startTime: "2025-12-18T07:55:56Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: resource-requirements
    creationTimestamp: "2025-12-18T07:55:56Z"
    generateName: cpc-bridge-proxy-ebpf-
    generation: 1
    labels:
      app: cpc-bridge-proxy
      controller-revision-hash: 5896bc9d67
      doks.digitalocean.com/managed: "true"
      pod-template-generation: "1"
    name: cpc-bridge-proxy-ebpf-qh2h5
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: cpc-bridge-proxy-ebpf
      uid: e8d6ecbd-94a8-4bc0-942c-ef57e3646d2c
    resourceVersion: "1352"
    uid: 56cec623-59fd-4f87-96ce-73875e571537
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ptoken-assets--pool-525qi3gh9-s44k1
    automountServiceAccountToken: false
    containers:
    - image: ghcr.io/digitalocean-packages/cpbridge:1.29.3
      imagePullPolicy: IfNotPresent
      name: cpc-bridge-proxy
      resources:
        requests:
          cpu: 100m
          memory: 75Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/nginx
        name: cpc-bridge-proxy-config
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - /bin/bash
      - -c
      - |
        set -o errexit
        set -o pipefail
        set -o nounset
        # Wait until the cpbridge interface is ready
        until ip addr show dev cpbridge | grep -q "inet 100.65.22.7"; do echo "waiting for cpbridge interface to be up"; sleep 3; done
        ip addr del 100.65.22.7/32 dev cpbridge
        ip addr add 100.65.22.7/32 dev cpbridge
        ipt_nat="iptables-legacy -t nat"
        ipt_output_args="OUTPUT -p tcp -d 10.109.0.1/32 --dport 443 -j DNAT --to-destination 100.65.22.7:16443"
        ipt_prerouting_args="PREROUTING -p tcp -d 100.65.22.7 --dport 443 -j DNAT --to-destination 100.65.22.7:16443"
        ipt_output_cpbridgeip_args="OUTPUT -p tcp -d 100.65.22.7 --dport 443 -j DNAT --to-destination 100.65.22.7:16443"
        ${ipt_nat} --check ${ipt_output_args} || ${ipt_nat} --insert ${ipt_output_args}
        ${ipt_nat} --check ${ipt_prerouting_args} || ${ipt_nat} --insert ${ipt_prerouting_args}
        ${ipt_nat} --check ${ipt_output_cpbridgeip_args} || ${ipt_nat} --insert ${ipt_output_cpbridgeip_args}
      image: ghcr.io/digitalocean-packages/cpbridge:1.29.3
      imagePullPolicy: IfNotPresent
      name: init-cpbridge
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    nodeName: ptoken-assets--pool-525qi3gh9-s44k1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: cpc-bridge-proxy-config
      name: cpc-bridge-proxy-config
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:03Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:03Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:04Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:04Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:56Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 75Mi
      containerID: containerd://972fae0f698ce9d400558822d858de89a2e949a9574509693915df33b82d1141
      image: ghcr.io/digitalocean-packages/cpbridge:1.29.3
      imageID: ghcr.io/digitalocean-packages/cpbridge@sha256:ef7027c591b53233d342cc252648bfc43d0b91ac2f616889afd997f80830ede2
      lastState: {}
      name: cpc-bridge-proxy
      ready: true
      resources:
        requests:
          cpu: 100m
          memory: 75Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T07:56:03Z"
      volumeMounts:
      - mountPath: /etc/nginx
        name: cpc-bridge-proxy-config
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.3
    hostIPs:
    - ip: 10.120.0.3
    initContainerStatuses:
    - containerID: containerd://d3715d4b7d607c291d0d0cde10b15fce8451ea1bdd99b49ea48449f776be1394
      image: ghcr.io/digitalocean-packages/cpbridge:1.29.3
      imageID: ghcr.io/digitalocean-packages/cpbridge@sha256:ef7027c591b53233d342cc252648bfc43d0b91ac2f616889afd997f80830ede2
      lastState: {}
      name: init-cpbridge
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://d3715d4b7d607c291d0d0cde10b15fce8451ea1bdd99b49ea48449f776be1394
          exitCode: 0
          finishedAt: "2025-12-18T07:56:02Z"
          reason: Completed
          startedAt: "2025-12-18T07:56:02Z"
    observedGeneration: 1
    phase: Running
    podIP: 10.120.0.3
    podIPs:
    - ip: 10.120.0.3
    qosClass: Burstable
    startTime: "2025-12-18T07:55:56Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: privileged-containers,non-root-user,resource-requirements,hostpath-volume
      kubectl.kubernetes.io/default-container: csi-do-plugin
    creationTimestamp: "2025-12-18T07:56:13Z"
    generateName: csi-do-node-
    generation: 1
    labels:
      app: csi-do-node
      controller-revision-hash: b87697748
      doks.digitalocean.com/managed: "true"
      pod-template-generation: "1"
      role: csi-do
    name: csi-do-node-fcvn9
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-do-node
      uid: 494ac330-89ed-49dd-9d85-8240a2532cd6
    resourceVersion: "1501"
    uid: 27f8b052-ff73-4093-a895-3d6f113c0edb
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ptoken-assets--pool-525qi3gh9-s44k0
    containers:
    - args:
      - --v=5
      - --csi-address=$(ADDRESS)
      - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      - name: DRIVER_REG_SOCK_PATH
        value: /var/lib/kubelet/plugins/dobs.csi.digitalocean.com/csi.sock
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: ghcr.io/digitalocean-packages/sig-storage/csi-node-driver-registrar:v2.15.0
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - rm -rf /registration/dobs.csi.digitalocean.com /registration/dobs.csi.digitalocean.com-reg.sock
      name: csi-node-driver-registrar
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: plugin-dir
      - mountPath: /registration/
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-86nfm
        readOnly: true
    - args:
      - --endpoint=$(CSI_ENDPOINT)
      - --validate-attachment=true
      - --volume-limit=15
      - --url=https://api.digitalocean.com
      - --driver-name=dobs.csi.digitalocean.com
      env:
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: ghcr.io/digitalocean-packages/do-csi-plugin:v4.15.0
      imagePullPolicy: Always
      name: csi-do-plugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /dev
        name: device-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-86nfm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k0
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: csi-do-node-sa
    serviceAccountName: csi-do-node-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: DirectoryOrCreate
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/plugins/dobs.csi.digitalocean.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: device-dir
    - name: kube-api-access-86nfm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:19Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:13Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:19Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:19Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:13Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c522a874b7bce2b4b31398732a09de915484e698a72d7001d07b324904f1cdda
      image: ghcr.io/digitalocean-packages/do-csi-plugin:v4.15.0
      imageID: ghcr.io/digitalocean-packages/do-csi-plugin@sha256:5aab8f0e0fe80cc835e977aec0242b99223ba0082a31a2bd038aeb74ab28423b
      lastState: {}
      name: csi-do-plugin
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T07:56:19Z"
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet
        name: pods-mount-dir
      - mountPath: /dev
        name: device-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-86nfm
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://ff832e978921037df2fbdae91e89a79244285a8c3e0029f65dffea4883afe18b
      image: ghcr.io/digitalocean-packages/sig-storage/csi-node-driver-registrar:v2.15.0
      imageID: ghcr.io/digitalocean-packages/sig-storage/csi-node-driver-registrar@sha256:1ca23bdbb1504e41233d527e826d02dac98571c45a6cd74c5fb28be485d680fa
      lastState: {}
      name: csi-node-driver-registrar
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T07:56:15Z"
      volumeMounts:
      - mountPath: /csi/
        name: plugin-dir
      - mountPath: /registration/
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-86nfm
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.4
    hostIPs:
    - ip: 10.120.0.4
    observedGeneration: 1
    phase: Running
    podIP: 10.120.0.4
    podIPs:
    - ip: 10.120.0.4
    qosClass: BestEffort
    startTime: "2025-12-18T07:56:13Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: privileged-containers,non-root-user,resource-requirements,hostpath-volume
      kubectl.kubernetes.io/default-container: csi-do-plugin
    creationTimestamp: "2025-12-18T07:56:13Z"
    generateName: csi-do-node-
    generation: 1
    labels:
      app: csi-do-node
      controller-revision-hash: b87697748
      doks.digitalocean.com/managed: "true"
      pod-template-generation: "1"
      role: csi-do
    name: csi-do-node-ztcf9
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-do-node
      uid: 494ac330-89ed-49dd-9d85-8240a2532cd6
    resourceVersion: "1499"
    uid: 6799b648-357d-4951-948c-b6191f208bb1
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ptoken-assets--pool-525qi3gh9-s44k1
    containers:
    - args:
      - --v=5
      - --csi-address=$(ADDRESS)
      - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      - name: DRIVER_REG_SOCK_PATH
        value: /var/lib/kubelet/plugins/dobs.csi.digitalocean.com/csi.sock
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: ghcr.io/digitalocean-packages/sig-storage/csi-node-driver-registrar:v2.15.0
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - rm -rf /registration/dobs.csi.digitalocean.com /registration/dobs.csi.digitalocean.com-reg.sock
      name: csi-node-driver-registrar
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: plugin-dir
      - mountPath: /registration/
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t295d
        readOnly: true
    - args:
      - --endpoint=$(CSI_ENDPOINT)
      - --validate-attachment=true
      - --volume-limit=15
      - --url=https://api.digitalocean.com
      - --driver-name=dobs.csi.digitalocean.com
      env:
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: ghcr.io/digitalocean-packages/do-csi-plugin:v4.15.0
      imagePullPolicy: Always
      name: csi-do-plugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /dev
        name: device-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t295d
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: csi-do-node-sa
    serviceAccountName: csi-do-node-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: DirectoryOrCreate
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/plugins/dobs.csi.digitalocean.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: device-dir
    - name: kube-api-access-t295d
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:19Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:13Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:19Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:19Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:13Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a1d55794a99a173ba3ed98a9111b547255628acd383e056ce4c1e067d55a2056
      image: ghcr.io/digitalocean-packages/do-csi-plugin:v4.15.0
      imageID: ghcr.io/digitalocean-packages/do-csi-plugin@sha256:5aab8f0e0fe80cc835e977aec0242b99223ba0082a31a2bd038aeb74ab28423b
      lastState: {}
      name: csi-do-plugin
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T07:56:19Z"
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet
        name: pods-mount-dir
      - mountPath: /dev
        name: device-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t295d
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://16560bc07c2128e30e275ef140046e7043f302ba681295825f3c02229feac730
      image: ghcr.io/digitalocean-packages/sig-storage/csi-node-driver-registrar:v2.15.0
      imageID: ghcr.io/digitalocean-packages/sig-storage/csi-node-driver-registrar@sha256:1ca23bdbb1504e41233d527e826d02dac98571c45a6cd74c5fb28be485d680fa
      lastState: {}
      name: csi-node-driver-registrar
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T07:56:16Z"
      volumeMounts:
      - mountPath: /csi/
        name: plugin-dir
      - mountPath: /registration/
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t295d
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.3
    hostIPs:
    - ip: 10.120.0.3
    observedGeneration: 1
    phase: Running
    podIP: 10.120.0.3
    podIPs:
    - ip: 10.120.0.3
    qosClass: BestEffort
    startTime: "2025-12-18T07:56:13Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: resource-requirements,hostpath-volume
    creationTimestamp: "2025-12-18T07:56:22Z"
    generateName: do-node-agent-
    generation: 1
    labels:
      app: do-node-agent
      controller-revision-hash: 7bdcddb799
      doks.digitalocean.com/managed: "true"
      pod-template-generation: "1"
    name: do-node-agent-28th5
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: do-node-agent
      uid: f3c49fdd-a457-4d85-8477-2b024d14d32f
    resourceVersion: "1579"
    uid: df83498c-04d0-41d6-a390-508965ae2473
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ptoken-assets--pool-525qi3gh9-s44k1
    containers:
    - args:
      - '@/etc/config/do-agent-config'
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --k8s-metrics-path=http://kube-state-metrics.kube-system.svc.cluster.local:8080/metrics
      - --additional-label=kubernetes_cluster_uuid:ca54c9b1-1f4a-498d-9a4d-af31964c74a9
      command:
      - /bin/do-agent
      image: ghcr.io/digitalocean-packages/do-agent:3.18.5-rc
      imagePullPolicy: IfNotPresent
      name: do-node-agent
      resources:
        limits:
          memory: 300Mi
        requests:
          cpu: 102m
          memory: 80Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
      - mountPath: /etc/config
        name: dynamic-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kwqtn
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - sh
      - -c
      - |
        set -o errexit
        set -o pipefail
        set -o nounset

        KUBECTL=/host/usr/bin/kubectl
        POOL_ID="$(${KUBECTL} get node ${NODE_NAME} -o jsonpath='{.metadata.labels.doks\.digitalocean\.com/node-pool-id}')"
        [[ -z "${POOL_ID}" ]] && echo "Pool ID label missing" && exit 1
        echo "--additional-label=kubernetes_node_pool_uuid:${POOL_ID}" > /etc/config/do-agent-config
        echo "Pool ID configured: ${POOL_ID}"
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: ghcr.io/digitalocean-packages/distroless/static-debian12:debug-nonroot-amd64
      imagePullPolicy: IfNotPresent
      name: dynamic-config
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: dynamic-config
      - mountPath: /host/usr/bin/kubectl
        name: host-kubectl
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kwqtn
        readOnly: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: do-agent
    serviceAccountName: do-agent
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
    - emptyDir: {}
      name: dynamic-config
    - hostPath:
        path: /usr/bin/kubectl
        type: File
      name: host-kubectl
    - name: kube-api-access-kwqtn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:24Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:25Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:27Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:27Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:22Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 102m
        memory: 80Mi
      containerID: containerd://b08ee4e79122da88d124ddfd349b19bcbce68614e1218693128bbc0d637009fa
      image: ghcr.io/digitalocean-packages/do-agent:3.18.5-rc
      imageID: ghcr.io/digitalocean-packages/do-agent@sha256:cefc887b8fed4d41be11245616643a7dd7b2eb2d2e440a3ff4bf2611d698180e
      lastState: {}
      name: do-node-agent
      ready: true
      resources:
        limits:
          memory: 300Mi
        requests:
          cpu: 102m
          memory: 80Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T07:56:27Z"
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/root
        name: root
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/config
        name: dynamic-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kwqtn
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.3
    hostIPs:
    - ip: 10.120.0.3
    initContainerStatuses:
    - containerID: containerd://c42d2575f5600cbec7eccebb52aefa67b7023494fcf7d3e094da7c10f23d0dd9
      image: ghcr.io/digitalocean-packages/distroless/static-debian12:debug-nonroot-amd64
      imageID: ghcr.io/digitalocean-packages/distroless/static-debian12@sha256:f04486101c5a74a46d84c16427ee6344cb5cf9aae381202dc95800ca89e2f199
      lastState: {}
      name: dynamic-config
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://c42d2575f5600cbec7eccebb52aefa67b7023494fcf7d3e094da7c10f23d0dd9
          exitCode: 0
          finishedAt: "2025-12-18T07:56:24Z"
          reason: Completed
          startedAt: "2025-12-18T07:56:24Z"
      volumeMounts:
      - mountPath: /etc/config
        name: dynamic-config
      - mountPath: /host/usr/bin/kubectl
        name: host-kubectl
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kwqtn
        readOnly: true
        recursiveReadOnly: Disabled
    observedGeneration: 1
    phase: Running
    podIP: 10.120.0.3
    podIPs:
    - ip: 10.120.0.3
    qosClass: Burstable
    startTime: "2025-12-18T07:56:22Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: resource-requirements,hostpath-volume
    creationTimestamp: "2025-12-18T07:56:22Z"
    generateName: do-node-agent-
    generation: 1
    labels:
      app: do-node-agent
      controller-revision-hash: 7bdcddb799
      doks.digitalocean.com/managed: "true"
      pod-template-generation: "1"
    name: do-node-agent-9gz22
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: do-node-agent
      uid: f3c49fdd-a457-4d85-8477-2b024d14d32f
    resourceVersion: "1580"
    uid: 338d23bf-630b-4129-a13d-3a6028145d96
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ptoken-assets--pool-525qi3gh9-s44k0
    containers:
    - args:
      - '@/etc/config/do-agent-config'
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --k8s-metrics-path=http://kube-state-metrics.kube-system.svc.cluster.local:8080/metrics
      - --additional-label=kubernetes_cluster_uuid:ca54c9b1-1f4a-498d-9a4d-af31964c74a9
      command:
      - /bin/do-agent
      image: ghcr.io/digitalocean-packages/do-agent:3.18.5-rc
      imagePullPolicy: IfNotPresent
      name: do-node-agent
      resources:
        limits:
          memory: 300Mi
        requests:
          cpu: 102m
          memory: 80Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
      - mountPath: /etc/config
        name: dynamic-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qs2pd
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - sh
      - -c
      - |
        set -o errexit
        set -o pipefail
        set -o nounset

        KUBECTL=/host/usr/bin/kubectl
        POOL_ID="$(${KUBECTL} get node ${NODE_NAME} -o jsonpath='{.metadata.labels.doks\.digitalocean\.com/node-pool-id}')"
        [[ -z "${POOL_ID}" ]] && echo "Pool ID label missing" && exit 1
        echo "--additional-label=kubernetes_node_pool_uuid:${POOL_ID}" > /etc/config/do-agent-config
        echo "Pool ID configured: ${POOL_ID}"
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: ghcr.io/digitalocean-packages/distroless/static-debian12:debug-nonroot-amd64
      imagePullPolicy: IfNotPresent
      name: dynamic-config
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: dynamic-config
      - mountPath: /host/usr/bin/kubectl
        name: host-kubectl
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qs2pd
        readOnly: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k0
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: do-agent
    serviceAccountName: do-agent
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
    - emptyDir: {}
      name: dynamic-config
    - hostPath:
        path: /usr/bin/kubectl
        type: File
      name: host-kubectl
    - name: kube-api-access-qs2pd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:24Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:25Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:27Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:27Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:56:22Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 102m
        memory: 80Mi
      containerID: containerd://1ea33e0c0238bbef4be3e03718b7d498cb652ea22f547434290bea46db82c644
      image: ghcr.io/digitalocean-packages/do-agent:3.18.5-rc
      imageID: ghcr.io/digitalocean-packages/do-agent@sha256:cefc887b8fed4d41be11245616643a7dd7b2eb2d2e440a3ff4bf2611d698180e
      lastState: {}
      name: do-node-agent
      ready: true
      resources:
        limits:
          memory: 300Mi
        requests:
          cpu: 102m
          memory: 80Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T07:56:27Z"
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/root
        name: root
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/config
        name: dynamic-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qs2pd
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.4
    hostIPs:
    - ip: 10.120.0.4
    initContainerStatuses:
    - containerID: containerd://4bcf9d7624d40b5ec5dc3d0fba83e3ee2d9a5283a4b483e6bfb2f455a3a57948
      image: ghcr.io/digitalocean-packages/distroless/static-debian12:debug-nonroot-amd64
      imageID: ghcr.io/digitalocean-packages/distroless/static-debian12@sha256:f04486101c5a74a46d84c16427ee6344cb5cf9aae381202dc95800ca89e2f199
      lastState: {}
      name: dynamic-config
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://4bcf9d7624d40b5ec5dc3d0fba83e3ee2d9a5283a4b483e6bfb2f455a3a57948
          exitCode: 0
          finishedAt: "2025-12-18T07:56:24Z"
          reason: Completed
          startedAt: "2025-12-18T07:56:23Z"
      volumeMounts:
      - mountPath: /etc/config
        name: dynamic-config
      - mountPath: /host/usr/bin/kubectl
        name: host-kubectl
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qs2pd
        readOnly: true
        recursiveReadOnly: Disabled
    observedGeneration: 1
    phase: Running
    podIP: 10.120.0.4
    podIPs:
    - ip: 10.120.0.4
    qosClass: Burstable
    startTime: "2025-12-18T07:56:22Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: resource-requirements
    creationTimestamp: "2026-02-04T02:01:12Z"
    generateName: doks-telemetry-config-reloader-
    generation: 1
    labels:
      app: doks-otel
      controller-revision-hash: 5ddf47cb94
      doks.digitalocean.com/managed: "true"
      pod-template-generation: "1"
    name: doks-telemetry-config-reloader-w2wb5
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: doks-telemetry-config-reloader
      uid: db8123dd-58b4-4222-b0e1-92d8c042b0f0
    resourceVersion: "14966883"
    uid: 4028fcf4-ed77-411b-8df4-ba5744fb0c0c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ptoken-assets--pool-525qi3gh9-s44k0
    automountServiceAccountToken: false
    containers:
    - command:
      - chroot
      - /host
      - /bin/bash
      - -c
      - |
        refresh_token() {
          EXISTING_TOKEN=$(yq eval ".extensions.bearertokenauth/withscheme.token" /etc/otelcol-contrib/config.yaml)
          NEW_TOKEN=$(cat /worker_observability_refresh_token/worker_observability_refresh_token)
          if [ "${EXISTING_TOKEN}" == "${NEW_TOKEN}" ]; then
            echo "config is in sync, skipping."
          elif [ "${NEW_TOKEN}" == "TO-BE-REPLACED" ]; then
            echo "waiting for refresh token to be available"
          else
            echo "refreshing token"
            yq e -i ".extensions.bearertokenauth/withscheme.token = \"$NEW_TOKEN\" " /etc/otelcol-contrib/config.yaml
            SYSTEMCTL_FORCE_BUS=1 systemctl reload otelcol-contrib
          fi
        }
        while true; do
          echo "trying to refesh token..."
          refresh_token
          echo "done."
          sleep 60;
        done
      image: ghcr.io/digitalocean-packages/busybox:1.36
      imagePullPolicy: IfNotPresent
      name: telemetry-config-reloader
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host
      - mountPath: /host/worker_observability_refresh_token
        name: worker-observability-refresh-token
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k0
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host
    - name: worker-observability-refresh-token
      secret:
        defaultMode: 420
        secretName: worker-observability-refresh-token
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T02:01:14Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T02:01:12Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T02:01:14Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T02:01:14Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T02:01:12Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4ab255fe056a5ee7b88dc57936bd0a874e6a34ccc985d9b54d9838ab81894dca
      image: ghcr.io/digitalocean-packages/busybox:1.36
      imageID: ghcr.io/digitalocean-packages/busybox@sha256:907ca53d7e2947e849b839b1cd258c98fd3916c60f2e6e70c30edbf741ab6754
      lastState: {}
      name: telemetry-config-reloader
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-04T02:01:13Z"
      volumeMounts:
      - mountPath: /host
        name: host
      - mountPath: /host/worker_observability_refresh_token
        name: worker-observability-refresh-token
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.4
    hostIPs:
    - ip: 10.120.0.4
    observedGeneration: 1
    phase: Running
    podIP: 10.120.0.4
    podIPs:
    - ip: 10.120.0.4
    qosClass: BestEffort
    startTime: "2026-02-04T02:01:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: resource-requirements
    creationTimestamp: "2026-02-04T02:01:12Z"
    generateName: doks-telemetry-config-reloader-
    generation: 1
    labels:
      app: doks-otel
      controller-revision-hash: 5ddf47cb94
      doks.digitalocean.com/managed: "true"
      pod-template-generation: "1"
    name: doks-telemetry-config-reloader-wnhlt
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: doks-telemetry-config-reloader
      uid: db8123dd-58b4-4222-b0e1-92d8c042b0f0
    resourceVersion: "14966881"
    uid: 18203067-533c-44f7-b151-32cbb898279b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ptoken-assets--pool-525qi3gh9-s44k1
    automountServiceAccountToken: false
    containers:
    - command:
      - chroot
      - /host
      - /bin/bash
      - -c
      - |
        refresh_token() {
          EXISTING_TOKEN=$(yq eval ".extensions.bearertokenauth/withscheme.token" /etc/otelcol-contrib/config.yaml)
          NEW_TOKEN=$(cat /worker_observability_refresh_token/worker_observability_refresh_token)
          if [ "${EXISTING_TOKEN}" == "${NEW_TOKEN}" ]; then
            echo "config is in sync, skipping."
          elif [ "${NEW_TOKEN}" == "TO-BE-REPLACED" ]; then
            echo "waiting for refresh token to be available"
          else
            echo "refreshing token"
            yq e -i ".extensions.bearertokenauth/withscheme.token = \"$NEW_TOKEN\" " /etc/otelcol-contrib/config.yaml
            SYSTEMCTL_FORCE_BUS=1 systemctl reload otelcol-contrib
          fi
        }
        while true; do
          echo "trying to refesh token..."
          refresh_token
          echo "done."
          sleep 60;
        done
      image: ghcr.io/digitalocean-packages/busybox:1.36
      imagePullPolicy: IfNotPresent
      name: telemetry-config-reloader
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host
      - mountPath: /host/worker_observability_refresh_token
        name: worker-observability-refresh-token
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host
    - name: worker-observability-refresh-token
      secret:
        defaultMode: 420
        secretName: worker-observability-refresh-token
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T02:01:13Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T02:01:12Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T02:01:13Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T02:01:13Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T02:01:12Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://bce9b02ccdc846a2b853711ab926bb681f7db958b05742fed0bbce283a60e8e9
      image: ghcr.io/digitalocean-packages/busybox:1.36
      imageID: ghcr.io/digitalocean-packages/busybox@sha256:907ca53d7e2947e849b839b1cd258c98fd3916c60f2e6e70c30edbf741ab6754
      lastState: {}
      name: telemetry-config-reloader
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-04T02:01:13Z"
      volumeMounts:
      - mountPath: /host
        name: host
      - mountPath: /host/worker_observability_refresh_token
        name: worker-observability-refresh-token
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.3
    hostIPs:
    - ip: 10.120.0.3
    observedGeneration: 1
    phase: Running
    podIP: 10.120.0.3
    podIPs:
    - ip: 10.120.0.3
    qosClass: BestEffort
    startTime: "2026-02-04T02:01:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      clusterlint.digitalocean.com/disabled-checks: resource-requirements
    creationTimestamp: "2025-12-18T07:53:39Z"
    generateName: hubble-relay-7d6bc4d6f4-
    generation: 1
    labels:
      app.kubernetes.io/name: hubble-relay
      app.kubernetes.io/part-of: cilium
      doks.digitalocean.com/managed: "true"
      k8s-app: hubble-relay
      pod-template-hash: 7d6bc4d6f4
    name: hubble-relay-7d6bc4d6f4-889rh
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: hubble-relay-7d6bc4d6f4
      uid: ff22318f-5320-41d3-8443-95f027f90086
    resourceVersion: "1169"
    uid: 2060a8ae-b988-4fef-84e5-c4d300d9fe5d
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: doks.digitalocean.com/gpu-brand
              operator: DoesNotExist
          weight: 100
      podAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              k8s-app: cilium
          topologyKey: kubernetes.io/hostname
    automountServiceAccountToken: false
    containers:
    - args:
      - serve
      command:
      - hubble-relay
      image: ghcr.io/digitalocean-packages/hubble-relay:v1.18.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 12
        grpc:
          port: 4222
          service: ""
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      name: hubble-relay
      ports:
      - containerPort: 4245
        name: grpc
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        runAsGroup: 65532
        runAsNonRoot: true
        runAsUser: 65532
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hubble-relay
        name: config
        readOnly: true
      - mountPath: /var/lib/hubble-relay/tls
        name: tls
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k0
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65532
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: hubble-relay
    serviceAccountName: hubble-relay
    terminationGracePeriodSeconds: 1
    tolerations:
    - key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: config.yaml
          path: config.yaml
        name: hubble-relay-config
      name: config
    - name: tls
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: tls.crt
              path: client.crt
            - key: tls.key
              path: client.key
            - key: ca.crt
              path: hubble-server-ca.crt
            name: hubble-relay-client-certs
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:39Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:33Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:39Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:39Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:33Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://23d41b3ccc43c3a28a527b717886fba0ae62fe5b1a0b84bb881b7617b271df6d
      image: ghcr.io/digitalocean-packages/hubble-relay:v1.18.3
      imageID: ghcr.io/digitalocean-packages/hubble-relay@sha256:67f21a3b686e7b72545b7ef7b55d9469d18c7e2a71c88c38f2bbf59809c5188c
      lastState: {}
      name: hubble-relay
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T07:55:38Z"
      volumeMounts:
      - mountPath: /etc/hubble-relay
        name: config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/hubble-relay/tls
        name: tls
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.4
    hostIPs:
    - ip: 10.120.0.4
    observedGeneration: 1
    phase: Running
    podIP: 10.108.0.8
    podIPs:
    - ip: 10.108.0.8
    qosClass: BestEffort
    startTime: "2025-12-18T07:55:33Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      clusterlint.digitalocean.com/disabled-checks: resource-requirements
    creationTimestamp: "2025-12-18T07:55:42Z"
    generateName: hubble-ui-b95c9f464-
    generation: 1
    labels:
      app.kubernetes.io/name: hubble-ui
      app.kubernetes.io/part-of: cilium
      doks.digitalocean.com/managed: "true"
      k8s-app: hubble-ui
      pod-template-hash: b95c9f464
    name: hubble-ui-b95c9f464-9mm7w
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: hubble-ui-b95c9f464
      uid: d655df8b-13eb-4633-bf2c-570b2d1475c0
    resourceVersion: "1234"
    uid: 91fecaef-9bf2-45e2-9f54-f74c905622ef
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: doks.digitalocean.com/gpu-brand
              operator: DoesNotExist
          weight: 100
    automountServiceAccountToken: true
    containers:
    - image: ghcr.io/digitalocean-packages/hubble-ui:v0.13.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8081
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: frontend
      ports:
      - containerPort: 8081
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 8081
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/nginx/conf.d/default.conf
        name: hubble-ui-nginx-conf
        subPath: nginx.conf
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jvkwc
        readOnly: true
    - env:
      - name: EVENTS_SERVER_PORT
        value: "8090"
      - name: FLOWS_API_ADDR
        value: hubble-relay:80
      image: ghcr.io/digitalocean-packages/hubble-ui-backend:v0.13.2
      imagePullPolicy: IfNotPresent
      name: backend
      ports:
      - containerPort: 8090
        name: grpc
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jvkwc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      runAsGroup: 1001
      runAsUser: 1001
    serviceAccount: hubble-ui
    serviceAccountName: hubble-ui
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: hubble-ui-nginx
      name: hubble-ui-nginx-conf
    - emptyDir: {}
      name: tmp-dir
    - name: kube-api-access-jvkwc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:49Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:42Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:49Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:49Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:42Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5253fa176fc5346b91adfd82845b5dc98d78bc1466bf16961f6a203cfae75725
      image: ghcr.io/digitalocean-packages/hubble-ui-backend:v0.13.2
      imageID: ghcr.io/digitalocean-packages/hubble-ui-backend@sha256:bfe0cb9158fd044ab499706c3e36469739e6b4accda8e79f80367935a17b4ac6
      lastState: {}
      name: backend
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T07:55:48Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jvkwc
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://12861be387bd992973428c582d3505332af53824949e917bfb824a50eeedc057
      image: ghcr.io/digitalocean-packages/hubble-ui:v0.13.2
      imageID: ghcr.io/digitalocean-packages/hubble-ui@sha256:f6574f768b890cbbe857f35f9083180b1a514b664592e874f7d7bbef2fbd6e3d
      lastState: {}
      name: frontend
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T07:55:45Z"
      volumeMounts:
      - mountPath: /etc/nginx/conf.d/default.conf
        name: hubble-ui-nginx-conf
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jvkwc
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.3
    hostIPs:
    - ip: 10.120.0.3
    observedGeneration: 1
    phase: Running
    podIP: 10.108.0.183
    podIPs:
    - ip: 10.108.0.183
    qosClass: BestEffort
    startTime: "2025-12-18T07:55:42Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: resource-requirements
    creationTimestamp: "2025-12-18T07:55:50Z"
    generateName: konnectivity-agent-
    generation: 1
    labels:
      controller-revision-hash: 58b6f7bdff
      doks.digitalocean.com/managed: "true"
      k8s-app: konnectivity-agent
      pod-template-generation: "1"
    name: konnectivity-agent-fhf2r
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: konnectivity-agent
      uid: 05ea5256-b724-471e-be05-167efe1d29d3
    resourceVersion: "1285"
    uid: 0c0fa573-15f8-44ae-b79c-73ac3978b1b9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ptoken-assets--pool-525qi3gh9-s44k0
    containers:
    - args:
      - --logtostderr=true
      - --ca-cert=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      - --proxy-server-port=8132
      - --admin-server-port=8133
      - --health-server-port=8134
      - --keepalive-time=5m
      - --service-account-token-path=/var/run/secrets/tokens/konnectivity-agent-token
      - --proxy-server-host=ca54c9b1-1f4a-498d-9a4d-af31964c74a9.k8s.ondigitalocean.com
      command:
      - /proxy-agent
      image: ghcr.io/digitalocean-packages/kas-network-proxy/proxy-agent:v0.33.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8134
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: konnectivity-agent
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/tokens
        name: konnectivity-agent-token
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-k6l7b
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k0
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: konnectivity-agent
    serviceAccountName: konnectivity-agent
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: konnectivity-agent-token
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            audience: system:konnectivity-server
            expirationSeconds: 3600
            path: konnectivity-agent-token
    - name: kube-api-access-k6l7b
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:54Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:50Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:54Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:54Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:50Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://cb65a77c01e73c5b9a4b032fcf5074548b2aece4ce6ec5028763212ba8f97a30
      image: ghcr.io/digitalocean-packages/kas-network-proxy/proxy-agent:v0.33.0
      imageID: ghcr.io/digitalocean-packages/kas-network-proxy/proxy-agent@sha256:0d1b560759bcad02cc09b3d5cf2bb4ba4a6ec1af61f9884dd62b398c09a56668
      lastState: {}
      name: konnectivity-agent
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T07:55:53Z"
      volumeMounts:
      - mountPath: /var/run/secrets/tokens
        name: konnectivity-agent-token
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-k6l7b
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.4
    hostIPs:
    - ip: 10.120.0.4
    observedGeneration: 1
    phase: Running
    podIP: 10.108.0.102
    podIPs:
    - ip: 10.108.0.102
    qosClass: BestEffort
    startTime: "2025-12-18T07:55:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      clusterlint.digitalocean.com/disabled-checks: resource-requirements
    creationTimestamp: "2025-12-18T07:55:50Z"
    generateName: konnectivity-agent-
    generation: 1
    labels:
      controller-revision-hash: 58b6f7bdff
      doks.digitalocean.com/managed: "true"
      k8s-app: konnectivity-agent
      pod-template-generation: "1"
    name: konnectivity-agent-xnxjh
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: konnectivity-agent
      uid: 05ea5256-b724-471e-be05-167efe1d29d3
    resourceVersion: "1283"
    uid: cd3db524-94be-431c-b5e9-6be397d105a2
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ptoken-assets--pool-525qi3gh9-s44k1
    containers:
    - args:
      - --logtostderr=true
      - --ca-cert=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      - --proxy-server-port=8132
      - --admin-server-port=8133
      - --health-server-port=8134
      - --keepalive-time=5m
      - --service-account-token-path=/var/run/secrets/tokens/konnectivity-agent-token
      - --proxy-server-host=ca54c9b1-1f4a-498d-9a4d-af31964c74a9.k8s.ondigitalocean.com
      command:
      - /proxy-agent
      image: ghcr.io/digitalocean-packages/kas-network-proxy/proxy-agent:v0.33.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8134
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: konnectivity-agent
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/tokens
        name: konnectivity-agent-token
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7jlqj
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: konnectivity-agent
    serviceAccountName: konnectivity-agent
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: konnectivity-agent-token
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            audience: system:konnectivity-server
            expirationSeconds: 3600
            path: konnectivity-agent-token
    - name: kube-api-access-7jlqj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:54Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:50Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:54Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:54Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T07:55:50Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3d42a952d51208b03aad7be91c81b0ad279459a20996aff1be604bb71b919ef7
      image: ghcr.io/digitalocean-packages/kas-network-proxy/proxy-agent:v0.33.0
      imageID: ghcr.io/digitalocean-packages/kas-network-proxy/proxy-agent@sha256:0d1b560759bcad02cc09b3d5cf2bb4ba4a6ec1af61f9884dd62b398c09a56668
      lastState: {}
      name: konnectivity-agent
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T07:55:54Z"
      volumeMounts:
      - mountPath: /var/run/secrets/tokens
        name: konnectivity-agent-token
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7jlqj
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.3
    hostIPs:
    - ip: 10.120.0.3
    observedGeneration: 1
    phase: Running
    podIP: 10.108.0.145
    podIPs:
    - ip: 10.108.0.145
    qosClass: BestEffort
    startTime: "2025-12-18T07:55:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-02-18T21:05:00Z"
    generateName: matomo-archive-29524145-
    generation: 1
    labels:
      app.kubernetes.io/component: archive
      app.kubernetes.io/instance: matomo
      app.kubernetes.io/name: matomo
      batch.kubernetes.io/controller-uid: 4a5c42b2-a0a7-4228-a25f-85bb4069951b
      batch.kubernetes.io/job-name: matomo-archive-29524145
      controller-uid: 4a5c42b2-a0a7-4228-a25f-85bb4069951b
      job-name: matomo-archive-29524145
    name: matomo-archive-29524145-wq9kl
    namespace: matomo
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: matomo-archive-29524145
      uid: 4a5c42b2-a0a7-4228-a25f-85bb4069951b
    resourceVersion: "19602464"
    uid: 8952a3d5-d968-46b3-8255-88b739a97c17
  spec:
    automountServiceAccountToken: false
    containers:
    - command:
      - /bin/sh
      - -c
      - |
        echo "[$(date)] Starting Matomo archive processing via HTTP..."

        # Wait for Matomo service to be ready
        echo "Checking Matomo service availability..."
        for i in $(seq 1 30); do
          if curl -f -s "http://matomo.matomo.svc.cluster.local" > /dev/null 2>&1; then
            echo "Γ£ô Matomo service is ready"
            break
          fi
          echo "Waiting for Matomo service... attempt $i/30"
          sleep 10
        done

        # Trigger archive processing via HTTP (no volume mount needed)
        echo "Triggering archive via HTTP endpoint..."
        ARCHIVE_URL="http://matomo.matomo.svc.cluster.local/misc/cron/archive.php?token=${ARCHIVE_TOKEN}"

        if curl -f -s "$ARCHIVE_URL" -o /tmp/archive_output.txt; then
          echo "Γ£ô Archive processing completed successfully"
          cat /tmp/archive_output.txt
          echo ""
          echo "[$(date)] Archive processing completed"
          exit 0
        else
          echo "ΓÜá∩╕Å Archive processing failed"
          cat /tmp/archive_output.txt 2>/dev/null || echo "No output"
          exit 1
        fi
      env:
      - name: MATOMO_DATABASE_HOST
        value: matomo-mariadb
      - name: MATOMO_DATABASE_USERNAME
        value: matomo
      - name: MATOMO_DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: mariadb-password
            name: matomo-mariadb
      - name: MATOMO_DATABASE_DBNAME
        value: matomo
      - name: ARCHIVE_TOKEN
        valueFrom:
          secretKeyRef:
            key: archive-token
            name: matomo
      image: docker.io/matomo:5.5.1-apache
      imagePullPolicy: IfNotPresent
      name: archive
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k0
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 33
      runAsGroup: 33
      runAsNonRoot: true
      runAsUser: 33
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: matomo
    serviceAccountName: matomo
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-18T21:05:03Z"
      observedGeneration: 1
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-18T21:05:00Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-18T21:05:00Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-18T21:05:00Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-18T21:05:00Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 50m
        memory: 128Mi
      containerID: containerd://27193e96e3577dadf6958114359a5e3826526a588117fa21c59d04015f70ec8a
      image: docker.io/library/matomo:5.5.1-apache
      imageID: docker.io/library/matomo@sha256:2ff9f2850f2900df15a729fe1b19d1f9c03db08ed20f626f68283199ca29b471
      lastState: {}
      name: archive
      ready: false
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://27193e96e3577dadf6958114359a5e3826526a588117fa21c59d04015f70ec8a
          exitCode: 0
          finishedAt: "2026-02-18T21:05:01Z"
          reason: Completed
          startedAt: "2026-02-18T21:05:01Z"
      volumeMounts:
      - mountPath: /tmp
        name: tmp
    hostIP: 10.120.0.4
    hostIPs:
    - ip: 10.120.0.4
    observedGeneration: 1
    phase: Succeeded
    podIP: 10.108.0.44
    podIPs:
    - ip: 10.108.0.44
    qosClass: Burstable
    startTime: "2026-02-18T21:05:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-12-18T11:20:31Z"
    generateName: matomo-b7d497c78-
    generation: 1
    labels:
      app.kubernetes.io/component: web
      app.kubernetes.io/instance: matomo
      app.kubernetes.io/name: matomo
      pod-template-hash: b7d497c78
    name: matomo-b7d497c78-h6bl2
    namespace: matomo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: matomo-b7d497c78
      uid: 6096c450-7cd2-4d49-b4f6-485bdfe29a6d
    resourceVersion: "44039"
    uid: 3ef28e29-591c-4a3a-a6aa-d4a82e4d1e05
  spec:
    containers:
    - env:
      - name: MATOMO_DATABASE_HOST
        value: matomo-mariadb
      - name: MATOMO_DATABASE_USERNAME
        value: matomo
      - name: MATOMO_DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: mariadb-password
            name: matomo-mariadb
      - name: MATOMO_DATABASE_DBNAME
        value: matomo
      - name: MATOMO_ENABLE_BROWSER_ARCHIVING_TRIGGERING
        value: "0"
      - name: MATOMO_ARCHIVING_RANGE_FORCE_ON_BROWSER_REQUEST
        value: "0"
      - name: MATOMO_FORCE_SSL
        value: "1"
      - name: MATOMO_FORCE_SSL_REDIRECT
        value: "1"
      - name: MATOMO_DATABASE_TABLES_PREFIX
        value: matomo_
      - name: MATOMO_DATABASE_ADAPTER
        value: PDO\MYSQL
      - name: MATOMO_GENERAL_FORCE_SSL
        value: "1"
      - name: MATOMO_GENERAL_ENABLE_BROWSER_ARCHIVING_TRIGGERING
        value: "0"
      - name: MATOMO_GENERAL_BROWSER_ARCHIVING_DISABLED_ENFORCE
        value: "1"
      - name: MATOMO_TRUSTED_HOSTS
      image: docker.io/matomo:5.5.1-apache
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        initialDelaySeconds: 60
        periodSeconds: 30
        successThreshold: 1
        tcpSocket:
          port: 80
        timeoutSeconds: 5
      name: matomo
      ports:
      - containerPort: 80
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 80
        timeoutSeconds: 3
      resources:
        limits:
          cpu: 500m
          ephemeral-storage: 1Gi
          memory: 1Gi
        requests:
          cpu: 100m
          ephemeral-storage: 1Gi
          memory: 256Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/www/html
        name: matomo-data
      - mountPath: /tmp
        name: tmp-volume
      - mountPath: /usr/local/etc/php/conf.d/99-performance.ini
        name: php-config
        subPath: php-performance.ini
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n4mlt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - /bin/sh
      - -c
      - |
        echo "Fixing Matomo configuration..."

        # Wait for config file to exist (in case it's generated during startup)
        for i in $(seq 1 30); do
          if [ -f /var/www/html/config/config.ini.php ]; then
            break
          fi
          echo "Waiting for config file... attempt $i/30"
          sleep 2
        done

        # Clean up any old backup files that might cause integrity issues
        rm -f /var/www/html/config/config.ini.php.backup

        # Ensure critical settings are in place
        CONFIG_FILE="/var/www/html/config/config.ini.php"

        # Add force_ssl = 1 if not present
        if ! grep -q "force_ssl = 1" "$CONFIG_FILE" 2>/dev/null; then
          echo "Adding force_ssl = 1 to config..."
          sed -i '/\[General\]/a force_ssl = 1' "$CONFIG_FILE" || true
        fi

        # Ensure browser archiving is disabled
        if ! grep -q "enable_browser_archiving_triggering = 0" "$CONFIG_FILE" 2>/dev/null; then
          echo "Adding browser archiving disable to config..."
          sed -i '/\[General\]/a enable_browser_archiving_triggering = 0' "$CONFIG_FILE" || true
        fi

        if ! grep -q "archiving_range_force_on_browser_request = 0" "$CONFIG_FILE" 2>/dev/null; then
          echo "Adding archiving range disable to config..."
          sed -i '/\[General\]/a archiving_range_force_on_browser_request = 0' "$CONFIG_FILE" || true
        fi

        # Fix the root cause: cache directory ownership issues
        echo "Fixing cache directory permissions (root cause of Oops errors)..."

        # Ensure all cache directories are owned by www-data
        chown -R www-data:www-data /var/www/html/tmp/ || true
        chown -R www-data:www-data /var/www/html/tmp/templates_c/ || true
        chown -R www-data:www-data /var/www/html/tmp/cache/ || true

        # Set proper permissions for cache writing
        chmod -R 755 /var/www/html/tmp/ || true
        chmod -R 775 /var/www/html/tmp/templates_c/ || true
        chmod -R 775 /var/www/html/tmp/cache/ || true

        echo "Γ£ô Cache directory permissions fixed for www-data"

        # Fix directory privacy issues by creating proper .htaccess
        echo "Fixing tmp directory privacy..."
        cat > /var/www/html/tmp/.htaccess << 'HTACCESS_END'
        # Block all access to tmp directory for security
        <Files "*">
            Require all denied
        </Files>
        # Block directory listing
        Options -Indexes
        HTACCESS_END

        # Fix Apache AllowOverride to enable .htaccess files properly
        echo "Fixing Apache AllowOverride configuration..."
        if ! grep -q "AllowOverride All" /etc/apache2/apache2.conf; then
          # Replace AllowOverride None with AllowOverride All for /var/www/ directory
          sed -i '/^<Directory \/var\/www\/>/,/^<\/Directory>/ s/AllowOverride None/AllowOverride All/' /etc/apache2/apache2.conf
          echo "Γ£ô Apache AllowOverride enabled for .htaccess processing"
        else
          echo "Γ£ô Apache AllowOverride already enabled"
        fi

        echo "Configuration fixes applied!"
        grep -A10 "\[General\]" "$CONFIG_FILE" | head -15
      image: docker.io/matomo:5.5.1-apache
      imagePullPolicy: IfNotPresent
      name: config-fix
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/www/html
        name: matomo-data
      - mountPath: /tmp
        name: tmp-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n4mlt
        readOnly: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: matomo-data
      persistentVolumeClaim:
        claimName: matomo
    - emptyDir: {}
      name: tmp-volume
    - configMap:
        defaultMode: 420
        name: matomo-config
      name: php-config
    - name: kube-api-access-n4mlt
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T11:20:32Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T11:21:32Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T11:22:04Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T11:22:04Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T11:20:31Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        ephemeral-storage: 1Gi
        memory: 256Mi
      containerID: containerd://e36d462ae17185ba85124c80573ca29211ce258580e8fbd9b14b6437a86763f9
      image: docker.io/library/matomo:5.5.1-apache
      imageID: docker.io/library/matomo@sha256:2ff9f2850f2900df15a729fe1b19d1f9c03db08ed20f626f68283199ca29b471
      lastState: {}
      name: matomo
      ready: true
      resources:
        limits:
          cpu: 500m
          ephemeral-storage: 1Gi
          memory: 1Gi
        requests:
          cpu: 100m
          ephemeral-storage: 1Gi
          memory: 256Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T11:21:33Z"
      volumeMounts:
      - mountPath: /var/www/html
        name: matomo-data
      - mountPath: /tmp
        name: tmp-volume
      - mountPath: /usr/local/etc/php/conf.d/99-performance.ini
        name: php-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n4mlt
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.3
    hostIPs:
    - ip: 10.120.0.3
    initContainerStatuses:
    - containerID: containerd://59fdd3f2c2cd01f3c330924912c512e2effed8145c89cf3310778c24011b2a12
      image: docker.io/library/matomo:5.5.1-apache
      imageID: docker.io/library/matomo@sha256:2ff9f2850f2900df15a729fe1b19d1f9c03db08ed20f626f68283199ca29b471
      lastState: {}
      name: config-fix
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://59fdd3f2c2cd01f3c330924912c512e2effed8145c89cf3310778c24011b2a12
          exitCode: 0
          finishedAt: "2025-12-18T11:21:31Z"
          reason: Completed
          startedAt: "2025-12-18T11:20:31Z"
      volumeMounts:
      - mountPath: /var/www/html
        name: matomo-data
      - mountPath: /tmp
        name: tmp-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n4mlt
        readOnly: true
        recursiveReadOnly: Disabled
    observedGeneration: 1
    phase: Running
    podIP: 10.108.0.245
    podIPs:
    - ip: 10.108.0.245
    qosClass: Burstable
    startTime: "2025-12-18T11:20:31Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-12-18T11:20:32Z"
    generateName: matomo-mariadb-
    generation: 1
    labels:
      app.kubernetes.io/component: mariadb
      app.kubernetes.io/instance: matomo
      app.kubernetes.io/name: matomo
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: matomo-mariadb-66868f54b8
      statefulset.kubernetes.io/pod-name: matomo-mariadb-0
    name: matomo-mariadb-0
    namespace: matomo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: matomo-mariadb
      uid: 8d9a3e4a-b1b8-48c8-8dab-3b7741c200a5
    resourceVersion: "43777"
    uid: 298f0059-5880-4ccf-bb3b-23b3858dc93e
  spec:
    containers:
    - args:
      - --max-allowed-packet=128M
      - --innodb-buffer-pool-size=256M
      - --innodb-log-file-size=64M
      env:
      - name: MYSQL_ROOT_PASSWORD
        valueFrom:
          secretKeyRef:
            key: mariadb-root-password
            name: matomo-mariadb
      - name: MYSQL_DATABASE
        value: matomo
      - name: MYSQL_USER
        value: matomo
      - name: MYSQL_PASSWORD
        valueFrom:
          secretKeyRef:
            key: mariadb-password
            name: matomo-mariadb
      image: mariadb:12.0.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 3306
        timeoutSeconds: 3
      name: mariadb
      ports:
      - containerPort: 3306
        name: mysql
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        initialDelaySeconds: 15
        periodSeconds: 5
        successThreshold: 1
        tcpSocket:
          port: 3306
        timeoutSeconds: 3
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsGroup: 999
        runAsNonRoot: true
        runAsUser: 999
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/mysql
        name: data
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jsn6m
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: matomo-mariadb-0
    nodeName: ptoken-assets--pool-525qi3gh9-s44k0
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 999
      runAsGroup: 999
      runAsNonRoot: true
      runAsUser: 999
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: default
    serviceAccountName: default
    subdomain: matomo-mariadb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-matomo-mariadb-0
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-jsn6m
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T11:20:36Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T11:20:32Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T11:20:53Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T11:20:53Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T11:20:32Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 256Mi
      containerID: containerd://600b39c76592f1cb5be38e11d4f353d9716770916ede21a23023a86aa8292c2f
      image: docker.io/library/mariadb:12.0.2
      imageID: docker.io/library/mariadb@sha256:607835cd628b78e2876f6a586d0ec37b296c47683b31ef750002d3d17d3d8f7a
      lastState: {}
      name: mariadb
      ready: true
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 256Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T11:20:36Z"
      volumeMounts:
      - mountPath: /var/lib/mysql
        name: data
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jsn6m
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.4
    hostIPs:
    - ip: 10.120.0.4
    observedGeneration: 1
    phase: Running
    podIP: 10.108.0.24
    podIPs:
    - ip: 10.108.0.24
    qosClass: Burstable
    startTime: "2025-12-18T11:20:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: a9cd1163d962b903539d6f4ecb193cef6ac0c4f42893a6456abed30147e0217c
      checksum/secret: 7c110ea1b6b37ea330d0154a625a43dcc9862398abea631cb35cfe66884e430c
    creationTimestamp: "2026-01-01T22:28:52Z"
    generateName: n8n-77bc44fb4-
    generation: 1
    labels:
      app.kubernetes.io/instance: n8n
      app.kubernetes.io/name: n8n
      component: main
      pod-template-hash: 77bc44fb4
    name: n8n-77bc44fb4-4hkgh
    namespace: n8n
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: n8n-77bc44fb4
      uid: 1663d2e7-f5d5-436a-837e-a8ee2de836ec
    resourceVersion: "4577077"
    uid: 7f6575e8-a1ee-434e-9734-f5d252c191e2
  spec:
    automountServiceAccountToken: false
    containers:
    - command:
      - n8n
      - start
      env:
      - name: DB_SQLITE_VACUUM_ON_STARTUP
        valueFrom:
          configMapKeyRef:
            key: DB_SQLITE_VACUUM_ON_STARTUP
            name: n8n
      - name: EXECUTIONS_DATA_MAX_AGE
        valueFrom:
          configMapKeyRef:
            key: EXECUTIONS_DATA_MAX_AGE
            name: n8n
      - name: EXECUTIONS_DATA_PRUNE
        valueFrom:
          configMapKeyRef:
            key: EXECUTIONS_DATA_PRUNE
            name: n8n
      - name: EXECUTIONS_DATA_SAVE_MANUAL_EXECUTIONS
        valueFrom:
          configMapKeyRef:
            key: EXECUTIONS_DATA_SAVE_MANUAL_EXECUTIONS
            name: n8n
      - name: EXECUTIONS_DATA_SAVE_ON_ERROR
        valueFrom:
          configMapKeyRef:
            key: EXECUTIONS_DATA_SAVE_ON_ERROR
            name: n8n
      - name: EXECUTIONS_DATA_SAVE_ON_SUCCESS
        valueFrom:
          configMapKeyRef:
            key: EXECUTIONS_DATA_SAVE_ON_SUCCESS
            name: n8n
      - name: EXECUTIONS_MODE
        valueFrom:
          configMapKeyRef:
            key: EXECUTIONS_MODE
            name: n8n
      - name: EXECUTIONS_PROCESS
        valueFrom:
          configMapKeyRef:
            key: EXECUTIONS_PROCESS
            name: n8n
      - name: N8N_BINARY_DATA_MODE
        valueFrom:
          configMapKeyRef:
            key: N8N_BINARY_DATA_MODE
            name: n8n
      - name: N8N_BINARY_DATA_TTL
        valueFrom:
          configMapKeyRef:
            key: N8N_BINARY_DATA_TTL
            name: n8n
      - name: N8N_CONCURRENCY
        valueFrom:
          configMapKeyRef:
            key: N8N_CONCURRENCY
            name: n8n
      - name: N8N_DISABLE_PRODUCTION_MAIN_PROCESS
        valueFrom:
          configMapKeyRef:
            key: N8N_DISABLE_PRODUCTION_MAIN_PROCESS
            name: n8n
      - name: N8N_HOST
        valueFrom:
          configMapKeyRef:
            key: N8N_HOST
            name: n8n
      - name: N8N_MAX_FILE_SIZE
        valueFrom:
          configMapKeyRef:
            key: N8N_MAX_FILE_SIZE
            name: n8n
      - name: N8N_METRICS
        valueFrom:
          configMapKeyRef:
            key: N8N_METRICS
            name: n8n
      - name: N8N_PORT
        valueFrom:
          configMapKeyRef:
            key: N8N_PORT
            name: n8n
      - name: N8N_PROTOCOL
        valueFrom:
          configMapKeyRef:
            key: N8N_PROTOCOL
            name: n8n
      - name: N8N_SECURE_COOKIE
        valueFrom:
          configMapKeyRef:
            key: N8N_SECURE_COOKIE
            name: n8n
      - name: NODE_FUNCTION_ALLOW_BUILTIN
        valueFrom:
          configMapKeyRef:
            key: NODE_FUNCTION_ALLOW_BUILTIN
            name: n8n
      - name: NODE_FUNCTION_ALLOW_EXTERNAL
        valueFrom:
          configMapKeyRef:
            key: NODE_FUNCTION_ALLOW_EXTERNAL
            name: n8n
      - name: QUEUE_BULL_REDIS_HOST
        valueFrom:
          configMapKeyRef:
            key: QUEUE_BULL_REDIS_HOST
            name: n8n
      - name: QUEUE_BULL_REDIS_PASSWORD
        valueFrom:
          configMapKeyRef:
            key: QUEUE_BULL_REDIS_PASSWORD
            name: n8n
      - name: QUEUE_BULL_REDIS_PORT
        valueFrom:
          configMapKeyRef:
            key: QUEUE_BULL_REDIS_PORT
            name: n8n
      - name: QUEUE_HEALTH_CHECK_ACTIVE
        valueFrom:
          configMapKeyRef:
            key: QUEUE_HEALTH_CHECK_ACTIVE
            name: n8n
      - name: WEBHOOK_URL
        valueFrom:
          configMapKeyRef:
            key: WEBHOOK_URL
            name: n8n
      - name: N8N_ENCRYPTION_KEY
        valueFrom:
          secretKeyRef:
            key: N8N_ENCRYPTION_KEY
            name: n8n
      - name: DB_TYPE
        value: sqlite
      - name: DB_SQLITE_DATABASE
        value: /home/node/.n8n/database.sqlite
      image: n8nio/n8n:2.1.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 5678
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: n8n
      ports:
      - containerPort: 5678
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 5678
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 500m
          ephemeral-storage: 1Gi
          memory: 1Gi
        requests:
          cpu: 100m
          ephemeral-storage: 1Gi
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /home/node/.n8n
        name: n8n-data
      - mountPath: /tmp
        name: tmp
      - mountPath: /home/node
        name: home
      - mountPath: /home/node/.n8n/custom
        name: custom-extensions
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: n8n
    serviceAccountName: n8n
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: n8n-data
      persistentVolumeClaim:
        claimName: n8n-data
    - emptyDir: {}
      name: tmp
    - emptyDir: {}
      name: home
    - emptyDir: {}
      name: custom-extensions
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-01T22:29:47Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-01T22:28:53Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-01T22:30:29Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-01T22:30:29Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-01T22:28:53Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        ephemeral-storage: 1Gi
        memory: 256Mi
      containerID: containerd://61b3fee21daf032325f455c4214c1ea0f019d6d9a0453103407970a1f9be1d9e
      image: docker.io/n8nio/n8n:2.1.4
      imageID: docker.io/n8nio/n8n@sha256:85214df20cd7bc020f8e4b0f60f87ea87f0a754ca7ba3d1ccdfc503ccd6e7f9c
      lastState: {}
      name: n8n
      ready: true
      resources:
        limits:
          cpu: 500m
          ephemeral-storage: 1Gi
          memory: 1Gi
        requests:
          cpu: 100m
          ephemeral-storage: 1Gi
          memory: 256Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-01T22:29:46Z"
      volumeMounts:
      - mountPath: /home/node/.n8n
        name: n8n-data
      - mountPath: /tmp
        name: tmp
      - mountPath: /home/node
        name: home
      - mountPath: /home/node/.n8n/custom
        name: custom-extensions
    hostIP: 10.120.0.3
    hostIPs:
    - ip: 10.120.0.3
    observedGeneration: 1
    phase: Running
    podIP: 10.108.0.199
    podIPs:
    - ip: 10.108.0.199
    qosClass: Burstable
    startTime: "2026-01-01T22:28:53Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-02-16T02:00:00Z"
    generateName: n8n-backup-29520120-
    generation: 1
    labels:
      app.kubernetes.io/instance: n8n
      app.kubernetes.io/name: n8n
      batch.kubernetes.io/controller-uid: 2ebb48a1-13ca-41ff-842a-8ea9dc7e4d9c
      batch.kubernetes.io/job-name: n8n-backup-29520120
      component: backup
      controller-uid: 2ebb48a1-13ca-41ff-842a-8ea9dc7e4d9c
      job-name: n8n-backup-29520120
    name: n8n-backup-29520120-lsdvn
    namespace: n8n
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: n8n-backup-29520120
      uid: 2ebb48a1-13ca-41ff-842a-8ea9dc7e4d9c
    resourceVersion: "18726445"
    uid: 92f17a1b-c147-4cce-bc5b-325c6dca69ad
  spec:
    affinity:
      podAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - n8n
            - key: component
              operator: In
              values:
              - main
          topologyKey: kubernetes.io/hostname
    containers:
    - command:
      - /bin/sh
      - -c
      - |
        echo "Starting n8n backup at $(date)"

        # Create backup directory
        mkdir -p /backup/$(date +%Y%m%d_%H%M%S)

        # Copy n8n data
        if [ -d "/data" ]; then
          cp -r /data/* /backup/$(date +%Y%m%d_%H%M%S)/ || true
          echo "Backup completed successfully at $(date)"
        else
          echo "No data directory found"
        fi

        # Cleanup old backups (keep last 7 days)
        find /backup -type d -name "20*" -mtime +7 -exec rm -rf {} + || true

        echo "Backup job finished at $(date)"
      image: busybox:1.36
      imagePullPolicy: IfNotPresent
      name: backup
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 10m
          memory: 32Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: n8n-data
      - mountPath: /backup
        name: backup-storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bns6t
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: n8n-data
      persistentVolumeClaim:
        claimName: n8n-data
    - name: backup-storage
      persistentVolumeClaim:
        claimName: n8n-backup
    - name: kube-api-access-bns6t
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-16T02:00:15Z"
      observedGeneration: 1
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-16T02:00:00Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-16T02:00:13Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-16T02:00:13Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-16T02:00:00Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 10m
        memory: 32Mi
      containerID: containerd://0d76c28806252ee7f2da935d64fe9c4af9f7cbe8c40d9fb60f2dd091595f1218
      image: docker.io/library/busybox:1.36
      imageID: docker.io/library/busybox@sha256:6b219909078e3fc93b81f83cb438bd7a5457984a01a478c76fe9777a8c67c39e
      lastState: {}
      name: backup
      ready: false
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 10m
          memory: 32Mi
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://0d76c28806252ee7f2da935d64fe9c4af9f7cbe8c40d9fb60f2dd091595f1218
          exitCode: 0
          finishedAt: "2026-02-16T02:00:13Z"
          reason: Completed
          startedAt: "2026-02-16T02:00:12Z"
      volumeMounts:
      - mountPath: /data
        name: n8n-data
      - mountPath: /backup
        name: backup-storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bns6t
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.3
    hostIPs:
    - ip: 10.120.0.3
    observedGeneration: 1
    phase: Succeeded
    podIP: 10.108.0.215
    podIPs:
    - ip: 10.108.0.215
    qosClass: Burstable
    startTime: "2026-02-16T02:00:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-02-17T02:00:00Z"
    generateName: n8n-backup-29521560-
    generation: 1
    labels:
      app.kubernetes.io/instance: n8n
      app.kubernetes.io/name: n8n
      batch.kubernetes.io/controller-uid: 2edbd3f4-fe8c-4dfe-8837-335465ae1c71
      batch.kubernetes.io/job-name: n8n-backup-29521560
      component: backup
      controller-uid: 2edbd3f4-fe8c-4dfe-8837-335465ae1c71
      job-name: n8n-backup-29521560
    name: n8n-backup-29521560-8ffpt
    namespace: n8n
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: n8n-backup-29521560
      uid: 2edbd3f4-fe8c-4dfe-8837-335465ae1c71
    resourceVersion: "19040072"
    uid: 19395e11-3139-499e-af5d-ace23c9fd65e
  spec:
    affinity:
      podAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - n8n
            - key: component
              operator: In
              values:
              - main
          topologyKey: kubernetes.io/hostname
    containers:
    - command:
      - /bin/sh
      - -c
      - |
        echo "Starting n8n backup at $(date)"

        # Create backup directory
        mkdir -p /backup/$(date +%Y%m%d_%H%M%S)

        # Copy n8n data
        if [ -d "/data" ]; then
          cp -r /data/* /backup/$(date +%Y%m%d_%H%M%S)/ || true
          echo "Backup completed successfully at $(date)"
        else
          echo "No data directory found"
        fi

        # Cleanup old backups (keep last 7 days)
        find /backup -type d -name "20*" -mtime +7 -exec rm -rf {} + || true

        echo "Backup job finished at $(date)"
      image: busybox:1.36
      imagePullPolicy: IfNotPresent
      name: backup
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 10m
          memory: 32Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: n8n-data
      - mountPath: /backup
        name: backup-storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-85t7m
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: n8n-data
      persistentVolumeClaim:
        claimName: n8n-data
    - name: backup-storage
      persistentVolumeClaim:
        claimName: n8n-backup
    - name: kube-api-access-85t7m
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-17T02:00:15Z"
      observedGeneration: 1
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-17T02:00:00Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-17T02:00:13Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-17T02:00:13Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-17T02:00:00Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 10m
        memory: 32Mi
      containerID: containerd://a95b7cddf5f8546d08733c197a3fe6f49901a9ace7ac8eaa6966d3f458c67246
      image: docker.io/library/busybox:1.36
      imageID: docker.io/library/busybox@sha256:6b219909078e3fc93b81f83cb438bd7a5457984a01a478c76fe9777a8c67c39e
      lastState: {}
      name: backup
      ready: false
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 10m
          memory: 32Mi
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://a95b7cddf5f8546d08733c197a3fe6f49901a9ace7ac8eaa6966d3f458c67246
          exitCode: 0
          finishedAt: "2026-02-17T02:00:13Z"
          reason: Completed
          startedAt: "2026-02-17T02:00:12Z"
      volumeMounts:
      - mountPath: /data
        name: n8n-data
      - mountPath: /backup
        name: backup-storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-85t7m
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.3
    hostIPs:
    - ip: 10.120.0.3
    observedGeneration: 1
    phase: Succeeded
    podIP: 10.108.0.163
    podIPs:
    - ip: 10.108.0.163
    qosClass: Burstable
    startTime: "2026-02-17T02:00:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-02-18T02:00:00Z"
    generateName: n8n-backup-29523000-
    generation: 1
    labels:
      app.kubernetes.io/instance: n8n
      app.kubernetes.io/name: n8n
      batch.kubernetes.io/controller-uid: c490d15f-3577-4b81-a38e-0ed21c99d103
      batch.kubernetes.io/job-name: n8n-backup-29523000
      component: backup
      controller-uid: c490d15f-3577-4b81-a38e-0ed21c99d103
      job-name: n8n-backup-29523000
    name: n8n-backup-29523000-bksl5
    namespace: n8n
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: n8n-backup-29523000
      uid: c490d15f-3577-4b81-a38e-0ed21c99d103
    resourceVersion: "19353389"
    uid: 53de5a08-12e7-4a5b-9f1a-14e0c8043bf7
  spec:
    affinity:
      podAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - n8n
            - key: component
              operator: In
              values:
              - main
          topologyKey: kubernetes.io/hostname
    containers:
    - command:
      - /bin/sh
      - -c
      - |
        echo "Starting n8n backup at $(date)"

        # Create backup directory
        mkdir -p /backup/$(date +%Y%m%d_%H%M%S)

        # Copy n8n data
        if [ -d "/data" ]; then
          cp -r /data/* /backup/$(date +%Y%m%d_%H%M%S)/ || true
          echo "Backup completed successfully at $(date)"
        else
          echo "No data directory found"
        fi

        # Cleanup old backups (keep last 7 days)
        find /backup -type d -name "20*" -mtime +7 -exec rm -rf {} + || true

        echo "Backup job finished at $(date)"
      image: busybox:1.36
      imagePullPolicy: IfNotPresent
      name: backup
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 10m
          memory: 32Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: n8n-data
      - mountPath: /backup
        name: backup-storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rczhh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: n8n-data
      persistentVolumeClaim:
        claimName: n8n-data
    - name: backup-storage
      persistentVolumeClaim:
        claimName: n8n-backup
    - name: kube-api-access-rczhh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-18T02:00:12Z"
      observedGeneration: 1
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-18T02:00:00Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-18T02:00:11Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-18T02:00:11Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-18T02:00:00Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 10m
        memory: 32Mi
      containerID: containerd://4cdc89ababeb6e98ac7ec7169067837e4c6c9f3db09bb6699e2a2e9e3e9e4b7d
      image: docker.io/library/busybox:1.36
      imageID: docker.io/library/busybox@sha256:6b219909078e3fc93b81f83cb438bd7a5457984a01a478c76fe9777a8c67c39e
      lastState: {}
      name: backup
      ready: false
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 10m
          memory: 32Mi
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://4cdc89ababeb6e98ac7ec7169067837e4c6c9f3db09bb6699e2a2e9e3e9e4b7d
          exitCode: 0
          finishedAt: "2026-02-18T02:00:10Z"
          reason: Completed
          startedAt: "2026-02-18T02:00:10Z"
      volumeMounts:
      - mountPath: /data
        name: n8n-data
      - mountPath: /backup
        name: backup-storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rczhh
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.3
    hostIPs:
    - ip: 10.120.0.3
    observedGeneration: 1
    phase: Succeeded
    podIP: 10.108.0.252
    podIPs:
    - ip: 10.108.0.252
    qosClass: Burstable
    startTime: "2026-02-18T02:00:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/configmap: c0245934eef0cacc7d5d0b534389fb64ec1dc6d03d2dd13adac301678590e257
      checksum/mu-plugins: b53395ea900f6b43591031d16e7125384322c5e440a6b0745e9b564e4da741e0
      checksum/secret: 9e9a0b6dd3425ea6cf0db1e203856611ed1943360e717afc3e6cb92eacc94076
    creationTimestamp: "2026-01-02T22:35:55Z"
    generateName: wordpress-54fd6bfbc5-
    generation: 1
    labels:
      app.kubernetes.io/component: wordpress
      app.kubernetes.io/instance: wordpress
      app.kubernetes.io/name: wordpress
      pod-template-hash: 54fd6bfbc5
    name: wordpress-54fd6bfbc5-4t5m6
    namespace: wordpress
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: wordpress-54fd6bfbc5
      uid: 5151a925-bd00-46a5-8a89-e1216843fbf8
    resourceVersion: "4892322"
    uid: 8e8aed59-f739-4716-8095-2a59650e9e5d
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: wordpress
                app.kubernetes.io/instance: wordpress
                app.kubernetes.io/name: wordpress
            topologyKey: kubernetes.io/hostname
          weight: 100
    automountServiceAccountToken: false
    containers:
    - env:
      - name: WORDPRESS_DB_HOST
        value: wordpress-mariadb
      - name: WORDPRESS_DB_PORT
        value: "3306"
      - name: WORDPRESS_DB_NAME
        value: wordpress
      - name: WORDPRESS_DB_USER
        value: wordpress
      - name: WORDPRESS_DB_PASSWORD
        valueFrom:
          secretKeyRef:
            key: mariadb-password
            name: wordpress
      - name: WORDPRESS_USERNAME
      - name: WORDPRESS_PASSWORD
        valueFrom:
          secretKeyRef:
            key: wordpress-password
            name: wordpress
      - name: WORDPRESS_EMAIL
      - name: WORDPRESS_FIRST_NAME
        value: Admin
      - name: WORDPRESS_LAST_NAME
        value: User
      - name: WORDPRESS_BLOG_NAME
        value: Enterprise WordPress Site
      - name: WORDPRESS_SCHEME
        value: https
      - name: WORDPRESS_TABLE_PREFIX
        value: wp_
      - name: WORDPRESS_HTACCESS_OVERRIDE_NONE
        value: "no"
      - name: WORDPRESS_ENABLE_HTACCESS_PERSISTENCE
        value: "yes"
      - name: WORDPRESS_AUTO_UPDATE_LEVEL
        value: minor
      - name: WORDPRESS_ENABLE_REVERSE_PROXY
        value: "yes"
      - name: WORDPRESS_CONFIG_EXTRA
        value: |
          define('WP_MEMORY_LIMIT', '256M');
          define('WP_MAX_MEMORY_LIMIT', '512M');

          // WordPress configuration
          define('WP_DEBUG', false);
          define('WP_DEBUG_LOG', false);
          define('WP_DEBUG_DISPLAY', false);
          define('AUTOMATIC_UPDATER_DISABLED', false);
          define('WP_AUTO_UPDATE_CORE', 'minor');
          define('DISALLOW_FILE_EDIT', true);
          define('FORCE_SSL_ADMIN', true);
          define('WP_MEMORY_LIMIT', '256M');

          // Disable WP-Cron (handled by Kubernetes CronJob)
          define('DISABLE_WP_CRON', true);

          // Performance optimizations
          define('WP_CACHE', true);
          define('WP_POST_REVISIONS', 3);
          define('AUTOSAVE_INTERVAL', 300);
          define('EMPTY_TRASH_DAYS', 7);

          // Security hardening
          ini_set('session.cookie_httponly', true);
          ini_set('session.cookie_secure', true);
          ini_set('session.use_only_cookies', true);

          // Enable object caching
          define('WP_CACHE_KEY_SALT', 'wordpress_cache_');

          // SMTP Configuration for reliable email delivery
          define('SMTP_FROM_EMAIL', 'dhruv@weown.email');
          define('SMTP_FROM_NAME', 'YOUR_NAME');

          // Enable WordPress mail debug logging
          define('WP_MAIL_DEBUG', false);


          // WordPress security keys (auto-generated)
          define('AUTH_KEY', '');
          define('SECURE_AUTH_KEY', '');
          define('LOGGED_IN_KEY', '');
          define('NONCE_KEY', '');
          define('AUTH_SALT', '');
          define('SECURE_AUTH_SALT', '');
          define('LOGGED_IN_SALT', '');
          define('NONCE_SALT', '');
      - name: WORDPRESS_ENABLE_REDIS
        value: "yes"
      - name: REDIS_HOST
        value: wordpress-redis-master
      - name: REDIS_PORT
        value: "6379"
      - name: REDIS_PASSWORD
        valueFrom:
          secretKeyRef:
            key: redis-password
            name: wordpress
      image: docker.io/wordpress:6.8.3-php8.3-apache
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        initialDelaySeconds: 120
        periodSeconds: 30
        successThreshold: 1
        tcpSocket:
          port: 80
        timeoutSeconds: 10
      name: wordpress
      ports:
      - containerPort: 80
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 10
        initialDelaySeconds: 60
        periodSeconds: 15
        successThreshold: 1
        tcpSocket:
          port: 80
        timeoutSeconds: 10
      resources:
        limits:
          cpu: 800m
          ephemeral-storage: 800Mi
          memory: 1Gi
        requests:
          cpu: 50m
          ephemeral-storage: 200Mi
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: false
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/www/html
        name: wordpress-core
      - mountPath: /var/www/html/wp-content
        name: wordpress-content
      - mountPath: /var/www/html/wp-config-custom
        name: wordpress-config
      - mountPath: /var/www/html/wp-content/mu-plugins
        name: mu-plugins
      - mountPath: /var/cache/wordpress
        name: wordpress-cache
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/log/apache2
        name: apache-logs
      - mountPath: /var/lib/php/sessions
        name: php-sessions
      - mountPath: /usr/local/etc/php/conf.d/uploads.ini
        name: php-config
        subPath: uploads.ini
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k0
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 0
      runAsGroup: 0
      runAsNonRoot: false
      runAsUser: 0
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: wordpress
    serviceAccountName: wordpress
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: wordpress-core
      persistentVolumeClaim:
        claimName: wordpress-core
    - name: wordpress-content
      persistentVolumeClaim:
        claimName: wordpress-content
    - name: wordpress-config
      persistentVolumeClaim:
        claimName: wordpress-config
    - name: wordpress-cache
      persistentVolumeClaim:
        claimName: wordpress-cache
    - configMap:
        defaultMode: 420
        name: wordpress-mu-plugins
      name: mu-plugins
    - configMap:
        defaultMode: 420
        name: wordpress-php-config
      name: php-config
    - emptyDir:
        sizeLimit: 1Gi
      name: tmp-dir
    - emptyDir:
        sizeLimit: 500Mi
      name: apache-logs
    - emptyDir:
        sizeLimit: 100Mi
      name: php-sessions
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-02T22:35:59Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-02T22:35:56Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-02T22:37:00Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-02T22:37:00Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-02T22:35:56Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 50m
        ephemeral-storage: 200Mi
        memory: 256Mi
      containerID: containerd://3911fc621c60567634aee53c679e5186e0ed70eece77e4a02cece5d46a6a08c4
      image: docker.io/library/wordpress:6.8.3-php8.3-apache
      imageID: docker.io/library/wordpress@sha256:30bff39330d1693b0ce13d32fc9b7bb67193064f040b7d60d3494e136fa599d4
      lastState: {}
      name: wordpress
      ready: true
      resources:
        limits:
          cpu: 800m
          ephemeral-storage: 800Mi
          memory: 1Gi
        requests:
          cpu: 50m
          ephemeral-storage: 200Mi
          memory: 256Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-02T22:35:59Z"
      volumeMounts:
      - mountPath: /var/www/html
        name: wordpress-core
      - mountPath: /var/www/html/wp-content
        name: wordpress-content
      - mountPath: /var/www/html/wp-config-custom
        name: wordpress-config
      - mountPath: /var/www/html/wp-content/mu-plugins
        name: mu-plugins
      - mountPath: /var/cache/wordpress
        name: wordpress-cache
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/log/apache2
        name: apache-logs
      - mountPath: /var/lib/php/sessions
        name: php-sessions
      - mountPath: /usr/local/etc/php/conf.d/uploads.ini
        name: php-config
    hostIP: 10.120.0.4
    hostIPs:
    - ip: 10.120.0.4
    observedGeneration: 1
    phase: Running
    podIP: 10.108.0.7
    podIPs:
    - ip: 10.108.0.7
    qosClass: Burstable
    startTime: "2026-01-02T22:35:56Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-02-18T21:45:00Z"
    generateName: wordpress-cron-29524185-
    generation: 1
    labels:
      app.kubernetes.io/component: cron
      app.kubernetes.io/instance: wordpress
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: wordpress
      app.kubernetes.io/part-of: wordpress-platform
      app.kubernetes.io/version: 6.8.3
      batch.kubernetes.io/controller-uid: 1ecfa7ec-d602-41bc-94f7-fbc32a12d4b2
      batch.kubernetes.io/job-name: wordpress-cron-29524185
      controller-uid: 1ecfa7ec-d602-41bc-94f7-fbc32a12d4b2
      helm.sh/chart: wordpress-3.2.6
      job-name: wordpress-cron-29524185
    name: wordpress-cron-29524185-khd9j
    namespace: wordpress
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: wordpress-cron-29524185
      uid: 1ecfa7ec-d602-41bc-94f7-fbc32a12d4b2
    resourceVersion: "19611156"
    uid: 8729a64d-4fe0-431b-92bf-683ab54bb73f
  spec:
    containers:
    - command:
      - /bin/sh
      - -c
      - |
        echo "Running WordPress cron at $(date)"
        curl -f -s -m 30 "http://wordpress/wp-cron.php?doing_wp_cron" || echo "Cron request failed"
        echo "WordPress cron completed at $(date)"
      image: curlimages/curl:latest
      imagePullPolicy: Always
      name: wordpress-cron
      resources:
        limits:
          cpu: 50m
          memory: 32Mi
        requests:
          cpu: 10m
          memory: 16Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xp4zj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ptoken-assets--pool-525qi3gh9-s44k0
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-xp4zj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-18T21:45:05Z"
      observedGeneration: 1
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-18T21:45:00Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-18T21:45:04Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-18T21:45:04Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-18T21:45:00Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 10m
        memory: 16Mi
      containerID: containerd://61d36c6ce23b9cda0adf5aaa4be0bb7ce770972398ee126f51ac5f8674f423dd
      image: docker.io/curlimages/curl:latest
      imageID: docker.io/curlimages/curl@sha256:d94d07ba9e7d6de898b6d96c1a072f6f8266c687af78a74f380087a0addf5d17
      lastState: {}
      name: wordpress-cron
      ready: false
      resources:
        limits:
          cpu: 50m
          memory: 32Mi
        requests:
          cpu: 10m
          memory: 16Mi
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://61d36c6ce23b9cda0adf5aaa4be0bb7ce770972398ee126f51ac5f8674f423dd
          exitCode: 0
          finishedAt: "2026-02-18T21:45:03Z"
          reason: Completed
          startedAt: "2026-02-18T21:45:02Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xp4zj
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.4
    hostIPs:
    - ip: 10.120.0.4
    observedGeneration: 1
    phase: Succeeded
    podIP: 10.108.0.66
    podIPs:
    - ip: 10.108.0.66
    qosClass: Burstable
    startTime: "2026-02-18T21:45:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-12-18T10:57:14Z"
    generateName: wordpress-mariadb-
    generation: 1
    labels:
      app.kubernetes.io/instance: wordpress
      app.kubernetes.io/name: mariadb
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: wordpress-mariadb-5c5989f9c6
      statefulset.kubernetes.io/pod-name: wordpress-mariadb-0
    name: wordpress-mariadb-0
    namespace: wordpress
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: wordpress-mariadb
      uid: b74eda07-58a9-4e73-bc25-6187e189c7a5
    resourceVersion: "38365"
    uid: 02fb49a0-57e0-4cd6-8be3-23a08337da0b
  spec:
    containers:
    - env:
      - name: MYSQL_ROOT_PASSWORD
        valueFrom:
          secretKeyRef:
            key: mariadb-root-password
            name: wordpress-mariadb
      - name: MYSQL_DATABASE
        value: wordpress
      - name: MYSQL_USER
        value: wordpress
      - name: MYSQL_PASSWORD
        valueFrom:
          secretKeyRef:
            key: mariadb-password
            name: wordpress-mariadb
      image: mariadb:12.0.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 3306
        timeoutSeconds: 5
      name: mariadb
      ports:
      - containerPort: 3306
        name: mysql
        protocol: TCP
      readinessProbe:
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 5
        successThreshold: 1
        tcpSocket:
          port: 3306
        timeoutSeconds: 3
      resources:
        limits:
          cpu: 250m
          ephemeral-storage: 1Gi
          memory: 512Mi
        requests:
          cpu: 50m
          ephemeral-storage: 1Gi
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsGroup: 999
        runAsNonRoot: true
        runAsUser: 999
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/mysql
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-85ssd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: wordpress-mariadb-0
    nodeName: ptoken-assets--pool-525qi3gh9-s44k1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 999
      runAsGroup: 999
      runAsNonRoot: true
      runAsUser: 999
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: default
    serviceAccountName: default
    subdomain: wordpress-mariadb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-wordpress-mariadb-0
    - name: kube-api-access-85ssd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T10:57:36Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T10:57:15Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T10:58:12Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T10:58:12Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-18T10:57:15Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 50m
        ephemeral-storage: 1Gi
        memory: 256Mi
      containerID: containerd://127116834a62e6d12aa099be506822b589c27aa3977edd69ba8b993cc05cb322
      image: docker.io/library/mariadb:12.0.2
      imageID: docker.io/library/mariadb@sha256:607835cd628b78e2876f6a586d0ec37b296c47683b31ef750002d3d17d3d8f7a
      lastState: {}
      name: mariadb
      ready: true
      resources:
        limits:
          cpu: 250m
          ephemeral-storage: 1Gi
          memory: 512Mi
        requests:
          cpu: 50m
          ephemeral-storage: 1Gi
          memory: 256Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-18T10:57:36Z"
      volumeMounts:
      - mountPath: /var/lib/mysql
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-85ssd
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.120.0.3
    hostIPs:
    - ip: 10.120.0.3
    observedGeneration: 1
    phase: Running
    podIP: 10.108.0.155
    podIPs:
    - ip: 10.108.0.155
    qosClass: Burstable
    startTime: "2025-12-18T10:57:15Z"
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: anythingllm
      meta.helm.sh/release-namespace: anything-llm
    creationTimestamp: "2025-12-18T11:03:08Z"
    labels:
      app.kubernetes.io/component: ai-assistant
      app.kubernetes.io/instance: anythingllm
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: anythingllm
      app.kubernetes.io/part-of: weown-mvp
      app.kubernetes.io/version: 1.9.0
      helm.sh/chart: anythingllm-2.0.2
    name: anythingllm
    namespace: anything-llm
    resourceVersion: "39489"
    uid: dd07caf0-67cd-4a37-a520-f1a00c67445f
  spec:
    clusterIP: 10.109.2.2
    clusterIPs:
    - 10.109.2.2
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 3001
    selector:
      app.kubernetes.io/instance: anythingllm
      app.kubernetes.io/name: anythingllm
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-18T10:55:15Z"
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.13.2
      helm.sh/chart: cert-manager-v1.13.2
    name: cert-manager
    namespace: cert-manager
    resourceVersion: "37353"
    uid: d8f5b30d-77e0-4c0e-9d6a-0f9662d49bad
  spec:
    clusterIP: 10.109.27.90
    clusterIPs:
    - 10.109.27.90
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-prometheus-servicemonitor
      port: 9402
      protocol: TCP
      targetPort: 9402
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cert-manager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-18T10:55:15Z"
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.13.2
      helm.sh/chart: cert-manager-v1.13.2
    name: cert-manager-webhook
    namespace: cert-manager
    resourceVersion: "37354"
    uid: 1880790f-899f-4d1f-afee-0727b6b052c8
  spec:
    clusterIP: 10.109.22.36
    clusterIPs:
    - 10.109.22.36
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: webhook
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-12-18T07:52:56Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "198"
    uid: c6d7ef69-6e42-4a0e-980f-b11f483aed9c
  spec:
    clusterIP: 10.109.0.1
    clusterIPs:
    - 10.109.0.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubernetes.digitalocean.com/load-balancer-id: 00da50cc-4a93-4d13-89e9-c3720f9b8a0e
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
      service.beta.kubernetes.io/do-loadbalancer-name: 1-34-1-do-1-sfo-nginx-ingress
      service.beta.kubernetes.io/do-loadbalancer-type: REGIONAL_NETWORK
    creationTimestamp: "2025-12-18T10:51:34Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.1
      helm.sh/chart: ingress-nginx-4.14.1
    name: ingress-nginx-controller
    namespace: ingress-nginx
    resourceVersion: "36583"
    uid: 431fa140-f190-4f16-a37d-6db5056fb03f
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 10.109.22.185
    clusterIPs:
    - 10.109.22.185
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: http
      name: http
      nodePort: 32401
      port: 80
      protocol: TCP
      targetPort: http
    - appProtocol: https
      name: https
      nodePort: 32041
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 138.197.234.232
        ipMode: VIP
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2025-12-18T10:51:34Z"
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.1
      helm.sh/chart: ingress-nginx-4.14.1
    name: ingress-nginx-controller-admission
    namespace: ingress-nginx
    resourceVersion: "36426"
    uid: b9fd6982-0358-4991-ac8a-0b58c008547f
  spec:
    clusterIP: 10.109.28.105
    clusterIPs:
    - 10.109.28.105
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: https
      name: https-webhook
      port: 443
      protocol: TCP
      targetPort: webhook
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2025-12-18T10:51:34Z"
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.1
      helm.sh/chart: ingress-nginx-4.14.1
    name: ingress-nginx-controller-metrics
    namespace: ingress-nginx
    resourceVersion: "36425"
    uid: 9b856d98-b089-4032-935c-41bfde7eb3ed
  spec:
    clusterIP: 10.109.24.234
    clusterIPs:
    - 10.109.24.234
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: metrics
      port: 10254
      protocol: TCP
      targetPort: metrics
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/port: "9964"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-12-18T07:53:39Z"
    labels:
      app.kubernetes.io/name: cilium-agent
      app.kubernetes.io/part-of: cilium
      c3.doks.digitalocean.com/component: cilium
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
      k8s-app: cilium
    name: cilium-agent
    namespace: kube-system
    resourceVersion: "496"
    uid: e19fa1b1-d2ca-437a-916c-0f403a87e477
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: envoy-metrics
      port: 9964
      protocol: TCP
      targetPort: envoy-metrics
    selector:
      k8s-app: cilium
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/port: "9965"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-12-18T07:53:39Z"
    labels:
      app.kubernetes.io/name: hubble
      app.kubernetes.io/part-of: cilium
      c3.doks.digitalocean.com/component: cilium
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
      k8s-app: hubble
    name: hubble-metrics
    namespace: kube-system
    resourceVersion: "499"
    uid: 69bd8e9c-a176-4527-8745-d2cc5787fe13
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: hubble-metrics
      port: 9965
      protocol: TCP
      targetPort: hubble-metrics
    selector:
      k8s-app: cilium
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-12-18T07:53:39Z"
    labels:
      app.kubernetes.io/name: hubble-peer
      app.kubernetes.io/part-of: cilium
      c3.doks.digitalocean.com/component: cilium
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
      k8s-app: cilium
    name: hubble-peer
    namespace: kube-system
    resourceVersion: "503"
    uid: 5c037024-0fa3-408a-a2ed-00c63d3820ca
  spec:
    clusterIP: 10.109.11.166
    clusterIPs:
    - 10.109.11.166
    internalTrafficPolicy: Local
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: peer-service
      port: 443
      protocol: TCP
      targetPort: 4244
    selector:
      k8s-app: cilium
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-12-18T07:53:39Z"
    labels:
      app.kubernetes.io/name: hubble-relay
      app.kubernetes.io/part-of: cilium
      c3.doks.digitalocean.com/component: cilium
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
      k8s-app: hubble-relay
    name: hubble-relay
    namespace: kube-system
    resourceVersion: "508"
    uid: 1f58eecc-7cb6-4cf1-b579-16e7ca88f513
  spec:
    clusterIP: 10.109.20.248
    clusterIPs:
    - 10.109.20.248
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 80
      protocol: TCP
      targetPort: grpc
    selector:
      k8s-app: hubble-relay
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-12-18T07:53:39Z"
    labels:
      app.kubernetes.io/name: hubble-ui
      app.kubernetes.io/part-of: cilium
      c3.doks.digitalocean.com/component: cilium
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
      k8s-app: hubble-ui
    name: hubble-ui
    namespace: kube-system
    resourceVersion: "512"
    uid: 82e03837-a2af-4bf5-824f-4cc8d3789036
  spec:
    clusterIP: 10.109.12.96
    clusterIPs:
    - 10.109.12.96
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 8081
    selector:
      k8s-app: hubble-ui
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/port: "9153"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-12-18T07:56:07Z"
    labels:
      c3.doks.digitalocean.com/component: coredns
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: CoreDNS
    name: kube-dns
    namespace: kube-system
    resourceVersion: "1377"
    uid: 65d046ee-d506-4d61-81b0-76a1e233d849
  spec:
    clusterIP: 10.109.0.10
    clusterIPs:
    - 10.109.0.10
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    - name: metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: matomo
      meta.helm.sh/release-namespace: matomo
    creationTimestamp: "2025-12-18T11:18:10Z"
    labels:
      app.kubernetes.io/instance: matomo
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: matomo
      app.kubernetes.io/version: 5.5.1
      helm.sh/chart: matomo-2.0.6
    name: matomo
    namespace: matomo
    resourceVersion: "42887"
    uid: 1b2de380-f5c5-4eb0-a0e8-49be84f0d168
  spec:
    clusterIP: 10.109.25.34
    clusterIPs:
    - 10.109.25.34
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 80
    selector:
      app.kubernetes.io/component: web
      app.kubernetes.io/instance: matomo
      app.kubernetes.io/name: matomo
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: matomo
      meta.helm.sh/release-namespace: matomo
    creationTimestamp: "2025-12-18T11:18:10Z"
    labels:
      app.kubernetes.io/component: mariadb
      app.kubernetes.io/instance: matomo
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: matomo
      app.kubernetes.io/version: 5.5.1
      helm.sh/chart: matomo-2.0.6
    name: matomo-mariadb
    namespace: matomo
    resourceVersion: "42888"
    uid: 4c9c35e9-afe2-478d-86cb-f1e8d5c9330c
  spec:
    clusterIP: 10.109.13.149
    clusterIPs:
    - 10.109.13.149
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: mysql
      port: 3306
      protocol: TCP
      targetPort: mysql
    selector:
      app.kubernetes.io/component: mariadb
      app.kubernetes.io/instance: matomo
      app.kubernetes.io/name: matomo
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: n8n
      meta.helm.sh/release-namespace: n8n
    creationTimestamp: "2026-01-01T22:28:52Z"
    labels:
      app.kubernetes.io/instance: n8n
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: n8n
      app.kubernetes.io/part-of: n8n
      app.kubernetes.io/version: 2.1.4
      component: main
      helm.sh/chart: n8n-2.8.1
      security.weown.xyz/compliance: SOC2-ISO42001
      security.weown.xyz/network-policy: zero-trust
    name: n8n
    namespace: n8n
    resourceVersion: "4576558"
    uid: 22069aa3-efb3-4ef0-b1c3-fe639095f293
  spec:
    clusterIP: 10.109.10.230
    clusterIPs:
    - 10.109.10.230
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 5678
      protocol: TCP
      targetPort: 5678
    selector:
      app.kubernetes.io/instance: n8n
      app.kubernetes.io/name: n8n
      component: main
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: wordpress
      meta.helm.sh/release-namespace: wordpress
    creationTimestamp: "2025-12-18T10:57:12Z"
    labels:
      app.kubernetes.io/component: wordpress
      app.kubernetes.io/instance: wordpress
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: wordpress
      app.kubernetes.io/part-of: wordpress-platform
      app.kubernetes.io/version: 6.8.3
      helm.sh/chart: wordpress-3.2.6
    name: wordpress
    namespace: wordpress
    resourceVersion: "37968"
    uid: a1ef21a4-8488-4f38-8ee1-633ed61bd9ec
  spec:
    clusterIP: 10.109.5.133
    clusterIPs:
    - 10.109.5.133
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 80
    selector:
      app.kubernetes.io/component: wordpress
      app.kubernetes.io/instance: wordpress
      app.kubernetes.io/name: wordpress
    sessionAffinity: ClientIP
    sessionAffinityConfig:
      clientIP:
        timeoutSeconds: 3600
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: wordpress
      meta.helm.sh/release-namespace: wordpress
    creationTimestamp: "2025-12-18T10:57:11Z"
    labels:
      app.kubernetes.io/instance: wordpress
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: mariadb
    name: wordpress-mariadb
    namespace: wordpress
    resourceVersion: "37955"
    uid: c03efeda-f9a5-44d8-9b7b-bb8b84bf8b4c
  spec:
    clusterIP: 10.109.27.56
    clusterIPs:
    - 10.109.27.56
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: mysql
      port: 3306
      protocol: TCP
      targetPort: mysql
    selector:
      app.kubernetes.io/instance: wordpress
      app.kubernetes.io/name: mariadb
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2025-12-18T07:53:39Z"
    generation: 1
    labels:
      app.kubernetes.io/name: cilium-agent
      app.kubernetes.io/part-of: cilium
      c3.doks.digitalocean.com/component: cilium
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
      k8s-app: cilium
      kubernetes.io/cluster-service: "true"
    name: cilium
    namespace: kube-system
    resourceVersion: "4586222"
    uid: b4e0f534-88c0-4aca-aeef-4ef4b29adb49
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: cilium
        kubernetes.io/cluster-service: "true"
    template:
      metadata:
        annotations:
          clusterlint.digitalocean.com/disabled-checks: privileged-containers,non-root-user,resource-requirements,hostpath-volume
          container.apparmor.security.beta.kubernetes.io/apply-sysctl-overwrites: unconfined
          container.apparmor.security.beta.kubernetes.io/cilium-agent: unconfined
          container.apparmor.security.beta.kubernetes.io/clean-cilium-state: unconfined
          container.apparmor.security.beta.kubernetes.io/mount-cgroup: unconfined
          kubectl.kubernetes.io/default-container: cilium-agent
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        labels:
          app.kubernetes.io/name: cilium-agent
          app.kubernetes.io/part-of: cilium
          doks.digitalocean.com/managed: "true"
          k8s-app: cilium
          kubernetes.io/cluster-service: "true"
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  k8s-app: cilium
              topologyKey: kubernetes.io/hostname
        automountServiceAccountToken: true
        containers:
        - args:
          - --config-dir=/tmp/cilium/config-map
          - --k8s-api-server=https://ca54c9b1-1f4a-498d-9a4d-af31964c74a9.k8s.ondigitalocean.com
          - --ipv4-native-routing-cidr=10.108.0.0/16
          command:
          - cilium-agent
          env:
          - name: K8S_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: CILIUM_K8S_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CILIUM_CLUSTERMESH_CONFIG
            value: /var/lib/cilium/clustermesh/
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.memory
          - name: KUBE_CLIENT_BACKOFF_BASE
            value: "1"
          - name: KUBE_CLIENT_BACKOFF_DURATION
            value: "120"
          - name: KUBERNETES_SERVICE_HOST
            value: ca54c9b1-1f4a-498d-9a4d-af31964c74a9.k8s.ondigitalocean.com
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          image: ghcr.io/digitalocean-packages/cilium:v1.18.3
          imagePullPolicy: IfNotPresent
          lifecycle:
            postStart:
              exec:
                command:
                - bash
                - -c
                - |
                  set -o errexit
                  set -o pipefail
                  set -o nounset

                  # When running in AWS ENI mode, it's likely that 'aws-node' has
                  # had a chance to install SNAT iptables rules. These can result
                  # in dropped traffic, so we should attempt to remove them.
                  # We do it using a 'postStart' hook since this may need to run
                  # for nodes which might have already been init'ed but may still
                  # have dangling rules. This is safe because there are no
                  # dependencies on anything that is part of the startup script
                  # itself, and can be safely run multiple times per node (e.g. in
                  # case of a restart).
                  if [[ "$(iptables-save | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')" != "0" ]];
                  then
                      echo 'Deleting iptables rules created by the AWS CNI VPC plugin'
                      iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN' | iptables-restore
                  fi
                  echo 'Done!'
            preStop:
              exec:
                command:
                - /cni-uninstall.sh
          livenessProbe:
            failureThreshold: 10
            httpGet:
              host: 127.0.0.1
              httpHeaders:
              - name: brief
                value: "true"
              - name: require-k8s-connectivity
                value: "false"
              path: /healthz
              port: 9879
              scheme: HTTP
            initialDelaySeconds: 120
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: cilium-agent
          ports:
          - containerPort: 4244
            hostPort: 4244
            name: peer-service
            protocol: TCP
          - containerPort: 9090
            hostPort: 9090
            name: prometheus
            protocol: TCP
          - containerPort: 9964
            hostPort: 9964
            name: envoy-metrics
            protocol: TCP
          - containerPort: 9965
            hostPort: 9965
            name: hubble-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              host: 127.0.0.1
              httpHeaders:
              - name: brief
                value: "true"
              path: /healthz
              port: 9879
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            requests:
              cpu: 300m
              memory: 300Mi
          securityContext:
            capabilities:
              add:
              - CHOWN
              - KILL
              - NET_ADMIN
              - NET_RAW
              - IPC_LOCK
              - SYS_MODULE
              - SYS_ADMIN
              - SYS_RESOURCE
              - DAC_OVERRIDE
              - FOWNER
              - SETGID
              - SETUID
              drop:
              - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          startupProbe:
            failureThreshold: 300
            httpGet:
              host: 127.0.0.1
              httpHeaders:
              - name: brief
                value: "true"
              path: /healthz
              port: 9879
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /host/proc/sys/net
            name: host-proc-sys-net
          - mountPath: /host/proc/sys/kernel
            name: host-proc-sys-kernel
          - mountPath: /sys/fs/bpf
            mountPropagation: HostToContainer
            name: bpf-maps
          - mountPath: /var/run/cilium
            name: cilium-run
          - mountPath: /var/run/cilium/netns
            mountPropagation: HostToContainer
            name: cilium-netns
          - mountPath: /host/etc/cni/net.d
            name: etc-cni-netd
          - mountPath: /var/lib/cilium/clustermesh
            name: clustermesh-secrets
            readOnly: true
          - mountPath: /etc/config
            name: ip-masq-agent
            readOnly: true
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /run/xtables.lock
            name: xtables-lock
          - mountPath: /var/lib/cilium/tls/hubble
            name: hubble-tls
            readOnly: true
          - mountPath: /tmp
            name: tmp
        - args:
          - -c
          - while true; do for IP in $( ip -4 neigh show dev eth1 | grep STALE | awk
            '{print $1}'); do if ping ${IP} -c 1 -4 -W 1 -q 2>&1 > /dev/null; then
            echo "$IP reachable"; else echo "$IP unreachable"; fi; done; sleep 10;
            done
          command:
          - /bin/sh
          image: ghcr.io/digitalocean-packages/busybox:1.36
          imagePullPolicy: IfNotPresent
          name: arp-fix
          resources:
            limits:
              memory: 50Mi
            requests:
              cpu: 20m
              memory: 50Mi
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        hostNetwork: true
        initContainers:
        - command:
          - chroot
          - /host
          - bash
          - -e
          - -c
          - |
            set -euo pipefail

            error(){
                echo "$(date +'%Y-%m-%d %H:%M:%S') ERROR: $*" >&2
            }

            log() {
                echo "$(date +'%Y-%m-%d %H:%M:%S') - $1"
            }

            wait_for_cloud_init_status() {
                local status_file="/var/lib/cloud/data/status.json"
                while true; do
                    if [ ! -f "$status_file" ]; then
                        log "Waiting for cloud-init status file..."
                        sleep 2
                        continue
                    fi

                    local start finish
                    start=$(jq -r '.v1."modules-final".start // "null" | if . != "null" then (.|floor) else "null" end' "$status_file")
                    finish=$(jq -r '.v1."modules-final".finished // "null" | if . != "null" then (.|floor) else "null" end' "$status_file")

                    if [ "$start" = "null" ] || [ "$finish" = "null" ]; then
                        log "Waiting for cloud-init to complete..."
                        sleep 1
                        continue
                    fi

                    if [ "$finish" -ge "$start" ]; then
                        log "Cloud-init has completed successfully"
                        break
                    else
                        log "Waiting for cloud-init to complete..."
                        sleep 1
                    fi
                done
            }

            fetch_anchor_ip() {
                local url="http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address"
                curl -s --fail "$url" || { error "Failed to fetch anchor IP"; exit 1; }
            }

            fetch_netmask() {
                local url="http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/netmask"
                curl -s --fail "$url" || { error "Failed to fetch anchor netmask"; exit 1; }
            }

            netmask_to_prefix() {
                local netmask=$1
                local IFS=.
                local -a octets=($netmask)
                local prefix=0
                for octet in "${octets[@]}"; do
                    while [ $octet -gt 0 ]; do
                        prefix=$((prefix + (octet & 1)))
                        octet=$((octet >> 1))
                    done
                done
                echo $prefix
            }

            calculate_network() {
                local ip=$1
                local prefix=$2
                local IFS=.
                local -a ip_octets=($ip)
                local -a netmask_octets
                local -a network_octets
                local netmask=$((0xFFFFFFFF << (32 - prefix) & 0xFFFFFFFF))
                for ((i=3; i>=0; i--)); do
                    netmask_octets[$i]=$((netmask & 0xFF))
                    netmask=$((netmask >> 8))
                done
                for ((i=0; i<4; i++)); do
                    network_octets[$i]=$((ip_octets[i] & netmask_octets[i]))
                done
                echo "${network_octets[0]}.${network_octets[1]}.${network_octets[2]}.${network_octets[3]}/$prefix"
            }

            remove_ip_from_eth0() {
                local ip=$1
                local prefix=$2
                local eth0_links=$(ip addr show eth0)
                if grep -q "$ip" <<< "$eth0_links"; then
                    ip addr del "$ip/$prefix" dev eth0 || { error "Failed to remove IP $ip from eth0"; exit 1; }
                    log "Removed IP $ip from eth0"
                else
                    log "IP $ip not found on eth0"
                fi
            }

            configure_anchor_interface() {
                local ip=$1
                local prefix=$2
                if ! ip link show anchor &>/dev/null; then
                    ip link add anchor type dummy || { error "Failed to create anchor interface"; exit 1; }
                    log "Created anchor interface"
                else
                    log "anchor interface already exists"
                fi

                local anchor_links=$(ip addr show anchor)
                if ! grep -q "$ip" <<< "$anchor_links"; then
                    ip addr add "$ip/$prefix" dev anchor || { error "Failed to assign IP $ip/$prefix to anchor interface"; exit 1; }
                    log "Assigned IP $ip to anchor interface"
                else
                    log "IP $ip/$prefix already assigned to anchor interface"
                fi

                ip link set anchor up || { log "Failed to set anchor interface up"; exit 1; }
                log "Set anchor interface up"
            }

            configure_anchor_route() {
                local network=$1
                ip route replace "$network" dev eth0
                log "Configure anchor $network eth0 route"
            }

            main() {
                log "Starting anchor fix script"

                wait_for_cloud_init_status

                anchor_ip=$(fetch_anchor_ip)
                log "Fetched anchor IP: $anchor_ip"

                netmask=$(fetch_netmask)
                log "Fetched netmask: $netmask"

                ip_prefix=$(netmask_to_prefix "$netmask")
                log "Calculated IP prefix: $ip_prefix"

                network=$(calculate_network "$anchor_ip" "$ip_prefix")
                log "Calculated network: $network"

                remove_ip_from_eth0 "$anchor_ip" "$ip_prefix"
                configure_anchor_interface "$anchor_ip" "$ip_prefix"

                configure_anchor_route "$network"

                log "Init script completed successfully"
            }
            main "$@"
          image: ghcr.io/digitalocean-packages/cilium:v1.18.3
          imagePullPolicy: IfNotPresent
          name: anchor-ip-fix
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host
            name: host
        - command:
          - cilium-dbg
          - build-config
          - --source=config-map:cilium-config
          env:
          - name: K8S_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: CILIUM_K8S_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: KUBERNETES_SERVICE_HOST
            value: ca54c9b1-1f4a-498d-9a4d-af31964c74a9.k8s.ondigitalocean.com
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          image: ghcr.io/digitalocean-packages/cilium:v1.18.3
          imagePullPolicy: IfNotPresent
          name: config
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        - command:
          - sh
          - -ec
          - |
            cp /usr/bin/cilium-mount /hostbin/cilium-mount;
            nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-mount" $CGROUP_ROOT;
            rm /hostbin/cilium-mount
          env:
          - name: CGROUP_ROOT
            value: /run/cilium/cgroupv2
          - name: BIN_PATH
            value: /opt/cni/bin
          image: ghcr.io/digitalocean-packages/cilium:v1.18.3
          imagePullPolicy: IfNotPresent
          name: mount-cgroup
          resources: {}
          securityContext:
            capabilities:
              add:
              - SYS_ADMIN
              - SYS_CHROOT
              - SYS_PTRACE
              drop:
              - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /hostproc
            name: hostproc
          - mountPath: /hostbin
            name: cni-path
        - command:
          - sh
          - -ec
          - |
            cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;
            nsenter --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-sysctlfix";
            rm /hostbin/cilium-sysctlfix
          env:
          - name: BIN_PATH
            value: /opt/cni/bin
          image: ghcr.io/digitalocean-packages/cilium:v1.18.3
          imagePullPolicy: IfNotPresent
          name: apply-sysctl-overwrites
          resources: {}
          securityContext:
            capabilities:
              add:
              - SYS_ADMIN
              - SYS_CHROOT
              - SYS_PTRACE
              drop:
              - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /hostproc
            name: hostproc
          - mountPath: /hostbin
            name: cni-path
        - args:
          - mount | grep "/sys/fs/bpf type bpf" || mount -t bpf bpf /sys/fs/bpf
          command:
          - /bin/bash
          - -c
          - --
          image: ghcr.io/digitalocean-packages/cilium:v1.18.3
          imagePullPolicy: IfNotPresent
          name: mount-bpf-fs
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /sys/fs/bpf
            mountPropagation: Bidirectional
            name: bpf-maps
        - command:
          - /init-container.sh
          env:
          - name: CILIUM_ALL_STATE
            valueFrom:
              configMapKeyRef:
                key: clean-cilium-state
                name: cilium-config
                optional: true
          - name: CILIUM_BPF_STATE
            valueFrom:
              configMapKeyRef:
                key: clean-cilium-bpf-state
                name: cilium-config
                optional: true
          - name: WRITE_CNI_CONF_WHEN_READY
            valueFrom:
              configMapKeyRef:
                key: write-cni-conf-when-ready
                name: cilium-config
                optional: true
          - name: KUBERNETES_SERVICE_HOST
            value: ca54c9b1-1f4a-498d-9a4d-af31964c74a9.k8s.ondigitalocean.com
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          image: ghcr.io/digitalocean-packages/cilium:v1.18.3
          imagePullPolicy: IfNotPresent
          name: clean-cilium-state
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
              - SYS_MODULE
              - SYS_ADMIN
              - SYS_RESOURCE
              drop:
              - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /sys/fs/bpf
            name: bpf-maps
          - mountPath: /run/cilium/cgroupv2
            mountPropagation: HostToContainer
            name: cilium-cgroup
          - mountPath: /var/run/cilium
            name: cilium-run
        - command:
          - /install-plugin.sh
          image: ghcr.io/digitalocean-packages/cilium:v1.18.3
          imagePullPolicy: IfNotPresent
          name: install-cni-binaries
          resources:
            requests:
              cpu: 100m
              memory: 10Mi
          securityContext:
            capabilities:
              drop:
              - ALL
            seLinuxOptions:
              level: s0
              type: spc_t
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /host/opt/cni/bin
            name: cni-path
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          appArmorProfile:
            type: Unconfined
          seccompProfile:
            type: Unconfined
        serviceAccount: cilium
        serviceAccountName: cilium
        terminationGracePeriodSeconds: 1
        tolerations:
        - operator: Exists
        volumes:
        - hostPath:
            path: /
            type: ""
          name: host
        - emptyDir: {}
          name: tmp
        - hostPath:
            path: /var/run/cilium
            type: DirectoryOrCreate
          name: cilium-run
        - hostPath:
            path: /var/run/netns
            type: DirectoryOrCreate
          name: cilium-netns
        - hostPath:
            path: /sys/fs/bpf
            type: DirectoryOrCreate
          name: bpf-maps
        - hostPath:
            path: /proc
            type: Directory
          name: hostproc
        - hostPath:
            path: /run/cilium/cgroupv2
            type: DirectoryOrCreate
          name: cilium-cgroup
        - hostPath:
            path: /opt/cni/bin
            type: DirectoryOrCreate
          name: cni-path
        - hostPath:
            path: /etc/cni/net.d
            type: DirectoryOrCreate
          name: etc-cni-netd
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - name: clustermesh-secrets
          projected:
            defaultMode: 256
            sources:
            - secret:
                name: cilium-clustermesh
                optional: true
            - secret:
                items:
                - key: tls.key
                  path: common-etcd-client.key
                - key: tls.crt
                  path: common-etcd-client.crt
                - key: ca.crt
                  path: common-etcd-client-ca.crt
                name: clustermesh-apiserver-remote-cert
                optional: true
            - secret:
                items:
                - key: tls.key
                  path: local-etcd-client.key
                - key: tls.crt
                  path: local-etcd-client.crt
                - key: ca.crt
                  path: local-etcd-client-ca.crt
                name: clustermesh-apiserver-local-cert
                optional: true
        - configMap:
            defaultMode: 420
            items:
            - key: config
              path: ip-masq-agent
            name: ip-masq-agent
            optional: true
          name: ip-masq-agent
        - hostPath:
            path: /proc/sys/net
            type: Directory
          name: host-proc-sys-net
        - hostPath:
            path: /proc/sys/kernel
            type: Directory
          name: host-proc-sys-kernel
        - name: hubble-tls
          projected:
            defaultMode: 256
            sources:
            - secret:
                items:
                - key: tls.crt
                  path: server.crt
                - key: tls.key
                  path: server.key
                - key: ca.crt
                  path: client-ca.crt
                name: hubble-server-certs
                optional: true
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 10%
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2025-12-18T07:55:56Z"
    generation: 1
    labels:
      app: cpc-bridge-proxy
      c3.doks.digitalocean.com/component: cpc-bridge-proxy
      c3.doks.digitalocean.com/plane: data
      c3.doks.digitalocean.com/variant: ebpf
      doks.digitalocean.com/managed: "true"
    name: cpc-bridge-proxy-ebpf
    namespace: kube-system
    resourceVersion: "4586230"
    uid: e8d6ecbd-94a8-4bc0-942c-ef57e3646d2c
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: cpc-bridge-proxy
    template:
      metadata:
        annotations:
          clusterlint.digitalocean.com/disabled-checks: resource-requirements
        labels:
          app: cpc-bridge-proxy
          doks.digitalocean.com/managed: "true"
      spec:
        automountServiceAccountToken: false
        containers:
        - image: ghcr.io/digitalocean-packages/cpbridge:1.29.3
          imagePullPolicy: IfNotPresent
          name: cpc-bridge-proxy
          resources:
            requests:
              cpu: 100m
              memory: 75Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/nginx
            name: cpc-bridge-proxy-config
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        initContainers:
        - command:
          - /bin/bash
          - -c
          - |
            set -o errexit
            set -o pipefail
            set -o nounset
            # Wait until the cpbridge interface is ready
            until ip addr show dev cpbridge | grep -q "inet 100.65.22.7"; do echo "waiting for cpbridge interface to be up"; sleep 3; done
            ip addr del 100.65.22.7/32 dev cpbridge
            ip addr add 100.65.22.7/32 dev cpbridge
            ipt_nat="iptables-legacy -t nat"
            ipt_output_args="OUTPUT -p tcp -d 10.109.0.1/32 --dport 443 -j DNAT --to-destination 100.65.22.7:16443"
            ipt_prerouting_args="PREROUTING -p tcp -d 100.65.22.7 --dport 443 -j DNAT --to-destination 100.65.22.7:16443"
            ipt_output_cpbridgeip_args="OUTPUT -p tcp -d 100.65.22.7 --dport 443 -j DNAT --to-destination 100.65.22.7:16443"
            ${ipt_nat} --check ${ipt_output_args} || ${ipt_nat} --insert ${ipt_output_args}
            ${ipt_nat} --check ${ipt_prerouting_args} || ${ipt_nat} --insert ${ipt_prerouting_args}
            ${ipt_nat} --check ${ipt_output_cpbridgeip_args} || ${ipt_nat} --insert ${ipt_output_cpbridgeip_args}
          image: ghcr.io/digitalocean-packages/cpbridge:1.29.3
          imagePullPolicy: IfNotPresent
          name: init-cpbridge
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: cpc-bridge-proxy-config
          name: cpc-bridge-proxy-config
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2025-12-18T07:56:13Z"
    generation: 1
    labels:
      c3.doks.digitalocean.com/component: csi-node-service
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
    name: csi-do-node
    namespace: kube-system
    resourceVersion: "4586231"
    uid: 494ac330-89ed-49dd-9d85-8240a2532cd6
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: csi-do-node
    template:
      metadata:
        annotations:
          clusterlint.digitalocean.com/disabled-checks: privileged-containers,non-root-user,resource-requirements,hostpath-volume
          kubectl.kubernetes.io/default-container: csi-do-plugin
        labels:
          app: csi-do-node
          doks.digitalocean.com/managed: "true"
          role: csi-do
      spec:
        containers:
        - args:
          - --v=5
          - --csi-address=$(ADDRESS)
          - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
          env:
          - name: ADDRESS
            value: /csi/csi.sock
          - name: DRIVER_REG_SOCK_PATH
            value: /var/lib/kubelet/plugins/dobs.csi.digitalocean.com/csi.sock
          - name: KUBE_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: ghcr.io/digitalocean-packages/sig-storage/csi-node-driver-registrar:v2.15.0
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/sh
                - -c
                - rm -rf /registration/dobs.csi.digitalocean.com /registration/dobs.csi.digitalocean.com-reg.sock
          name: csi-node-driver-registrar
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi/
            name: plugin-dir
          - mountPath: /registration/
            name: registration-dir
        - args:
          - --endpoint=$(CSI_ENDPOINT)
          - --validate-attachment=true
          - --volume-limit=15
          - --url=https://api.digitalocean.com
          - --driver-name=dobs.csi.digitalocean.com
          env:
          - name: CSI_ENDPOINT
            value: unix:///csi/csi.sock
          image: ghcr.io/digitalocean-packages/do-csi-plugin:v4.15.0
          imagePullPolicy: Always
          name: csi-do-plugin
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: plugin-dir
          - mountPath: /var/lib/kubelet
            mountPropagation: Bidirectional
            name: pods-mount-dir
          - mountPath: /dev
            name: device-dir
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: csi-do-node-sa
        serviceAccountName: csi-do-node-sa
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          operator: Exists
        - effect: NoSchedule
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins_registry/
            type: DirectoryOrCreate
          name: registration-dir
        - hostPath:
            path: /var/lib/kubelet/plugins/dobs.csi.digitalocean.com
            type: DirectoryOrCreate
          name: plugin-dir
        - hostPath:
            path: /var/lib/kubelet
            type: Directory
          name: pods-mount-dir
        - hostPath:
            path: /dev
            type: ""
          name: device-dir
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 10%
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2025-12-18T07:56:22Z"
    generation: 1
    labels:
      app: do-node-agent
      c3.doks.digitalocean.com/component: do-node-agent
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
    name: do-node-agent
    namespace: kube-system
    resourceVersion: "4586232"
    uid: f3c49fdd-a457-4d85-8477-2b024d14d32f
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: do-node-agent
    template:
      metadata:
        annotations:
          clusterlint.digitalocean.com/disabled-checks: resource-requirements,hostpath-volume
        labels:
          app: do-node-agent
          doks.digitalocean.com/managed: "true"
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: doks.digitalocean.com/gpu-brand
                  operator: NotIn
                  values:
                  - amd
                  - nvidia
              - matchExpressions:
                - key: doks.digitalocean.com/gpu-brand
                  operator: In
                  values:
                  - nvidia
                - key: doks.digitalocean.com/nvidia-dcgm-enabled
                  operator: In
                  values:
                  - "false"
              - matchExpressions:
                - key: doks.digitalocean.com/gpu-brand
                  operator: In
                  values:
                  - nvidia
                - key: doks.digitalocean.com/nvidia-dcgm-enabled
                  operator: DoesNotExist
        containers:
        - args:
          - '@/etc/config/do-agent-config'
          - --path.procfs=/host/proc
          - --path.sysfs=/host/sys
          - --k8s-metrics-path=http://kube-state-metrics.kube-system.svc.cluster.local:8080/metrics
          - --additional-label=kubernetes_cluster_uuid:ca54c9b1-1f4a-498d-9a4d-af31964c74a9
          command:
          - /bin/do-agent
          image: ghcr.io/digitalocean-packages/do-agent:3.18.5-rc
          imagePullPolicy: IfNotPresent
          name: do-node-agent
          resources:
            limits:
              memory: 300Mi
            requests:
              cpu: 102m
              memory: 80Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/proc
            name: proc
            readOnly: true
          - mountPath: /host/sys
            name: sys
            readOnly: true
          - mountPath: /host/root
            mountPropagation: HostToContainer
            name: root
            readOnly: true
          - mountPath: /etc/config
            name: dynamic-config
        dnsPolicy: ClusterFirstWithHostNet
        hostNetwork: true
        initContainers:
        - command:
          - sh
          - -c
          - |
            set -o errexit
            set -o pipefail
            set -o nounset

            KUBECTL=/host/usr/bin/kubectl
            POOL_ID="$(${KUBECTL} get node ${NODE_NAME} -o jsonpath='{.metadata.labels.doks\.digitalocean\.com/node-pool-id}')"
            [[ -z "${POOL_ID}" ]] && echo "Pool ID label missing" && exit 1
            echo "--additional-label=kubernetes_node_pool_uuid:${POOL_ID}" > /etc/config/do-agent-config
            echo "Pool ID configured: ${POOL_ID}"
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: ghcr.io/digitalocean-packages/distroless/static-debian12:debug-nonroot-amd64
          imagePullPolicy: IfNotPresent
          name: dynamic-config
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: dynamic-config
          - mountPath: /host/usr/bin/kubectl
            name: host-kubectl
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: do-agent
        serviceAccountName: do-agent
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - hostPath:
            path: /proc
            type: ""
          name: proc
        - hostPath:
            path: /sys
            type: ""
          name: sys
        - hostPath:
            path: /
            type: ""
          name: root
        - emptyDir: {}
          name: dynamic-config
        - hostPath:
            path: /usr/bin/kubectl
            type: File
          name: host-kubectl
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "2"
    creationTimestamp: "2025-12-18T07:56:28Z"
    generation: 2
    labels:
      app: do-node-agent-amd-device-metrics-exporter
      c3.doks.digitalocean.com/component: do-node-agent
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
    name: do-node-agent-amd-device-metrics-exporter
    namespace: kube-system
    resourceVersion: "14966642"
    uid: 6697a0d7-589f-4293-89fd-1cef82d59f19
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: do-node-agent-amd-device-metrics-exporter
    template:
      metadata:
        annotations:
          clusterlint.digitalocean.com/disabled-checks: resource-requirements,hostpath-volume
        labels:
          app: do-node-agent-amd-device-metrics-exporter
          doks.digitalocean.com/managed: "true"
      spec:
        containers:
        - env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: METRICS_EXPORTER_PORT
            value: "5000"
          image: ghcr.io/digitalocean-packages/amd-gpu-device-metrics-exporter:v1.4.0
          imagePullPolicy: Always
          name: amdgpu-metrics-exporter-container
          ports:
          - containerPort: 5000
            protocol: TCP
          resources:
            limits:
              cpu: "1"
              ephemeral-storage: 10Gi
              memory: 1Gi
            requests:
              cpu: 200m
              ephemeral-storage: 5Gi
              memory: 200Mi
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dev
            name: dev-volume
          - mountPath: /sys
            name: sys
          - mountPath: /var/lib/kubelet/pod-resources
            name: pod-resources
          - mountPath: /var/lib/amd-metrics-exporter
            name: exporter-health-grpc-volume
          - mountPath: /var/run/exporter
            name: exporter-slurm-job
          - mountPath: /etc/metrics/
            name: metrics-config-volume
          workingDir: /root
        - args:
          - '@/etc/config/do-agent-config'
          - --path.procfs=/host/proc
          - --path.sysfs=/host/sys
          - --k8s-metrics-path=http://kube-state-metrics.kube-system.svc.cluster.local:8080/metrics
          - --gpu-metrics-path=http://127.0.0.1:5000/metrics
          - --additional-label=kubernetes_cluster_uuid:ca54c9b1-1f4a-498d-9a4d-af31964c74a9
          command:
          - /bin/do-agent
          image: ghcr.io/digitalocean-packages/do-agent:3.18.5-rc
          imagePullPolicy: IfNotPresent
          name: do-node-agent
          resources:
            limits:
              memory: 300Mi
            requests:
              cpu: 102m
              memory: 80Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/proc
            name: proc
            readOnly: true
          - mountPath: /host/sys
            name: sys
            readOnly: true
          - mountPath: /host/root
            mountPropagation: HostToContainer
            name: root
            readOnly: true
          - mountPath: /etc/config
            name: dynamic-config
        dnsPolicy: ClusterFirstWithHostNet
        hostNetwork: true
        initContainers:
        - command:
          - sh
          - -c
          - |
            set -o errexit
            set -o pipefail
            set -o nounset

            KUBECTL=/host/usr/bin/kubectl
            POOL_ID="$(${KUBECTL} get node ${NODE_NAME} -o jsonpath='{.metadata.labels.doks\.digitalocean\.com/node-pool-id}')"
            [[ -z "${POOL_ID}" ]] && echo "Pool ID label missing" && exit 1
            echo "--additional-label=kubernetes_node_pool_uuid:${POOL_ID}" > /etc/config/do-agent-config
            echo "Pool ID configured: ${POOL_ID}"
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: ghcr.io/digitalocean-packages/distroless/static-debian12:debug-nonroot-amd64
          imagePullPolicy: IfNotPresent
          name: dynamic-config
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: dynamic-config
          - mountPath: /host/usr/bin/kubectl
            name: host-kubectl
        - command:
          - sh
          - -c
          - while [ ! -d /host-sys/class/kfd ] || [ ! -d /host-sys/module/amdgpu/drivers/
            ]; do echo "amdgpu driver is not loaded "; sleep 2 ;done
          image: busybox:1.36
          imagePullPolicy: IfNotPresent
          name: driver-init
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host-sys
            name: sys
        - args:
          - |
            IPT="iptables-legacy"
            CHAIN_NAME="K8S-GPU-AGENT-FIREWALL"
            PORTS="6601,50061"

            echo "Setting up custom chain: $CHAIN_NAME"

            # Ensure custom chain exists
            $IPT -N $CHAIN_NAME 2>/dev/null || true

            # Ensure clean state for idempotency
            $IPT -F $CHAIN_NAME

            # Only allow localhost traffic on the custom chain
            $IPT -A $CHAIN_NAME -s 127.0.0.1 -j ACCEPT
            $IPT -A $CHAIN_NAME -j DROP

            # Link the main INPUT chain to our custom chain for the respective ports
            if $IPT -C INPUT -p tcp -m multiport --dports $PORTS -j $CHAIN_NAME 2>/dev/null; then
               echo "Link to custom chain already exists"
            else
               echo "Setting up link to custom chain"
               $IPT -I INPUT 1 -p tcp -m multiport --dports $PORTS -j $CHAIN_NAME
            fi

            echo "IPTables Config (link to custom chain + custom chain)"
            echo ""
            $IPT -L INPUT -n -v --line-numbers | grep $CHAIN_NAME
            $IPT -L $CHAIN_NAME -n -v
          command:
          - /bin/sh
          - -c
          image: ghcr.io/digitalocean-packages/debian-trixie-slim-iptables:v1
          imagePullPolicy: IfNotPresent
          name: firewall
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        nodeSelector:
          doks.digitalocean.com/gpu-brand: amd
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: do-agent
        serviceAccountName: do-agent
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - hostPath:
            path: /proc
            type: ""
          name: proc
        - hostPath:
            path: /sys
            type: ""
          name: sys
        - hostPath:
            path: /
            type: ""
          name: root
        - emptyDir: {}
          name: dynamic-config
        - hostPath:
            path: /usr/bin/kubectl
            type: File
          name: host-kubectl
        - hostPath:
            path: /dev
            type: Directory
          name: dev-volume
        - hostPath:
            path: /var/lib/kubelet/pod-resources
            type: Directory
          name: pod-resources
        - hostPath:
            path: /var/lib/amd-metrics-exporter
            type: DirectoryOrCreate
          name: exporter-health-grpc-volume
        - hostPath:
            path: /var/run/exporter
            type: DirectoryOrCreate
          name: exporter-slurm-job
        - configMap:
            defaultMode: 420
            name: do-node-agent-device-metrics-exporter-charts-configmap
          name: metrics-config-volume
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 0
    desiredNumberScheduled: 0
    numberMisscheduled: 0
    numberReady: 0
    observedGeneration: 2
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2025-12-18T07:56:31Z"
    generation: 1
    labels:
      app: do-node-agent-nvidia-dcgm-exporter
      c3.doks.digitalocean.com/component: do-node-agent
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
    name: do-node-agent-nvidia-dcgm-exporter
    namespace: kube-system
    resourceVersion: "1602"
    uid: 8b55d0a2-80e6-4ea2-ab19-916220300b0d
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: do-node-agent-nvidia-dcgm-exporter
    template:
      metadata:
        annotations:
          clusterlint.digitalocean.com/disabled-checks: resource-requirements,hostpath-volume
        labels:
          app: do-node-agent-nvidia-dcgm-exporter
          doks.digitalocean.com/managed: "true"
      spec:
        containers:
        - env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: DCGM_EXPORTER_KUBERNETES
            value: "true"
          - name: DCGM_EXPORTER_LISTEN
            value: :5000
          - name: DCGM_REMOTE_HOSTENGINE_INFO
            value: localhost:5555
          image: ghcr.io/digitalocean-packages/nvidia-dcgm-exporter:4.4.1-4.6.0-distroless
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 45
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          name: nvidia-dcgm-container
          ports:
          - containerPort: 5000
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              ephemeral-storage: 5Gi
              memory: 1Gi
            requests:
              cpu: 200m
              ephemeral-storage: 5Gi
              memory: 200Mi
          securityContext:
            capabilities:
              add:
              - SYS_ADMIN
            runAsNonRoot: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/kubelet/pod-resources
            name: pod-gpu-resources
            readOnly: true
          - mountPath: /etc/dcgm-exporter/default-counters.csv
            name: exporter-metrics-volume
            subPath: default-counters.csv
          workingDir: /root
        - args:
          - '@/etc/config/do-agent-config'
          - --path.procfs=/host/proc
          - --path.sysfs=/host/sys
          - --k8s-metrics-path=http://kube-state-metrics.kube-system.svc.cluster.local:8080/metrics
          - --gpu-metrics-path=http://127.0.0.1:5000/metrics
          - --additional-label=kubernetes_cluster_uuid:ca54c9b1-1f4a-498d-9a4d-af31964c74a9
          command:
          - /bin/do-agent
          image: ghcr.io/digitalocean-packages/do-agent:3.18.5-rc
          imagePullPolicy: IfNotPresent
          name: do-node-agent
          resources:
            limits:
              memory: 300Mi
            requests:
              cpu: 102m
              memory: 80Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/proc
            name: proc
            readOnly: true
          - mountPath: /host/sys
            name: sys
            readOnly: true
          - mountPath: /host/root
            mountPropagation: HostToContainer
            name: root
            readOnly: true
          - mountPath: /etc/config
            name: dynamic-config
          - mountPath: /var/lib/kubelet/pod-resources
            name: pod-gpu-resources
            readOnly: true
          - mountPath: /etc/dcgm-exporter/default-counters.csv
            name: exporter-metrics-volume
            subPath: default-counters.csv
        dnsPolicy: ClusterFirstWithHostNet
        hostNetwork: true
        initContainers:
        - command:
          - sh
          - -c
          - |
            set -o errexit
            set -o pipefail
            set -o nounset

            KUBECTL=/host/usr/bin/kubectl
            POOL_ID="$(${KUBECTL} get node ${NODE_NAME} -o jsonpath='{.metadata.labels.doks\.digitalocean\.com/node-pool-id}')"
            [[ -z "${POOL_ID}" ]] && echo "Pool ID label missing" && exit 1
            echo "--additional-label=kubernetes_node_pool_uuid:${POOL_ID}" > /etc/config/do-agent-config
            echo "Pool ID configured: ${POOL_ID}"
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: ghcr.io/digitalocean-packages/distroless/static-debian12:debug-nonroot-amd64
          imagePullPolicy: IfNotPresent
          name: dynamic-config
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: dynamic-config
          - mountPath: /host/usr/bin/kubectl
            name: host-kubectl
        nodeSelector:
          doks.digitalocean.com/gpu-brand: nvidia
          doks.digitalocean.com/nvidia-dcgm-enabled: "true"
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: do-agent
        serviceAccountName: do-agent
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - hostPath:
            path: /proc
            type: ""
          name: proc
        - hostPath:
            path: /sys
            type: ""
          name: sys
        - hostPath:
            path: /
            type: ""
          name: root
        - emptyDir: {}
          name: dynamic-config
        - hostPath:
            path: /var/lib/kubelet/pod-resources
            type: ""
          name: pod-gpu-resources
        - hostPath:
            path: /usr/bin/kubectl
            type: File
          name: host-kubectl
        - hostPath:
            path: /var/lib/kubelet/pod-resources
            type: Directory
          name: pod-resources
        - configMap:
            defaultMode: 420
            items:
            - key: metrics
              path: default-counters.csv
            name: do-node-agent-nvidia-dcgm-exporter-metrics-config-map
          name: exporter-metrics-volume
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 0
    desiredNumberScheduled: 0
    numberMisscheduled: 0
    numberReady: 0
    observedGeneration: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2026-02-04T02:01:12Z"
    generation: 1
    labels:
      app: doks-otel
      c3.doks.digitalocean.com/component: doks-otel
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
    name: doks-telemetry-config-reloader
    namespace: kube-system
    resourceVersion: "14966885"
    uid: db8123dd-58b4-4222-b0e1-92d8c042b0f0
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: doks-otel
    template:
      metadata:
        annotations:
          clusterlint.digitalocean.com/disabled-checks: resource-requirements
        labels:
          app: doks-otel
          doks.digitalocean.com/managed: "true"
      spec:
        automountServiceAccountToken: false
        containers:
        - command:
          - chroot
          - /host
          - /bin/bash
          - -c
          - |
            refresh_token() {
              EXISTING_TOKEN=$(yq eval ".extensions.bearertokenauth/withscheme.token" /etc/otelcol-contrib/config.yaml)
              NEW_TOKEN=$(cat /worker_observability_refresh_token/worker_observability_refresh_token)
              if [ "${EXISTING_TOKEN}" == "${NEW_TOKEN}" ]; then
                echo "config is in sync, skipping."
              elif [ "${NEW_TOKEN}" == "TO-BE-REPLACED" ]; then
                echo "waiting for refresh token to be available"
              else
                echo "refreshing token"
                yq e -i ".extensions.bearertokenauth/withscheme.token = \"$NEW_TOKEN\" " /etc/otelcol-contrib/config.yaml
                SYSTEMCTL_FORCE_BUS=1 systemctl reload otelcol-contrib
              fi
            }
            while true; do
              echo "trying to refesh token..."
              refresh_token
              echo "done."
              sleep 60;
            done
          image: ghcr.io/digitalocean-packages/busybox:1.36
          imagePullPolicy: IfNotPresent
          name: telemetry-config-reloader
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host
            name: host
          - mountPath: /host/worker_observability_refresh_token
            name: worker-observability-refresh-token
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - hostPath:
            path: /
            type: ""
          name: host
        - name: worker-observability-refresh-token
          secret:
            defaultMode: 420
            secretName: worker-observability-refresh-token
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2025-12-18T07:55:50Z"
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      c3.doks.digitalocean.com/component: konnectivity-agent
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
      k8s-app: konnectivity-agent
    name: konnectivity-agent
    namespace: kube-system
    resourceVersion: "4586229"
    uid: 05ea5256-b724-471e-be05-167efe1d29d3
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: konnectivity-agent
    template:
      metadata:
        annotations:
          clusterlint.digitalocean.com/disabled-checks: resource-requirements
        labels:
          doks.digitalocean.com/managed: "true"
          k8s-app: konnectivity-agent
      spec:
        containers:
        - args:
          - --logtostderr=true
          - --ca-cert=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          - --proxy-server-port=8132
          - --admin-server-port=8133
          - --health-server-port=8134
          - --keepalive-time=5m
          - --service-account-token-path=/var/run/secrets/tokens/konnectivity-agent-token
          - --proxy-server-host=ca54c9b1-1f4a-498d-9a4d-af31964c74a9.k8s.ondigitalocean.com
          command:
          - /proxy-agent
          image: ghcr.io/digitalocean-packages/kas-network-proxy/proxy-agent:v0.33.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8134
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 15
          name: konnectivity-agent
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/run/secrets/tokens
            name: konnectivity-agent-token
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: konnectivity-agent
        serviceAccountName: konnectivity-agent
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - name: konnectivity-agent-token
          projected:
            defaultMode: 420
            sources:
            - serviceAccountToken:
                audience: system:konnectivity-server
                expirationSeconds: 3600
                path: konnectivity-agent-token
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: anythingllm
      meta.helm.sh/release-namespace: anything-llm
    creationTimestamp: "2025-12-18T11:03:08Z"
    generation: 2
    labels:
      app.kubernetes.io/component: ai-assistant
      app.kubernetes.io/instance: anythingllm
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: anythingllm
      app.kubernetes.io/part-of: weown-mvp
      app.kubernetes.io/version: 1.9.0
      helm.sh/chart: anythingllm-2.0.2
    name: anythingllm
    namespace: anything-llm
    resourceVersion: "10206337"
    uid: 46848552-8ee3-4975-a02e-ade5ee37d9b0
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: anythingllm
        app.kubernetes.io/name: anythingllm
    strategy:
      type: Recreate
    template:
      metadata:
        labels:
          app.kubernetes.io/component: ai-assistant
          app.kubernetes.io/instance: anythingllm
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: anythingllm
          app.kubernetes.io/part-of: weown-mvp
          app.kubernetes.io/version: 1.9.0
          helm.sh/chart: anythingllm-2.0.2
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: DISABLE_TELEMETRY
            value: "true"
          - name: EMBEDDING_ENGINE
            value: native
          - name: LLM_PROVIDER
            value: openai
          - name: SERVER_PORT
            value: "3001"
          - name: STORAGE_DIR
            value: /app/server/storage
          - name: VECTOR_DB
            value: lancedb
          envFrom:
          - secretRef:
              name: anythingllm-secrets
          image: mintplexlabs/anythingllm:1.9.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /api/ping
              port: 3001
              scheme: HTTP
            initialDelaySeconds: 120
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: anythingllm
          ports:
          - containerPort: 3001
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /api/ping
              port: 3001
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 200m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - SYS_ADMIN
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/server/storage
            name: anythingllm-storage
          - mountPath: /tmp
            name: tmp-dir
          - mountPath: /app/.cache
            name: cache-dir
          - mountPath: /collector
            name: anythingllm-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: anythingllm
        serviceAccountName: anythingllm
        terminationGracePeriodSeconds: 30
        volumes:
        - name: anythingllm-storage
          persistentVolumeClaim:
            claimName: anythingllm-storage
        - emptyDir: {}
          name: tmp-dir
        - emptyDir: {}
          name: cache-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-01-19T21:29:02Z"
      lastUpdateTime: "2026-01-19T21:29:02Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-12-18T11:03:08Z"
      lastUpdateTime: "2026-01-19T21:29:02Z"
      message: ReplicaSet "anythingllm-647f966459" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-18T10:55:15Z"
    generation: 2
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.13.2
      helm.sh/chart: cert-manager-v1.13.2
    name: cert-manager
    namespace: cert-manager
    resourceVersion: "46926"
    uid: 48afe8cc-1412-4bff-a58c-bc10d3bed5a8
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cert-manager
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-18T16:33:11+05:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        labels:
          app: cert-manager
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cert-manager
          app.kubernetes.io/version: v1.13.2
          helm.sh/chart: cert-manager-v1.13.2
      spec:
        containers:
        - args:
          - --v=2
          - --cluster-resource-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=kube-system
          - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.13.2
          - --max-concurrent-challenges=60
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.13.2
          imagePullPolicy: IfNotPresent
          name: cert-manager-controller
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          - containerPort: 9403
            name: http-healthz
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager
        serviceAccountName: cert-manager
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-18T10:55:19Z"
      lastUpdateTime: "2025-12-18T10:55:19Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-12-18T10:55:15Z"
      lastUpdateTime: "2025-12-18T11:33:16Z"
      message: ReplicaSet "cert-manager-84f97659c5" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-18T10:55:15Z"
    generation: 2
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.13.2
      helm.sh/chart: cert-manager-v1.13.2
    name: cert-manager-cainjector
    namespace: cert-manager
    resourceVersion: "46979"
    uid: 53cb98a1-a089-4b08-abc6-988b20896c21
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cainjector
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-18T16:33:13+05:00"
        labels:
          app: cainjector
          app.kubernetes.io/component: cainjector
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cainjector
          app.kubernetes.io/version: v1.13.2
          helm.sh/chart: cert-manager-v1.13.2
      spec:
        containers:
        - args:
          - --v=2
          - --leader-election-namespace=kube-system
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.13.2
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-cainjector
        serviceAccountName: cert-manager-cainjector
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-18T10:55:20Z"
      lastUpdateTime: "2025-12-18T10:55:20Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-12-18T10:55:15Z"
      lastUpdateTime: "2025-12-18T11:33:20Z"
      message: ReplicaSet "cert-manager-cainjector-8499fb697f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-18T10:55:15Z"
    generation: 2
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.13.2
      helm.sh/chart: cert-manager-v1.13.2
    name: cert-manager-webhook
    namespace: cert-manager
    resourceVersion: "47032"
    uid: df4aadee-6086-4554-ae03-5d47645a3d3d
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: webhook
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-18T16:33:15+05:00"
        labels:
          app: webhook
          app.kubernetes.io/component: webhook
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: webhook
          app.kubernetes.io/version: v1.13.2
          helm.sh/chart: cert-manager-v1.13.2
      spec:
        containers:
        - args:
          - --v=2
          - --secure-port=10250
          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
          - --dynamic-serving-dns-names=cert-manager-webhook
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.13.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          - containerPort: 6080
            name: healthcheck
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-webhook
        serviceAccountName: cert-manager-webhook
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-18T10:55:26Z"
      lastUpdateTime: "2025-12-18T10:55:26Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-12-18T10:55:15Z"
      lastUpdateTime: "2025-12-18T11:33:28Z"
      message: ReplicaSet "cert-manager-webhook-59ff465c5c" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2025-12-18T10:51:35Z"
    generation: 1
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.1
      helm.sh/chart: ingress-nginx-4.14.1
    name: ingress-nginx-controller
    namespace: ingress-nginx
    resourceVersion: "36555"
    uid: 10689848-7008-45be-945a-323724e6cf72
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/port: "10254"
          prometheus.io/scrape: "true"
        labels:
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: ingress-nginx
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: ingress-nginx
          app.kubernetes.io/part-of: ingress-nginx
          app.kubernetes.io/version: 1.14.1
          helm.sh/chart: ingress-nginx-4.14.1
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - /nginx-ingress-controller
          - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
          - --election-id=ingress-nginx-leader
          - --controller-class=k8s.io/ingress-nginx
          - --ingress-class=nginx
          - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
          - --validating-webhook=:8443
          - --validating-webhook-certificate=/usr/local/certificates/cert
          - --validating-webhook-key=/usr/local/certificates/key
          - --enable-metrics=true
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: LD_PRELOAD
            value: /usr/local/lib/libmimalloc.so
          image: registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /wait-shutdown
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: controller
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            name: https
            protocol: TCP
          - containerPort: 10254
            name: metrics
            protocol: TCP
          - containerPort: 8443
            name: webhook
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 90Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsGroup: 82
            runAsNonRoot: true
            runAsUser: 101
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /usr/local/certificates/
            name: webhook-cert
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: ingress-nginx
        serviceAccountName: ingress-nginx
        terminationGracePeriodSeconds: 300
        volumes:
        - name: webhook-cert
          secret:
            defaultMode: 420
            secretName: ingress-nginx-admission
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-18T10:51:56Z"
      lastUpdateTime: "2025-12-18T10:51:56Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-12-18T10:51:35Z"
      lastUpdateTime: "2025-12-18T10:51:56Z"
      message: ReplicaSet "ingress-nginx-controller-8878d8bfd" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-12-18T07:56:07Z"
    generation: 1
    labels:
      c3.doks.digitalocean.com/component: coredns
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
      k8s-app: kube-dns
      kubernetes.io/name: CoreDNS
    name: coredns
    namespace: kube-system
    resourceVersion: "1438"
    uid: d50a3672-37af-480b-8881-bdd39139df0e
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 100%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        labels:
          doks.digitalocean.com/managed: "true"
          k8s-app: kube-dns
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: doks.digitalocean.com/gpu-brand
                  operator: DoesNotExist
              weight: 50
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - kube-dns
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: ghcr.io/digitalocean-packages/coredns/coredns:1.12.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 170Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
          - mountPath: /etc/coredns/custom
            name: custom-config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - key: nvidia.com/gpu
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
        - configMap:
            defaultMode: 420
            name: coredns-custom
            optional: true
          name: custom-config-volume
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2025-12-18T07:56:11Z"
      lastUpdateTime: "2025-12-18T07:56:11Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-12-18T07:56:07Z"
      lastUpdateTime: "2025-12-18T07:56:12Z"
      message: ReplicaSet "coredns-7c475d69" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-12-18T07:53:39Z"
    generation: 1
    labels:
      app.kubernetes.io/name: hubble-relay
      app.kubernetes.io/part-of: cilium
      c3.doks.digitalocean.com/component: cilium
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
      k8s-app: hubble-relay
    name: hubble-relay
    namespace: kube-system
    resourceVersion: "1173"
    uid: 16bbf56c-1413-4fce-9b6b-922fb203979b
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: hubble-relay
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
          clusterlint.digitalocean.com/disabled-checks: resource-requirements
        labels:
          app.kubernetes.io/name: hubble-relay
          app.kubernetes.io/part-of: cilium
          doks.digitalocean.com/managed: "true"
          k8s-app: hubble-relay
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: doks.digitalocean.com/gpu-brand
                  operator: DoesNotExist
              weight: 100
          podAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  k8s-app: cilium
              topologyKey: kubernetes.io/hostname
        automountServiceAccountToken: false
        containers:
        - args:
          - serve
          command:
          - hubble-relay
          image: ghcr.io/digitalocean-packages/hubble-relay:v1.18.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 12
            grpc:
              port: 4222
              service: ""
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: hubble-relay
          ports:
          - containerPort: 4245
            name: grpc
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsGroup: 65532
            runAsNonRoot: true
            runAsUser: 65532
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/hubble-relay
            name: config
            readOnly: true
          - mountPath: /var/lib/hubble-relay/tls
            name: tls
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65532
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: hubble-relay
        serviceAccountName: hubble-relay
        terminationGracePeriodSeconds: 1
        tolerations:
        - key: nvidia.com/gpu
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: config.yaml
              path: config.yaml
            name: hubble-relay-config
          name: config
        - name: tls
          projected:
            defaultMode: 256
            sources:
            - secret:
                items:
                - key: tls.crt
                  path: client.crt
                - key: tls.key
                  path: client.key
                - key: ca.crt
                  path: hubble-server-ca.crt
                name: hubble-relay-client-certs
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-18T07:53:39Z"
      lastUpdateTime: "2025-12-18T07:53:39Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-12-18T07:53:39Z"
      lastUpdateTime: "2025-12-18T07:55:39Z"
      message: ReplicaSet "hubble-relay-7d6bc4d6f4" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-12-18T07:55:42Z"
    generation: 1
    labels:
      app.kubernetes.io/name: hubble-ui
      app.kubernetes.io/part-of: cilium
      c3.doks.digitalocean.com/component: cilium
      c3.doks.digitalocean.com/plane: data
      doks.digitalocean.com/managed: "true"
      k8s-app: hubble-ui
    name: hubble-ui
    namespace: kube-system
    resourceVersion: "1238"
    uid: fd572ef6-b79a-447d-8f12-900ab3ddad79
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: hubble-ui
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
          clusterlint.digitalocean.com/disabled-checks: resource-requirements
        labels:
          app.kubernetes.io/name: hubble-ui
          app.kubernetes.io/part-of: cilium
          doks.digitalocean.com/managed: "true"
          k8s-app: hubble-ui
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: doks.digitalocean.com/gpu-brand
                  operator: DoesNotExist
              weight: 100
        automountServiceAccountToken: true
        containers:
        - image: ghcr.io/digitalocean-packages/hubble-ui:v0.13.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8081
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: frontend
          ports:
          - containerPort: 8081
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 8081
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/nginx/conf.d/default.conf
            name: hubble-ui-nginx-conf
            subPath: nginx.conf
          - mountPath: /tmp
            name: tmp-dir
        - env:
          - name: EVENTS_SERVER_PORT
            value: "8090"
          - name: FLOWS_API_ADDR
            value: hubble-relay:80
          image: ghcr.io/digitalocean-packages/hubble-ui-backend:v0.13.2
          imagePullPolicy: IfNotPresent
          name: backend
          ports:
          - containerPort: 8090
            name: grpc
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsUser: 1001
        serviceAccount: hubble-ui
        serviceAccountName: hubble-ui
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: nvidia.com/gpu
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: hubble-ui-nginx
          name: hubble-ui-nginx-conf
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-18T07:55:42Z"
      lastUpdateTime: "2025-12-18T07:55:42Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-12-18T07:55:42Z"
      lastUpdateTime: "2025-12-18T07:55:49Z"
      message: ReplicaSet "hubble-ui-b95c9f464" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: matomo
      meta.helm.sh/release-namespace: matomo
    creationTimestamp: "2025-12-18T11:18:10Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: matomo
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: matomo
      app.kubernetes.io/version: 5.5.1
      helm.sh/chart: matomo-2.0.6
    name: matomo
    namespace: matomo
    resourceVersion: "44056"
    uid: ee1cccdd-aeca-4f63-a3cd-b51b9da1539c
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: web
        app.kubernetes.io/instance: matomo
        app.kubernetes.io/name: matomo
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        labels:
          app.kubernetes.io/component: web
          app.kubernetes.io/instance: matomo
          app.kubernetes.io/name: matomo
      spec:
        containers:
        - env:
          - name: MATOMO_DATABASE_HOST
            value: matomo-mariadb
          - name: MATOMO_DATABASE_USERNAME
            value: matomo
          - name: MATOMO_DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: mariadb-password
                name: matomo-mariadb
          - name: MATOMO_DATABASE_DBNAME
            value: matomo
          - name: MATOMO_ENABLE_BROWSER_ARCHIVING_TRIGGERING
            value: "0"
          - name: MATOMO_ARCHIVING_RANGE_FORCE_ON_BROWSER_REQUEST
            value: "0"
          - name: MATOMO_FORCE_SSL
            value: "1"
          - name: MATOMO_FORCE_SSL_REDIRECT
            value: "1"
          - name: MATOMO_DATABASE_TABLES_PREFIX
            value: matomo_
          - name: MATOMO_DATABASE_ADAPTER
            value: PDO\MYSQL
          - name: MATOMO_GENERAL_FORCE_SSL
            value: "1"
          - name: MATOMO_GENERAL_ENABLE_BROWSER_ARCHIVING_TRIGGERING
            value: "0"
          - name: MATOMO_GENERAL_BROWSER_ARCHIVING_DISABLED_ENFORCE
            value: "1"
          - name: MATOMO_TRUSTED_HOSTS
          image: docker.io/matomo:5.5.1-apache
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            initialDelaySeconds: 60
            periodSeconds: 30
            successThreshold: 1
            tcpSocket:
              port: 80
            timeoutSeconds: 5
          name: matomo
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 80
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 500m
              ephemeral-storage: 1Gi
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/www/html
            name: matomo-data
          - mountPath: /tmp
            name: tmp-volume
          - mountPath: /usr/local/etc/php/conf.d/99-performance.ini
            name: php-config
            subPath: php-performance.ini
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/sh
          - -c
          - |
            echo "Fixing Matomo configuration..."

            # Wait for config file to exist (in case it's generated during startup)
            for i in $(seq 1 30); do
              if [ -f /var/www/html/config/config.ini.php ]; then
                break
              fi
              echo "Waiting for config file... attempt $i/30"
              sleep 2
            done

            # Clean up any old backup files that might cause integrity issues
            rm -f /var/www/html/config/config.ini.php.backup

            # Ensure critical settings are in place
            CONFIG_FILE="/var/www/html/config/config.ini.php"

            # Add force_ssl = 1 if not present
            if ! grep -q "force_ssl = 1" "$CONFIG_FILE" 2>/dev/null; then
              echo "Adding force_ssl = 1 to config..."
              sed -i '/\[General\]/a force_ssl = 1' "$CONFIG_FILE" || true
            fi

            # Ensure browser archiving is disabled
            if ! grep -q "enable_browser_archiving_triggering = 0" "$CONFIG_FILE" 2>/dev/null; then
              echo "Adding browser archiving disable to config..."
              sed -i '/\[General\]/a enable_browser_archiving_triggering = 0' "$CONFIG_FILE" || true
            fi

            if ! grep -q "archiving_range_force_on_browser_request = 0" "$CONFIG_FILE" 2>/dev/null; then
              echo "Adding archiving range disable to config..."
              sed -i '/\[General\]/a archiving_range_force_on_browser_request = 0' "$CONFIG_FILE" || true
            fi

            # Fix the root cause: cache directory ownership issues
            echo "Fixing cache directory permissions (root cause of Oops errors)..."

            # Ensure all cache directories are owned by www-data
            chown -R www-data:www-data /var/www/html/tmp/ || true
            chown -R www-data:www-data /var/www/html/tmp/templates_c/ || true
            chown -R www-data:www-data /var/www/html/tmp/cache/ || true

            # Set proper permissions for cache writing
            chmod -R 755 /var/www/html/tmp/ || true
            chmod -R 775 /var/www/html/tmp/templates_c/ || true
            chmod -R 775 /var/www/html/tmp/cache/ || true

            echo "Γ£ô Cache directory permissions fixed for www-data"

            # Fix directory privacy issues by creating proper .htaccess
            echo "Fixing tmp directory privacy..."
            cat > /var/www/html/tmp/.htaccess << 'HTACCESS_END'
            # Block all access to tmp directory for security
            <Files "*">
                Require all denied
            </Files>
            # Block directory listing
            Options -Indexes
            HTACCESS_END

            # Fix Apache AllowOverride to enable .htaccess files properly
            echo "Fixing Apache AllowOverride configuration..."
            if ! grep -q "AllowOverride All" /etc/apache2/apache2.conf; then
              # Replace AllowOverride None with AllowOverride All for /var/www/ directory
              sed -i '/^<Directory \/var\/www\/>/,/^<\/Directory>/ s/AllowOverride None/AllowOverride All/' /etc/apache2/apache2.conf
              echo "Γ£ô Apache AllowOverride enabled for .htaccess processing"
            else
              echo "Γ£ô Apache AllowOverride already enabled"
            fi

            echo "Configuration fixes applied!"
            grep -A10 "\[General\]" "$CONFIG_FILE" | head -15
          image: docker.io/matomo:5.5.1-apache
          imagePullPolicy: IfNotPresent
          name: config-fix
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/www/html
            name: matomo-data
          - mountPath: /tmp
            name: tmp-volume
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: matomo-data
          persistentVolumeClaim:
            claimName: matomo
        - emptyDir: {}
          name: tmp-volume
        - configMap:
            defaultMode: 420
            name: matomo-config
          name: php-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-18T11:20:06Z"
      lastUpdateTime: "2025-12-18T11:20:06Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-12-18T11:18:10Z"
      lastUpdateTime: "2025-12-18T11:22:04Z"
      message: ReplicaSet "matomo-b7d497c78" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: n8n
      meta.helm.sh/release-namespace: n8n
    creationTimestamp: "2026-01-01T22:28:52Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: n8n
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: n8n
      app.kubernetes.io/part-of: n8n
      app.kubernetes.io/version: 2.1.4
      component: main
      helm.sh/chart: n8n-2.8.1
      security.weown.xyz/compliance: SOC2-ISO42001
      security.weown.xyz/network-policy: zero-trust
    name: n8n
    namespace: n8n
    resourceVersion: "4577082"
    uid: f1044d53-bb6d-424a-8497-1acbd9cc7143
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: n8n
        app.kubernetes.io/name: n8n
        component: main
    strategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: a9cd1163d962b903539d6f4ecb193cef6ac0c4f42893a6456abed30147e0217c
          checksum/secret: 7c110ea1b6b37ea330d0154a625a43dcc9862398abea631cb35cfe66884e430c
        labels:
          app.kubernetes.io/instance: n8n
          app.kubernetes.io/name: n8n
          component: main
      spec:
        automountServiceAccountToken: false
        containers:
        - command:
          - n8n
          - start
          env:
          - name: DB_SQLITE_VACUUM_ON_STARTUP
            valueFrom:
              configMapKeyRef:
                key: DB_SQLITE_VACUUM_ON_STARTUP
                name: n8n
          - name: EXECUTIONS_DATA_MAX_AGE
            valueFrom:
              configMapKeyRef:
                key: EXECUTIONS_DATA_MAX_AGE
                name: n8n
          - name: EXECUTIONS_DATA_PRUNE
            valueFrom:
              configMapKeyRef:
                key: EXECUTIONS_DATA_PRUNE
                name: n8n
          - name: EXECUTIONS_DATA_SAVE_MANUAL_EXECUTIONS
            valueFrom:
              configMapKeyRef:
                key: EXECUTIONS_DATA_SAVE_MANUAL_EXECUTIONS
                name: n8n
          - name: EXECUTIONS_DATA_SAVE_ON_ERROR
            valueFrom:
              configMapKeyRef:
                key: EXECUTIONS_DATA_SAVE_ON_ERROR
                name: n8n
          - name: EXECUTIONS_DATA_SAVE_ON_SUCCESS
            valueFrom:
              configMapKeyRef:
                key: EXECUTIONS_DATA_SAVE_ON_SUCCESS
                name: n8n
          - name: EXECUTIONS_MODE
            valueFrom:
              configMapKeyRef:
                key: EXECUTIONS_MODE
                name: n8n
          - name: EXECUTIONS_PROCESS
            valueFrom:
              configMapKeyRef:
                key: EXECUTIONS_PROCESS
                name: n8n
          - name: N8N_BINARY_DATA_MODE
            valueFrom:
              configMapKeyRef:
                key: N8N_BINARY_DATA_MODE
                name: n8n
          - name: N8N_BINARY_DATA_TTL
            valueFrom:
              configMapKeyRef:
                key: N8N_BINARY_DATA_TTL
                name: n8n
          - name: N8N_CONCURRENCY
            valueFrom:
              configMapKeyRef:
                key: N8N_CONCURRENCY
                name: n8n
          - name: N8N_DISABLE_PRODUCTION_MAIN_PROCESS
            valueFrom:
              configMapKeyRef:
                key: N8N_DISABLE_PRODUCTION_MAIN_PROCESS
                name: n8n
          - name: N8N_HOST
            valueFrom:
              configMapKeyRef:
                key: N8N_HOST
                name: n8n
          - name: N8N_MAX_FILE_SIZE
            valueFrom:
              configMapKeyRef:
                key: N8N_MAX_FILE_SIZE
                name: n8n
          - name: N8N_METRICS
            valueFrom:
              configMapKeyRef:
                key: N8N_METRICS
                name: n8n
          - name: N8N_PORT
            valueFrom:
              configMapKeyRef:
                key: N8N_PORT
                name: n8n
          - name: N8N_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: N8N_PROTOCOL
                name: n8n
          - name: N8N_SECURE_COOKIE
            valueFrom:
              configMapKeyRef:
                key: N8N_SECURE_COOKIE
                name: n8n
          - name: NODE_FUNCTION_ALLOW_BUILTIN
            valueFrom:
              configMapKeyRef:
                key: NODE_FUNCTION_ALLOW_BUILTIN
                name: n8n
          - name: NODE_FUNCTION_ALLOW_EXTERNAL
            valueFrom:
              configMapKeyRef:
                key: NODE_FUNCTION_ALLOW_EXTERNAL
                name: n8n
          - name: QUEUE_BULL_REDIS_HOST
            valueFrom:
              configMapKeyRef:
                key: QUEUE_BULL_REDIS_HOST
                name: n8n
          - name: QUEUE_BULL_REDIS_PASSWORD
            valueFrom:
              configMapKeyRef:
                key: QUEUE_BULL_REDIS_PASSWORD
                name: n8n
          - name: QUEUE_BULL_REDIS_PORT
            valueFrom:
              configMapKeyRef:
                key: QUEUE_BULL_REDIS_PORT
                name: n8n
          - name: QUEUE_HEALTH_CHECK_ACTIVE
            valueFrom:
              configMapKeyRef:
                key: QUEUE_HEALTH_CHECK_ACTIVE
                name: n8n
          - name: WEBHOOK_URL
            valueFrom:
              configMapKeyRef:
                key: WEBHOOK_URL
                name: n8n
          - name: N8N_ENCRYPTION_KEY
            valueFrom:
              secretKeyRef:
                key: N8N_ENCRYPTION_KEY
                name: n8n
          - name: DB_TYPE
            value: sqlite
          - name: DB_SQLITE_DATABASE
            value: /home/node/.n8n/database.sqlite
          image: n8nio/n8n:2.1.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 5678
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: n8n
          ports:
          - containerPort: 5678
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 5678
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              ephemeral-storage: 1Gi
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/node/.n8n
            name: n8n-data
          - mountPath: /tmp
            name: tmp
          - mountPath: /home/node
            name: home
          - mountPath: /home/node/.n8n/custom
            name: custom-extensions
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: n8n
        serviceAccountName: n8n
        terminationGracePeriodSeconds: 30
        volumes:
        - name: n8n-data
          persistentVolumeClaim:
            claimName: n8n-data
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: home
        - emptyDir: {}
          name: custom-extensions
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-01-01T22:28:52Z"
      lastUpdateTime: "2026-01-01T22:28:52Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2026-01-01T22:28:52Z"
      lastUpdateTime: "2026-01-01T22:30:29Z"
      message: ReplicaSet "n8n-77bc44fb4" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: wordpress
      meta.helm.sh/release-namespace: wordpress
    creationTimestamp: "2025-12-18T10:57:13Z"
    generation: 3
    labels:
      app.kubernetes.io/component: wordpress
      app.kubernetes.io/instance: wordpress
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: wordpress
      app.kubernetes.io/part-of: wordpress-platform
      app.kubernetes.io/version: 6.8.3
      helm.sh/chart: wordpress-3.2.6
    name: wordpress
    namespace: wordpress
    resourceVersion: "4892327"
    uid: a471dd15-4497-4ad8-8536-90dfb42962f8
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: wordpress
        app.kubernetes.io/instance: wordpress
        app.kubernetes.io/name: wordpress
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          checksum/configmap: c0245934eef0cacc7d5d0b534389fb64ec1dc6d03d2dd13adac301678590e257
          checksum/mu-plugins: b53395ea900f6b43591031d16e7125384322c5e440a6b0745e9b564e4da741e0
          checksum/secret: 9e9a0b6dd3425ea6cf0db1e203856611ed1943360e717afc3e6cb92eacc94076
        labels:
          app.kubernetes.io/component: wordpress
          app.kubernetes.io/instance: wordpress
          app.kubernetes.io/name: wordpress
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: wordpress
                    app.kubernetes.io/instance: wordpress
                    app.kubernetes.io/name: wordpress
                topologyKey: kubernetes.io/hostname
              weight: 100
        automountServiceAccountToken: false
        containers:
        - env:
          - name: WORDPRESS_DB_HOST
            value: wordpress-mariadb
          - name: WORDPRESS_DB_PORT
            value: "3306"
          - name: WORDPRESS_DB_NAME
            value: wordpress
          - name: WORDPRESS_DB_USER
            value: wordpress
          - name: WORDPRESS_DB_PASSWORD
            valueFrom:
              secretKeyRef:
                key: mariadb-password
                name: wordpress
          - name: WORDPRESS_USERNAME
          - name: WORDPRESS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: wordpress-password
                name: wordpress
          - name: WORDPRESS_EMAIL
          - name: WORDPRESS_FIRST_NAME
            value: Admin
          - name: WORDPRESS_LAST_NAME
            value: User
          - name: WORDPRESS_BLOG_NAME
            value: Enterprise WordPress Site
          - name: WORDPRESS_SCHEME
            value: https
          - name: WORDPRESS_TABLE_PREFIX
            value: wp_
          - name: WORDPRESS_HTACCESS_OVERRIDE_NONE
            value: "no"
          - name: WORDPRESS_ENABLE_HTACCESS_PERSISTENCE
            value: "yes"
          - name: WORDPRESS_AUTO_UPDATE_LEVEL
            value: minor
          - name: WORDPRESS_ENABLE_REVERSE_PROXY
            value: "yes"
          - name: WORDPRESS_CONFIG_EXTRA
            value: |
              define('WP_MEMORY_LIMIT', '256M');
              define('WP_MAX_MEMORY_LIMIT', '512M');

              // WordPress configuration
              define('WP_DEBUG', false);
              define('WP_DEBUG_LOG', false);
              define('WP_DEBUG_DISPLAY', false);
              define('AUTOMATIC_UPDATER_DISABLED', false);
              define('WP_AUTO_UPDATE_CORE', 'minor');
              define('DISALLOW_FILE_EDIT', true);
              define('FORCE_SSL_ADMIN', true);
              define('WP_MEMORY_LIMIT', '256M');

              // Disable WP-Cron (handled by Kubernetes CronJob)
              define('DISABLE_WP_CRON', true);

              // Performance optimizations
              define('WP_CACHE', true);
              define('WP_POST_REVISIONS', 3);
              define('AUTOSAVE_INTERVAL', 300);
              define('EMPTY_TRASH_DAYS', 7);

              // Security hardening
              ini_set('session.cookie_httponly', true);
              ini_set('session.cookie_secure', true);
              ini_set('session.use_only_cookies', true);

              // Enable object caching
              define('WP_CACHE_KEY_SALT', 'wordpress_cache_');

              // SMTP Configuration for reliable email delivery
              define('SMTP_FROM_EMAIL', 'dhruv@weown.email');
              define('SMTP_FROM_NAME', 'YOUR_NAME');

              // Enable WordPress mail debug logging
              define('WP_MAIL_DEBUG', false);


              // WordPress security keys (auto-generated)
              define('AUTH_KEY', '');
              define('SECURE_AUTH_KEY', '');
              define('LOGGED_IN_KEY', '');
              define('NONCE_KEY', '');
              define('AUTH_SALT', '');
              define('SECURE_AUTH_SALT', '');
              define('LOGGED_IN_SALT', '');
              define('NONCE_SALT', '');
          - name: WORDPRESS_ENABLE_REDIS
            value: "yes"
          - name: REDIS_HOST
            value: wordpress-redis-master
          - name: REDIS_PORT
            value: "6379"
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis-password
                name: wordpress
          image: docker.io/wordpress:6.8.3-php8.3-apache
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 120
            periodSeconds: 30
            successThreshold: 1
            tcpSocket:
              port: 80
            timeoutSeconds: 10
          name: wordpress
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 10
            initialDelaySeconds: 60
            periodSeconds: 15
            successThreshold: 1
            tcpSocket:
              port: 80
            timeoutSeconds: 10
          resources:
            limits:
              cpu: 800m
              ephemeral-storage: 800Mi
              memory: 1Gi
            requests:
              cpu: 50m
              ephemeral-storage: 200Mi
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: false
            runAsGroup: 0
            runAsNonRoot: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/www/html
            name: wordpress-core
          - mountPath: /var/www/html/wp-content
            name: wordpress-content
          - mountPath: /var/www/html/wp-config-custom
            name: wordpress-config
          - mountPath: /var/www/html/wp-content/mu-plugins
            name: mu-plugins
          - mountPath: /var/cache/wordpress
            name: wordpress-cache
          - mountPath: /tmp
            name: tmp-dir
          - mountPath: /var/log/apache2
            name: apache-logs
          - mountPath: /var/lib/php/sessions
            name: php-sessions
          - mountPath: /usr/local/etc/php/conf.d/uploads.ini
            name: php-config
            subPath: uploads.ini
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
          runAsGroup: 0
          runAsNonRoot: false
          runAsUser: 0
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: wordpress
        serviceAccountName: wordpress
        terminationGracePeriodSeconds: 30
        volumes:
        - name: wordpress-core
          persistentVolumeClaim:
            claimName: wordpress-core
        - name: wordpress-content
          persistentVolumeClaim:
            claimName: wordpress-content
        - name: wordpress-config
          persistentVolumeClaim:
            claimName: wordpress-config
        - name: wordpress-cache
          persistentVolumeClaim:
            claimName: wordpress-cache
        - configMap:
            defaultMode: 420
            name: wordpress-mu-plugins
          name: mu-plugins
        - configMap:
            defaultMode: 420
            name: wordpress-php-config
          name: php-config
        - emptyDir:
            sizeLimit: 1Gi
          name: tmp-dir
        - emptyDir:
            sizeLimit: 500Mi
          name: apache-logs
        - emptyDir:
            sizeLimit: 100Mi
          name: php-sessions
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-01-02T22:37:00Z"
      lastUpdateTime: "2026-01-02T22:37:00Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-12-18T10:57:13Z"
      lastUpdateTime: "2026-01-02T22:37:00Z"
      message: ReplicaSet "wordpress-54fd6bfbc5" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: anythingllm
      meta.helm.sh/release-namespace: anything-llm
    creationTimestamp: "2026-01-19T21:27:59Z"
    generation: 1
    labels:
      app.kubernetes.io/component: ai-assistant
      app.kubernetes.io/instance: anythingllm
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: anythingllm
      app.kubernetes.io/part-of: weown-mvp
      app.kubernetes.io/version: 1.9.0
      helm.sh/chart: anythingllm-2.0.2
      pod-template-hash: 647f966459
    name: anythingllm-647f966459
    namespace: anything-llm
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: anythingllm
      uid: 46848552-8ee3-4975-a02e-ade5ee37d9b0
    resourceVersion: "10206336"
    uid: d4083395-0d02-4215-8dcb-167598248593
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: anythingllm
        app.kubernetes.io/name: anythingllm
        pod-template-hash: 647f966459
    template:
      metadata:
        labels:
          app.kubernetes.io/component: ai-assistant
          app.kubernetes.io/instance: anythingllm
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: anythingllm
          app.kubernetes.io/part-of: weown-mvp
          app.kubernetes.io/version: 1.9.0
          helm.sh/chart: anythingllm-2.0.2
          pod-template-hash: 647f966459
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: DISABLE_TELEMETRY
            value: "true"
          - name: EMBEDDING_ENGINE
            value: native
          - name: LLM_PROVIDER
            value: openai
          - name: SERVER_PORT
            value: "3001"
          - name: STORAGE_DIR
            value: /app/server/storage
          - name: VECTOR_DB
            value: lancedb
          envFrom:
          - secretRef:
              name: anythingllm-secrets
          image: mintplexlabs/anythingllm:1.9.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /api/ping
              port: 3001
              scheme: HTTP
            initialDelaySeconds: 120
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: anythingllm
          ports:
          - containerPort: 3001
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /api/ping
              port: 3001
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 200m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - SYS_ADMIN
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/server/storage
            name: anythingllm-storage
          - mountPath: /tmp
            name: tmp-dir
          - mountPath: /app/.cache
            name: cache-dir
          - mountPath: /collector
            name: anythingllm-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: anythingllm
        serviceAccountName: anythingllm
        terminationGracePeriodSeconds: 30
        volumes:
        - name: anythingllm-storage
          persistentVolumeClaim:
            claimName: anythingllm-storage
        - emptyDir: {}
          name: tmp-dir
        - emptyDir: {}
          name: cache-dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: anythingllm
      meta.helm.sh/release-namespace: anything-llm
    creationTimestamp: "2025-12-18T11:03:08Z"
    generation: 2
    labels:
      app.kubernetes.io/component: ai-assistant
      app.kubernetes.io/instance: anythingllm
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: anythingllm
      app.kubernetes.io/part-of: weown-mvp
      app.kubernetes.io/version: 1.9.0
      helm.sh/chart: anythingllm-2.0.2
      pod-template-hash: 97f5f4884
    name: anythingllm-97f5f4884
    namespace: anything-llm
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: anythingllm
      uid: 46848552-8ee3-4975-a02e-ade5ee37d9b0
    resourceVersion: "10205972"
    uid: 1fcea396-883e-4475-a438-0993af596210
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: anythingllm
        app.kubernetes.io/name: anythingllm
        pod-template-hash: 97f5f4884
    template:
      metadata:
        labels:
          app.kubernetes.io/component: ai-assistant
          app.kubernetes.io/instance: anythingllm
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: anythingllm
          app.kubernetes.io/part-of: weown-mvp
          app.kubernetes.io/version: 1.9.0
          helm.sh/chart: anythingllm-2.0.2
          pod-template-hash: 97f5f4884
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: DISABLE_TELEMETRY
            value: "true"
          - name: EMBEDDING_ENGINE
            value: native
          - name: LLM_PROVIDER
            value: openai
          - name: SERVER_PORT
            value: "3001"
          - name: STORAGE_DIR
            value: /app/server/storage
          - name: VECTOR_DB
            value: lancedb
          envFrom:
          - secretRef:
              name: anythingllm-secrets
          image: mintplexlabs/anythingllm:1.9.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /api/ping
              port: 3001
              scheme: HTTP
            initialDelaySeconds: 120
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: anythingllm
          ports:
          - containerPort: 3001
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /api/ping
              port: 3001
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 200m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - SYS_ADMIN
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/server/storage
            name: anythingllm-storage
          - mountPath: /tmp
            name: tmp-dir
          - mountPath: /app/.cache
            name: cache-dir
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: anythingllm
        serviceAccountName: anythingllm
        terminationGracePeriodSeconds: 30
        volumes:
        - name: anythingllm-storage
          persistentVolumeClaim:
            claimName: anythingllm-storage
        - emptyDir: {}
          name: tmp-dir
        - emptyDir: {}
          name: cache-dir
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-18T11:33:14Z"
    generation: 1
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.13.2
      helm.sh/chart: cert-manager-v1.13.2
      pod-template-hash: 84f97659c5
    name: cert-manager-84f97659c5
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager
      uid: 48afe8cc-1412-4bff-a58c-bc10d3bed5a8
    resourceVersion: "46905"
    uid: 1e783221-38ae-423d-8e9c-9bc51b6f67db
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cert-manager
        pod-template-hash: 84f97659c5
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-18T16:33:11+05:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        labels:
          app: cert-manager
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cert-manager
          app.kubernetes.io/version: v1.13.2
          helm.sh/chart: cert-manager-v1.13.2
          pod-template-hash: 84f97659c5
      spec:
        containers:
        - args:
          - --v=2
          - --cluster-resource-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=kube-system
          - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.13.2
          - --max-concurrent-challenges=60
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.13.2
          imagePullPolicy: IfNotPresent
          name: cert-manager-controller
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          - containerPort: 9403
            name: http-healthz
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager
        serviceAccountName: cert-manager
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-18T10:55:15Z"
    generation: 2
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.13.2
      helm.sh/chart: cert-manager-v1.13.2
      pod-template-hash: 9f5b8c779
    name: cert-manager-9f5b8c779
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager
      uid: 48afe8cc-1412-4bff-a58c-bc10d3bed5a8
    resourceVersion: "46924"
    uid: b924fc53-c84a-45db-89b5-ba9cabbc15f2
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cert-manager
        pod-template-hash: 9f5b8c779
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        labels:
          app: cert-manager
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cert-manager
          app.kubernetes.io/version: v1.13.2
          helm.sh/chart: cert-manager-v1.13.2
          pod-template-hash: 9f5b8c779
      spec:
        containers:
        - args:
          - --v=2
          - --cluster-resource-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=kube-system
          - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.13.2
          - --max-concurrent-challenges=60
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.13.2
          imagePullPolicy: IfNotPresent
          name: cert-manager-controller
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          - containerPort: 9403
            name: http-healthz
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager
        serviceAccountName: cert-manager
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-18T10:55:15Z"
    generation: 2
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.13.2
      helm.sh/chart: cert-manager-v1.13.2
      pod-template-hash: 6fb7b4d4
    name: cert-manager-cainjector-6fb7b4d4
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-cainjector
      uid: 53cb98a1-a089-4b08-abc6-988b20896c21
    resourceVersion: "46976"
    uid: cc975cb4-1d32-4cc8-a219-1abc7e95fb4a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cainjector
        pod-template-hash: 6fb7b4d4
    template:
      metadata:
        labels:
          app: cainjector
          app.kubernetes.io/component: cainjector
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cainjector
          app.kubernetes.io/version: v1.13.2
          helm.sh/chart: cert-manager-v1.13.2
          pod-template-hash: 6fb7b4d4
      spec:
        containers:
        - args:
          - --v=2
          - --leader-election-namespace=kube-system
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.13.2
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-cainjector
        serviceAccountName: cert-manager-cainjector
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-18T11:33:16Z"
    generation: 1
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.13.2
      helm.sh/chart: cert-manager-v1.13.2
      pod-template-hash: 8499fb697f
    name: cert-manager-cainjector-8499fb697f
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-cainjector
      uid: 53cb98a1-a089-4b08-abc6-988b20896c21
    resourceVersion: "46968"
    uid: ada12946-dc49-4fdf-baa5-010463d6df62
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cainjector
        pod-template-hash: 8499fb697f
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-18T16:33:13+05:00"
        labels:
          app: cainjector
          app.kubernetes.io/component: cainjector
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cainjector
          app.kubernetes.io/version: v1.13.2
          helm.sh/chart: cert-manager-v1.13.2
          pod-template-hash: 8499fb697f
      spec:
        containers:
        - args:
          - --v=2
          - --leader-election-namespace=kube-system
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.13.2
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-cainjector
        serviceAccountName: cert-manager-cainjector
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-18T11:33:18Z"
    generation: 1
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.13.2
      helm.sh/chart: cert-manager-v1.13.2
      pod-template-hash: 59ff465c5c
    name: cert-manager-webhook-59ff465c5c
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-webhook
      uid: df4aadee-6086-4554-ae03-5d47645a3d3d
    resourceVersion: "47020"
    uid: 6474f8b3-243c-427f-9006-da2bcf5eb229
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: webhook
        pod-template-hash: 59ff465c5c
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-18T16:33:15+05:00"
        labels:
          app: webhook
          app.kubernetes.io/component: webhook
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: webhook
          app.kubernetes.io/version: v1.13.2
          helm.sh/chart: cert-manager-v1.13.2
          pod-template-hash: 59ff465c5c
      spec:
        containers:
        - args:
          - --v=2
          - --secure-port=10250
          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
          - --dynamic-serving-dns-names=cert-manager-webhook
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.13.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          - containerPort: 6080
            name: healthcheck
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-webhook
        serviceAccountName: cert-manager-webhook
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-18T10:55:15Z"
    generation: 2
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.13.2
      helm.sh/chart: cert-manager-v1.13.2
      pod-template-hash: 7965b94d98
    name: cert-manager-webhook-7965b94d98
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-webhook
      uid: df4aadee-6086-4554-ae03-5d47645a3d3d
    resourceVersion: "47031"
    uid: 274d3040-a7e7-4b0c-a434-3d2100d29f40
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: webhook
        pod-template-hash: 7965b94d98
    template:
      metadata:
        labels:
          app: webhook
          app.kubernetes.io/component: webhook
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: webhook
          app.kubernetes.io/version: v1.13.2
          helm.sh/chart: cert-manager-v1.13.2
          pod-template-hash: 7965b94d98
      spec:
        containers:
        - args:
          - --v=2
          - --secure-port=10250
          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
          - --dynamic-serving-dns-names=cert-manager-webhook
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.13.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          - containerPort: 6080
            name: healthcheck
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-webhook
        serviceAccountName: cert-manager-webhook
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2025-12-18T10:51:35Z"
    generation: 1
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.1
      helm.sh/chart: ingress-nginx-4.14.1
      pod-template-hash: 8878d8bfd
    name: ingress-nginx-controller-8878d8bfd
    namespace: ingress-nginx
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: ingress-nginx-controller
      uid: 10689848-7008-45be-945a-323724e6cf72
    resourceVersion: "36553"
    uid: dfecd7b7-a3a6-4fed-8b2b-42373885e6fa
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
        pod-template-hash: 8878d8bfd
    template:
      metadata:
        annotations:
          prometheus.io/port: "10254"
          prometheus.io/scrape: "true"
        labels:
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: ingress-nginx
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: ingress-nginx
          app.kubernetes.io/part-of: ingress-nginx
          app.kubernetes.io/version: 1.14.1
          helm.sh/chart: ingress-nginx-4.14.1
          pod-template-hash: 8878d8bfd
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - /nginx-ingress-controller
          - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
          - --election-id=ingress-nginx-leader
          - --controller-class=k8s.io/ingress-nginx
          - --ingress-class=nginx
          - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
          - --validating-webhook=:8443
          - --validating-webhook-certificate=/usr/local/certificates/cert
          - --validating-webhook-key=/usr/local/certificates/key
          - --enable-metrics=true
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: LD_PRELOAD
            value: /usr/local/lib/libmimalloc.so
          image: registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /wait-shutdown
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: controller
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            name: https
            protocol: TCP
          - containerPort: 10254
            name: metrics
            protocol: TCP
          - containerPort: 8443
            name: webhook
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 90Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsGroup: 82
            runAsNonRoot: true
            runAsUser: 101
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /usr/local/certificates/
            name: webhook-cert
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: ingress-nginx
        serviceAccountName: ingress-nginx
        terminationGracePeriodSeconds: 300
        volumes:
        - name: webhook-cert
          secret:
            defaultMode: 420
            secretName: ingress-nginx-admission
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "4"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-12-18T07:56:07Z"
    generation: 1
    labels:
      doks.digitalocean.com/managed: "true"
      k8s-app: kube-dns
      pod-template-hash: 7c475d69
    name: coredns-7c475d69
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: coredns
      uid: d50a3672-37af-480b-8881-bdd39139df0e
    resourceVersion: "1437"
    uid: 9c459965-ff5f-4bd7-b00d-ad8363dc000d
  spec:
    replicas: 2
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: 7c475d69
    template:
      metadata:
        labels:
          doks.digitalocean.com/managed: "true"
          k8s-app: kube-dns
          pod-template-hash: 7c475d69
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: doks.digitalocean.com/gpu-brand
                  operator: DoesNotExist
              weight: 50
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - kube-dns
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: ghcr.io/digitalocean-packages/coredns/coredns:1.12.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 170Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
          - mountPath: /etc/coredns/custom
            name: custom-config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - key: nvidia.com/gpu
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
        - configMap:
            defaultMode: 420
            name: coredns-custom
            optional: true
          name: custom-config-volume
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-12-18T07:53:39Z"
    generation: 1
    labels:
      app.kubernetes.io/name: hubble-relay
      app.kubernetes.io/part-of: cilium
      doks.digitalocean.com/managed: "true"
      k8s-app: hubble-relay
      pod-template-hash: 7d6bc4d6f4
    name: hubble-relay-7d6bc4d6f4
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: hubble-relay
      uid: 16bbf56c-1413-4fce-9b6b-922fb203979b
    resourceVersion: "1172"
    uid: ff22318f-5320-41d3-8443-95f027f90086
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: hubble-relay
        pod-template-hash: 7d6bc4d6f4
    template:
      metadata:
        annotations:
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
          clusterlint.digitalocean.com/disabled-checks: resource-requirements
        labels:
          app.kubernetes.io/name: hubble-relay
          app.kubernetes.io/part-of: cilium
          doks.digitalocean.com/managed: "true"
          k8s-app: hubble-relay
          pod-template-hash: 7d6bc4d6f4
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: doks.digitalocean.com/gpu-brand
                  operator: DoesNotExist
              weight: 100
          podAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  k8s-app: cilium
              topologyKey: kubernetes.io/hostname
        automountServiceAccountToken: false
        containers:
        - args:
          - serve
          command:
          - hubble-relay
          image: ghcr.io/digitalocean-packages/hubble-relay:v1.18.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 12
            grpc:
              port: 4222
              service: ""
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: hubble-relay
          ports:
          - containerPort: 4245
            name: grpc
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsGroup: 65532
            runAsNonRoot: true
            runAsUser: 65532
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/hubble-relay
            name: config
            readOnly: true
          - mountPath: /var/lib/hubble-relay/tls
            name: tls
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65532
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: hubble-relay
        serviceAccountName: hubble-relay
        terminationGracePeriodSeconds: 1
        tolerations:
        - key: nvidia.com/gpu
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: config.yaml
              path: config.yaml
            name: hubble-relay-config
          name: config
        - name: tls
          projected:
            defaultMode: 256
            sources:
            - secret:
                items:
                - key: tls.crt
                  path: client.crt
                - key: tls.key
                  path: client.key
                - key: ca.crt
                  path: hubble-server-ca.crt
                name: hubble-relay-client-certs
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-12-18T07:55:42Z"
    generation: 1
    labels:
      app.kubernetes.io/name: hubble-ui
      app.kubernetes.io/part-of: cilium
      doks.digitalocean.com/managed: "true"
      k8s-app: hubble-ui
      pod-template-hash: b95c9f464
    name: hubble-ui-b95c9f464
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: hubble-ui
      uid: fd572ef6-b79a-447d-8f12-900ab3ddad79
    resourceVersion: "1237"
    uid: d655df8b-13eb-4633-bf2c-570b2d1475c0
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: hubble-ui
        pod-template-hash: b95c9f464
    template:
      metadata:
        annotations:
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
          clusterlint.digitalocean.com/disabled-checks: resource-requirements
        labels:
          app.kubernetes.io/name: hubble-ui
          app.kubernetes.io/part-of: cilium
          doks.digitalocean.com/managed: "true"
          k8s-app: hubble-ui
          pod-template-hash: b95c9f464
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: doks.digitalocean.com/gpu-brand
                  operator: DoesNotExist
              weight: 100
        automountServiceAccountToken: true
        containers:
        - image: ghcr.io/digitalocean-packages/hubble-ui:v0.13.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8081
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: frontend
          ports:
          - containerPort: 8081
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 8081
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/nginx/conf.d/default.conf
            name: hubble-ui-nginx-conf
            subPath: nginx.conf
          - mountPath: /tmp
            name: tmp-dir
        - env:
          - name: EVENTS_SERVER_PORT
            value: "8090"
          - name: FLOWS_API_ADDR
            value: hubble-relay:80
          image: ghcr.io/digitalocean-packages/hubble-ui-backend:v0.13.2
          imagePullPolicy: IfNotPresent
          name: backend
          ports:
          - containerPort: 8090
            name: grpc
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsUser: 1001
        serviceAccount: hubble-ui
        serviceAccountName: hubble-ui
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: nvidia.com/gpu
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: hubble-ui-nginx
          name: hubble-ui-nginx-conf
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: matomo
      meta.helm.sh/release-namespace: matomo
    creationTimestamp: "2025-12-18T11:18:10Z"
    generation: 2
    labels:
      app.kubernetes.io/component: web
      app.kubernetes.io/instance: matomo
      app.kubernetes.io/name: matomo
      pod-template-hash: 7c45d4654c
    name: matomo-7c45d4654c
    namespace: matomo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: matomo
      uid: ee1cccdd-aeca-4f63-a3cd-b51b9da1539c
    resourceVersion: "44054"
    uid: f130a070-37d3-4de5-b08b-786ad228e782
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: web
        app.kubernetes.io/instance: matomo
        app.kubernetes.io/name: matomo
        pod-template-hash: 7c45d4654c
    template:
      metadata:
        labels:
          app.kubernetes.io/component: web
          app.kubernetes.io/instance: matomo
          app.kubernetes.io/name: matomo
          pod-template-hash: 7c45d4654c
      spec:
        containers:
        - env:
          - name: MATOMO_DATABASE_HOST
            value: matomo-mariadb
          - name: MATOMO_DATABASE_USERNAME
            value: mariadb-admin
          - name: MATOMO_DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: mariadb-password
                name: matomo-mariadb
          - name: MATOMO_DATABASE_DBNAME
            value: matomo
          - name: MATOMO_ENABLE_BROWSER_ARCHIVING_TRIGGERING
            value: "0"
          - name: MATOMO_ARCHIVING_RANGE_FORCE_ON_BROWSER_REQUEST
            value: "0"
          - name: MATOMO_FORCE_SSL
            value: "1"
          - name: MATOMO_FORCE_SSL_REDIRECT
            value: "1"
          - name: MATOMO_DATABASE_TABLES_PREFIX
            value: matomo_
          - name: MATOMO_DATABASE_ADAPTER
            value: PDO\MYSQL
          - name: MATOMO_GENERAL_FORCE_SSL
            value: "1"
          - name: MATOMO_GENERAL_ENABLE_BROWSER_ARCHIVING_TRIGGERING
            value: "0"
          - name: MATOMO_GENERAL_BROWSER_ARCHIVING_DISABLED_ENFORCE
            value: "1"
          - name: MATOMO_TRUSTED_HOSTS
          image: docker.io/matomo:5.5.1-apache
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            initialDelaySeconds: 60
            periodSeconds: 30
            successThreshold: 1
            tcpSocket:
              port: 80
            timeoutSeconds: 5
          name: matomo
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 80
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 500m
              ephemeral-storage: 1Gi
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/www/html
            name: matomo-data
          - mountPath: /tmp
            name: tmp-volume
          - mountPath: /usr/local/etc/php/conf.d/99-performance.ini
            name: php-config
            subPath: php-performance.ini
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/sh
          - -c
          - |
            echo "Fixing Matomo configuration..."

            # Wait for config file to exist (in case it's generated during startup)
            for i in $(seq 1 30); do
              if [ -f /var/www/html/config/config.ini.php ]; then
                break
              fi
              echo "Waiting for config file... attempt $i/30"
              sleep 2
            done

            # Clean up any old backup files that might cause integrity issues
            rm -f /var/www/html/config/config.ini.php.backup

            # Ensure critical settings are in place
            CONFIG_FILE="/var/www/html/config/config.ini.php"

            # Add force_ssl = 1 if not present
            if ! grep -q "force_ssl = 1" "$CONFIG_FILE" 2>/dev/null; then
              echo "Adding force_ssl = 1 to config..."
              sed -i '/\[General\]/a force_ssl = 1' "$CONFIG_FILE" || true
            fi

            # Ensure browser archiving is disabled
            if ! grep -q "enable_browser_archiving_triggering = 0" "$CONFIG_FILE" 2>/dev/null; then
              echo "Adding browser archiving disable to config..."
              sed -i '/\[General\]/a enable_browser_archiving_triggering = 0' "$CONFIG_FILE" || true
            fi

            if ! grep -q "archiving_range_force_on_browser_request = 0" "$CONFIG_FILE" 2>/dev/null; then
              echo "Adding archiving range disable to config..."
              sed -i '/\[General\]/a archiving_range_force_on_browser_request = 0' "$CONFIG_FILE" || true
            fi

            # Fix the root cause: cache directory ownership issues
            echo "Fixing cache directory permissions (root cause of Oops errors)..."

            # Ensure all cache directories are owned by www-data
            chown -R www-data:www-data /var/www/html/tmp/ || true
            chown -R www-data:www-data /var/www/html/tmp/templates_c/ || true
            chown -R www-data:www-data /var/www/html/tmp/cache/ || true

            # Set proper permissions for cache writing
            chmod -R 755 /var/www/html/tmp/ || true
            chmod -R 775 /var/www/html/tmp/templates_c/ || true
            chmod -R 775 /var/www/html/tmp/cache/ || true

            echo "Γ£ô Cache directory permissions fixed for www-data"

            # Fix directory privacy issues by creating proper .htaccess
            echo "Fixing tmp directory privacy..."
            cat > /var/www/html/tmp/.htaccess << 'HTACCESS_END'
            # Block all access to tmp directory for security
            <Files "*">
                Require all denied
            </Files>
            # Block directory listing
            Options -Indexes
            HTACCESS_END

            # Fix Apache AllowOverride to enable .htaccess files properly
            echo "Fixing Apache AllowOverride configuration..."
            if ! grep -q "AllowOverride All" /etc/apache2/apache2.conf; then
              # Replace AllowOverride None with AllowOverride All for /var/www/ directory
              sed -i '/^<Directory \/var\/www\/>/,/^<\/Directory>/ s/AllowOverride None/AllowOverride All/' /etc/apache2/apache2.conf
              echo "Γ£ô Apache AllowOverride enabled for .htaccess processing"
            else
              echo "Γ£ô Apache AllowOverride already enabled"
            fi

            echo "Configuration fixes applied!"
            grep -A10 "\[General\]" "$CONFIG_FILE" | head -15
          image: docker.io/matomo:5.5.1-apache
          imagePullPolicy: IfNotPresent
          name: config-fix
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/www/html
            name: matomo-data
          - mountPath: /tmp
            name: tmp-volume
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: matomo-data
          persistentVolumeClaim:
            claimName: matomo
        - emptyDir: {}
          name: tmp-volume
        - configMap:
            defaultMode: 420
            name: matomo-config
          name: php-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: matomo
      meta.helm.sh/release-namespace: matomo
    creationTimestamp: "2025-12-18T11:20:31Z"
    generation: 1
    labels:
      app.kubernetes.io/component: web
      app.kubernetes.io/instance: matomo
      app.kubernetes.io/name: matomo
      pod-template-hash: b7d497c78
    name: matomo-b7d497c78
    namespace: matomo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: matomo
      uid: ee1cccdd-aeca-4f63-a3cd-b51b9da1539c
    resourceVersion: "44043"
    uid: 6096c450-7cd2-4d49-b4f6-485bdfe29a6d
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: web
        app.kubernetes.io/instance: matomo
        app.kubernetes.io/name: matomo
        pod-template-hash: b7d497c78
    template:
      metadata:
        labels:
          app.kubernetes.io/component: web
          app.kubernetes.io/instance: matomo
          app.kubernetes.io/name: matomo
          pod-template-hash: b7d497c78
      spec:
        containers:
        - env:
          - name: MATOMO_DATABASE_HOST
            value: matomo-mariadb
          - name: MATOMO_DATABASE_USERNAME
            value: matomo
          - name: MATOMO_DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: mariadb-password
                name: matomo-mariadb
          - name: MATOMO_DATABASE_DBNAME
            value: matomo
          - name: MATOMO_ENABLE_BROWSER_ARCHIVING_TRIGGERING
            value: "0"
          - name: MATOMO_ARCHIVING_RANGE_FORCE_ON_BROWSER_REQUEST
            value: "0"
          - name: MATOMO_FORCE_SSL
            value: "1"
          - name: MATOMO_FORCE_SSL_REDIRECT
            value: "1"
          - name: MATOMO_DATABASE_TABLES_PREFIX
            value: matomo_
          - name: MATOMO_DATABASE_ADAPTER
            value: PDO\MYSQL
          - name: MATOMO_GENERAL_FORCE_SSL
            value: "1"
          - name: MATOMO_GENERAL_ENABLE_BROWSER_ARCHIVING_TRIGGERING
            value: "0"
          - name: MATOMO_GENERAL_BROWSER_ARCHIVING_DISABLED_ENFORCE
            value: "1"
          - name: MATOMO_TRUSTED_HOSTS
          image: docker.io/matomo:5.5.1-apache
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            initialDelaySeconds: 60
            periodSeconds: 30
            successThreshold: 1
            tcpSocket:
              port: 80
            timeoutSeconds: 5
          name: matomo
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 80
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 500m
              ephemeral-storage: 1Gi
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/www/html
            name: matomo-data
          - mountPath: /tmp
            name: tmp-volume
          - mountPath: /usr/local/etc/php/conf.d/99-performance.ini
            name: php-config
            subPath: php-performance.ini
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/sh
          - -c
          - |
            echo "Fixing Matomo configuration..."

            # Wait for config file to exist (in case it's generated during startup)
            for i in $(seq 1 30); do
              if [ -f /var/www/html/config/config.ini.php ]; then
                break
              fi
              echo "Waiting for config file... attempt $i/30"
              sleep 2
            done

            # Clean up any old backup files that might cause integrity issues
            rm -f /var/www/html/config/config.ini.php.backup

            # Ensure critical settings are in place
            CONFIG_FILE="/var/www/html/config/config.ini.php"

            # Add force_ssl = 1 if not present
            if ! grep -q "force_ssl = 1" "$CONFIG_FILE" 2>/dev/null; then
              echo "Adding force_ssl = 1 to config..."
              sed -i '/\[General\]/a force_ssl = 1' "$CONFIG_FILE" || true
            fi

            # Ensure browser archiving is disabled
            if ! grep -q "enable_browser_archiving_triggering = 0" "$CONFIG_FILE" 2>/dev/null; then
              echo "Adding browser archiving disable to config..."
              sed -i '/\[General\]/a enable_browser_archiving_triggering = 0' "$CONFIG_FILE" || true
            fi

            if ! grep -q "archiving_range_force_on_browser_request = 0" "$CONFIG_FILE" 2>/dev/null; then
              echo "Adding archiving range disable to config..."
              sed -i '/\[General\]/a archiving_range_force_on_browser_request = 0' "$CONFIG_FILE" || true
            fi

            # Fix the root cause: cache directory ownership issues
            echo "Fixing cache directory permissions (root cause of Oops errors)..."

            # Ensure all cache directories are owned by www-data
            chown -R www-data:www-data /var/www/html/tmp/ || true
            chown -R www-data:www-data /var/www/html/tmp/templates_c/ || true
            chown -R www-data:www-data /var/www/html/tmp/cache/ || true

            # Set proper permissions for cache writing
            chmod -R 755 /var/www/html/tmp/ || true
            chmod -R 775 /var/www/html/tmp/templates_c/ || true
            chmod -R 775 /var/www/html/tmp/cache/ || true

            echo "Γ£ô Cache directory permissions fixed for www-data"

            # Fix directory privacy issues by creating proper .htaccess
            echo "Fixing tmp directory privacy..."
            cat > /var/www/html/tmp/.htaccess << 'HTACCESS_END'
            # Block all access to tmp directory for security
            <Files "*">
                Require all denied
            </Files>
            # Block directory listing
            Options -Indexes
            HTACCESS_END

            # Fix Apache AllowOverride to enable .htaccess files properly
            echo "Fixing Apache AllowOverride configuration..."
            if ! grep -q "AllowOverride All" /etc/apache2/apache2.conf; then
              # Replace AllowOverride None with AllowOverride All for /var/www/ directory
              sed -i '/^<Directory \/var\/www\/>/,/^<\/Directory>/ s/AllowOverride None/AllowOverride All/' /etc/apache2/apache2.conf
              echo "Γ£ô Apache AllowOverride enabled for .htaccess processing"
            else
              echo "Γ£ô Apache AllowOverride already enabled"
            fi

            echo "Configuration fixes applied!"
            grep -A10 "\[General\]" "$CONFIG_FILE" | head -15
          image: docker.io/matomo:5.5.1-apache
          imagePullPolicy: IfNotPresent
          name: config-fix
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/www/html
            name: matomo-data
          - mountPath: /tmp
            name: tmp-volume
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: matomo-data
          persistentVolumeClaim:
            claimName: matomo
        - emptyDir: {}
          name: tmp-volume
        - configMap:
            defaultMode: 420
            name: matomo-config
          name: php-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: n8n
      meta.helm.sh/release-namespace: n8n
    creationTimestamp: "2026-01-01T22:28:52Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: n8n
      app.kubernetes.io/name: n8n
      component: main
      pod-template-hash: 77bc44fb4
    name: n8n-77bc44fb4
    namespace: n8n
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: n8n
      uid: f1044d53-bb6d-424a-8497-1acbd9cc7143
    resourceVersion: "4577081"
    uid: 1663d2e7-f5d5-436a-837e-a8ee2de836ec
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: n8n
        app.kubernetes.io/name: n8n
        component: main
        pod-template-hash: 77bc44fb4
    template:
      metadata:
        annotations:
          checksum/config: a9cd1163d962b903539d6f4ecb193cef6ac0c4f42893a6456abed30147e0217c
          checksum/secret: 7c110ea1b6b37ea330d0154a625a43dcc9862398abea631cb35cfe66884e430c
        labels:
          app.kubernetes.io/instance: n8n
          app.kubernetes.io/name: n8n
          component: main
          pod-template-hash: 77bc44fb4
      spec:
        automountServiceAccountToken: false
        containers:
        - command:
          - n8n
          - start
          env:
          - name: DB_SQLITE_VACUUM_ON_STARTUP
            valueFrom:
              configMapKeyRef:
                key: DB_SQLITE_VACUUM_ON_STARTUP
                name: n8n
          - name: EXECUTIONS_DATA_MAX_AGE
            valueFrom:
              configMapKeyRef:
                key: EXECUTIONS_DATA_MAX_AGE
                name: n8n
          - name: EXECUTIONS_DATA_PRUNE
            valueFrom:
              configMapKeyRef:
                key: EXECUTIONS_DATA_PRUNE
                name: n8n
          - name: EXECUTIONS_DATA_SAVE_MANUAL_EXECUTIONS
            valueFrom:
              configMapKeyRef:
                key: EXECUTIONS_DATA_SAVE_MANUAL_EXECUTIONS
                name: n8n
          - name: EXECUTIONS_DATA_SAVE_ON_ERROR
            valueFrom:
              configMapKeyRef:
                key: EXECUTIONS_DATA_SAVE_ON_ERROR
                name: n8n
          - name: EXECUTIONS_DATA_SAVE_ON_SUCCESS
            valueFrom:
              configMapKeyRef:
                key: EXECUTIONS_DATA_SAVE_ON_SUCCESS
                name: n8n
          - name: EXECUTIONS_MODE
            valueFrom:
              configMapKeyRef:
                key: EXECUTIONS_MODE
                name: n8n
          - name: EXECUTIONS_PROCESS
            valueFrom:
              configMapKeyRef:
                key: EXECUTIONS_PROCESS
                name: n8n
          - name: N8N_BINARY_DATA_MODE
            valueFrom:
              configMapKeyRef:
                key: N8N_BINARY_DATA_MODE
                name: n8n
          - name: N8N_BINARY_DATA_TTL
            valueFrom:
              configMapKeyRef:
                key: N8N_BINARY_DATA_TTL
                name: n8n
          - name: N8N_CONCURRENCY
            valueFrom:
              configMapKeyRef:
                key: N8N_CONCURRENCY
                name: n8n
          - name: N8N_DISABLE_PRODUCTION_MAIN_PROCESS
            valueFrom:
              configMapKeyRef:
                key: N8N_DISABLE_PRODUCTION_MAIN_PROCESS
                name: n8n
          - name: N8N_HOST
            valueFrom:
              configMapKeyRef:
                key: N8N_HOST
                name: n8n
          - name: N8N_MAX_FILE_SIZE
            valueFrom:
              configMapKeyRef:
                key: N8N_MAX_FILE_SIZE
                name: n8n
          - name: N8N_METRICS
            valueFrom:
              configMapKeyRef:
                key: N8N_METRICS
                name: n8n
          - name: N8N_PORT
            valueFrom:
              configMapKeyRef:
                key: N8N_PORT
                name: n8n
          - name: N8N_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: N8N_PROTOCOL
                name: n8n
          - name: N8N_SECURE_COOKIE
            valueFrom:
              configMapKeyRef:
                key: N8N_SECURE_COOKIE
                name: n8n
          - name: NODE_FUNCTION_ALLOW_BUILTIN
            valueFrom:
              configMapKeyRef:
                key: NODE_FUNCTION_ALLOW_BUILTIN
                name: n8n
          - name: NODE_FUNCTION_ALLOW_EXTERNAL
            valueFrom:
              configMapKeyRef:
                key: NODE_FUNCTION_ALLOW_EXTERNAL
                name: n8n
          - name: QUEUE_BULL_REDIS_HOST
            valueFrom:
              configMapKeyRef:
                key: QUEUE_BULL_REDIS_HOST
                name: n8n
          - name: QUEUE_BULL_REDIS_PASSWORD
            valueFrom:
              configMapKeyRef:
                key: QUEUE_BULL_REDIS_PASSWORD
                name: n8n
          - name: QUEUE_BULL_REDIS_PORT
            valueFrom:
              configMapKeyRef:
                key: QUEUE_BULL_REDIS_PORT
                name: n8n
          - name: QUEUE_HEALTH_CHECK_ACTIVE
            valueFrom:
              configMapKeyRef:
                key: QUEUE_HEALTH_CHECK_ACTIVE
                name: n8n
          - name: WEBHOOK_URL
            valueFrom:
              configMapKeyRef:
                key: WEBHOOK_URL
                name: n8n
          - name: N8N_ENCRYPTION_KEY
            valueFrom:
              secretKeyRef:
                key: N8N_ENCRYPTION_KEY
                name: n8n
          - name: DB_TYPE
            value: sqlite
          - name: DB_SQLITE_DATABASE
            value: /home/node/.n8n/database.sqlite
          image: n8nio/n8n:2.1.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 5678
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: n8n
          ports:
          - containerPort: 5678
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 5678
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              ephemeral-storage: 1Gi
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/node/.n8n
            name: n8n-data
          - mountPath: /tmp
            name: tmp
          - mountPath: /home/node
            name: home
          - mountPath: /home/node/.n8n/custom
            name: custom-extensions
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: n8n
        serviceAccountName: n8n
        terminationGracePeriodSeconds: 30
        volumes:
        - name: n8n-data
          persistentVolumeClaim:
            claimName: n8n-data
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: home
        - emptyDir: {}
          name: custom-extensions
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: wordpress
      meta.helm.sh/release-namespace: wordpress
    creationTimestamp: "2026-01-02T22:35:55Z"
    generation: 1
    labels:
      app.kubernetes.io/component: wordpress
      app.kubernetes.io/instance: wordpress
      app.kubernetes.io/name: wordpress
      pod-template-hash: 54fd6bfbc5
    name: wordpress-54fd6bfbc5
    namespace: wordpress
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: wordpress
      uid: a471dd15-4497-4ad8-8536-90dfb42962f8
    resourceVersion: "4892326"
    uid: 5151a925-bd00-46a5-8a89-e1216843fbf8
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: wordpress
        app.kubernetes.io/instance: wordpress
        app.kubernetes.io/name: wordpress
        pod-template-hash: 54fd6bfbc5
    template:
      metadata:
        annotations:
          checksum/configmap: c0245934eef0cacc7d5d0b534389fb64ec1dc6d03d2dd13adac301678590e257
          checksum/mu-plugins: b53395ea900f6b43591031d16e7125384322c5e440a6b0745e9b564e4da741e0
          checksum/secret: 9e9a0b6dd3425ea6cf0db1e203856611ed1943360e717afc3e6cb92eacc94076
        labels:
          app.kubernetes.io/component: wordpress
          app.kubernetes.io/instance: wordpress
          app.kubernetes.io/name: wordpress
          pod-template-hash: 54fd6bfbc5
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: wordpress
                    app.kubernetes.io/instance: wordpress
                    app.kubernetes.io/name: wordpress
                topologyKey: kubernetes.io/hostname
              weight: 100
        automountServiceAccountToken: false
        containers:
        - env:
          - name: WORDPRESS_DB_HOST
            value: wordpress-mariadb
          - name: WORDPRESS_DB_PORT
            value: "3306"
          - name: WORDPRESS_DB_NAME
            value: wordpress
          - name: WORDPRESS_DB_USER
            value: wordpress
          - name: WORDPRESS_DB_PASSWORD
            valueFrom:
              secretKeyRef:
                key: mariadb-password
                name: wordpress
          - name: WORDPRESS_USERNAME
          - name: WORDPRESS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: wordpress-password
                name: wordpress
          - name: WORDPRESS_EMAIL
          - name: WORDPRESS_FIRST_NAME
            value: Admin
          - name: WORDPRESS_LAST_NAME
            value: User
          - name: WORDPRESS_BLOG_NAME
            value: Enterprise WordPress Site
          - name: WORDPRESS_SCHEME
            value: https
          - name: WORDPRESS_TABLE_PREFIX
            value: wp_
          - name: WORDPRESS_HTACCESS_OVERRIDE_NONE
            value: "no"
          - name: WORDPRESS_ENABLE_HTACCESS_PERSISTENCE
            value: "yes"
          - name: WORDPRESS_AUTO_UPDATE_LEVEL
            value: minor
          - name: WORDPRESS_ENABLE_REVERSE_PROXY
            value: "yes"
          - name: WORDPRESS_CONFIG_EXTRA
            value: |
              define('WP_MEMORY_LIMIT', '256M');
              define('WP_MAX_MEMORY_LIMIT', '512M');

              // WordPress configuration
              define('WP_DEBUG', false);
              define('WP_DEBUG_LOG', false);
              define('WP_DEBUG_DISPLAY', false);
              define('AUTOMATIC_UPDATER_DISABLED', false);
              define('WP_AUTO_UPDATE_CORE', 'minor');
              define('DISALLOW_FILE_EDIT', true);
              define('FORCE_SSL_ADMIN', true);
              define('WP_MEMORY_LIMIT', '256M');

              // Disable WP-Cron (handled by Kubernetes CronJob)
              define('DISABLE_WP_CRON', true);

              // Performance optimizations
              define('WP_CACHE', true);
              define('WP_POST_REVISIONS', 3);
              define('AUTOSAVE_INTERVAL', 300);
              define('EMPTY_TRASH_DAYS', 7);

              // Security hardening
              ini_set('session.cookie_httponly', true);
              ini_set('session.cookie_secure', true);
              ini_set('session.use_only_cookies', true);

              // Enable object caching
              define('WP_CACHE_KEY_SALT', 'wordpress_cache_');

              // SMTP Configuration for reliable email delivery
              define('SMTP_FROM_EMAIL', 'dhruv@weown.email');
              define('SMTP_FROM_NAME', 'YOUR_NAME');

              // Enable WordPress mail debug logging
              define('WP_MAIL_DEBUG', false);


              // WordPress security keys (auto-generated)
              define('AUTH_KEY', '');
              define('SECURE_AUTH_KEY', '');
              define('LOGGED_IN_KEY', '');
              define('NONCE_KEY', '');
              define('AUTH_SALT', '');
              define('SECURE_AUTH_SALT', '');
              define('LOGGED_IN_SALT', '');
              define('NONCE_SALT', '');
          - name: WORDPRESS_ENABLE_REDIS
            value: "yes"
          - name: REDIS_HOST
            value: wordpress-redis-master
          - name: REDIS_PORT
            value: "6379"
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis-password
                name: wordpress
          image: docker.io/wordpress:6.8.3-php8.3-apache
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 120
            periodSeconds: 30
            successThreshold: 1
            tcpSocket:
              port: 80
            timeoutSeconds: 10
          name: wordpress
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 10
            initialDelaySeconds: 60
            periodSeconds: 15
            successThreshold: 1
            tcpSocket:
              port: 80
            timeoutSeconds: 10
          resources:
            limits:
              cpu: 800m
              ephemeral-storage: 800Mi
              memory: 1Gi
            requests:
              cpu: 50m
              ephemeral-storage: 200Mi
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: false
            runAsGroup: 0
            runAsNonRoot: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/www/html
            name: wordpress-core
          - mountPath: /var/www/html/wp-content
            name: wordpress-content
          - mountPath: /var/www/html/wp-config-custom
            name: wordpress-config
          - mountPath: /var/www/html/wp-content/mu-plugins
            name: mu-plugins
          - mountPath: /var/cache/wordpress
            name: wordpress-cache
          - mountPath: /tmp
            name: tmp-dir
          - mountPath: /var/log/apache2
            name: apache-logs
          - mountPath: /var/lib/php/sessions
            name: php-sessions
          - mountPath: /usr/local/etc/php/conf.d/uploads.ini
            name: php-config
            subPath: uploads.ini
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
          runAsGroup: 0
          runAsNonRoot: false
          runAsUser: 0
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: wordpress
        serviceAccountName: wordpress
        terminationGracePeriodSeconds: 30
        volumes:
        - name: wordpress-core
          persistentVolumeClaim:
            claimName: wordpress-core
        - name: wordpress-content
          persistentVolumeClaim:
            claimName: wordpress-content
        - name: wordpress-config
          persistentVolumeClaim:
            claimName: wordpress-config
        - name: wordpress-cache
          persistentVolumeClaim:
            claimName: wordpress-cache
        - configMap:
            defaultMode: 420
            name: wordpress-mu-plugins
          name: mu-plugins
        - configMap:
            defaultMode: 420
            name: wordpress-php-config
          name: php-config
        - emptyDir:
            sizeLimit: 1Gi
          name: tmp-dir
        - emptyDir:
            sizeLimit: 500Mi
          name: apache-logs
        - emptyDir:
            sizeLimit: 100Mi
          name: php-sessions
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: wordpress
      meta.helm.sh/release-namespace: wordpress
    creationTimestamp: "2025-12-19T01:38:39Z"
    generation: 2
    labels:
      app.kubernetes.io/component: wordpress
      app.kubernetes.io/instance: wordpress
      app.kubernetes.io/name: wordpress
      pod-template-hash: 5c6d4cd7c5
    name: wordpress-5c6d4cd7c5
    namespace: wordpress
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: wordpress
      uid: a471dd15-4497-4ad8-8536-90dfb42962f8
    resourceVersion: "4892052"
    uid: 84d3e98e-91fe-426d-9419-5856a4d36d1f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: wordpress
        app.kubernetes.io/instance: wordpress
        app.kubernetes.io/name: wordpress
        pod-template-hash: 5c6d4cd7c5
    template:
      metadata:
        annotations:
          checksum/configmap: 8857430bd90ff4c7db5f66a280f63dfe200672d889e4415439592bfaa834ac31
          checksum/secret: 2358267409e6597c93a73eb24d37c9d4ef418ab7485c9c55e9325ac5d9adeada
        labels:
          app.kubernetes.io/component: wordpress
          app.kubernetes.io/instance: wordpress
          app.kubernetes.io/name: wordpress
          pod-template-hash: 5c6d4cd7c5
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: wordpress
                    app.kubernetes.io/instance: wordpress
                    app.kubernetes.io/name: wordpress
                topologyKey: kubernetes.io/hostname
              weight: 100
        automountServiceAccountToken: false
        containers:
        - env:
          - name: WORDPRESS_DB_HOST
            value: wordpress-mariadb
          - name: WORDPRESS_DB_PORT
            value: "3306"
          - name: WORDPRESS_DB_NAME
            value: wordpress
          - name: WORDPRESS_DB_USER
            value: wordpress
          - name: WORDPRESS_DB_PASSWORD
            valueFrom:
              secretKeyRef:
                key: mariadb-password
                name: wordpress
          - name: WORDPRESS_USERNAME
          - name: WORDPRESS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: wordpress-password
                name: wordpress
          - name: WORDPRESS_EMAIL
          - name: WORDPRESS_FIRST_NAME
            value: Admin
          - name: WORDPRESS_LAST_NAME
            value: User
          - name: WORDPRESS_BLOG_NAME
            value: Enterprise WordPress Site
          - name: WORDPRESS_SCHEME
            value: https
          - name: WORDPRESS_TABLE_PREFIX
            value: wp_
          - name: WORDPRESS_HTACCESS_OVERRIDE_NONE
            value: "no"
          - name: WORDPRESS_ENABLE_HTACCESS_PERSISTENCE
            value: "yes"
          - name: WORDPRESS_AUTO_UPDATE_LEVEL
            value: minor
          - name: WORDPRESS_ENABLE_REVERSE_PROXY
            value: "yes"
          - name: WORDPRESS_CONFIG_EXTRA
            value: |
              define('WP_MEMORY_LIMIT', '256M');
              define('WP_MAX_MEMORY_LIMIT', '512M');

              // WordPress configuration
              define('WP_DEBUG', false);
              define('WP_DEBUG_LOG', false);
              define('WP_DEBUG_DISPLAY', false);
              define('AUTOMATIC_UPDATER_DISABLED', false);
              define('WP_AUTO_UPDATE_CORE', 'minor');
              define('DISALLOW_FILE_EDIT', true);
              define('FORCE_SSL_ADMIN', true);
              define('WP_MEMORY_LIMIT', '256M');

              // Disable WP-Cron (handled by Kubernetes CronJob)
              define('DISABLE_WP_CRON', true);

              // Performance optimizations
              define('WP_CACHE', true);
              define('WP_POST_REVISIONS', 3);
              define('AUTOSAVE_INTERVAL', 300);
              define('EMPTY_TRASH_DAYS', 7);

              // Security hardening
              ini_set('session.cookie_httponly', true);
              ini_set('session.cookie_secure', true);
              ini_set('session.use_only_cookies', true);

              // Enable object caching
              define('WP_CACHE_KEY_SALT', 'wordpress_cache_');

              // SMTP Configuration for reliable email delivery
              define('SMTP_FROM_EMAIL', 'dhruv@weown.email');
              define('SMTP_FROM_NAME', 'YOUR_NAME');

              // Enable WordPress mail debug logging
              define('WP_MAIL_DEBUG', false);


              // WordPress security keys (auto-generated)
              define('AUTH_KEY', '');
              define('SECURE_AUTH_KEY', '');
              define('LOGGED_IN_KEY', '');
              define('NONCE_KEY', '');
              define('AUTH_SALT', '');
              define('SECURE_AUTH_SALT', '');
              define('LOGGED_IN_SALT', '');
              define('NONCE_SALT', '');
          - name: WORDPRESS_ENABLE_REDIS
            value: "yes"
          - name: REDIS_HOST
            value: wordpress-redis-master
          - name: REDIS_PORT
            value: "6379"
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis-password
                name: wordpress
          image: docker.io/wordpress:6.8.3-php8.3-apache
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 120
            periodSeconds: 30
            successThreshold: 1
            tcpSocket:
              port: 80
            timeoutSeconds: 10
          name: wordpress
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 10
            initialDelaySeconds: 60
            periodSeconds: 15
            successThreshold: 1
            tcpSocket:
              port: 80
            timeoutSeconds: 10
          resources:
            limits:
              cpu: 800m
              ephemeral-storage: 800Mi
              memory: 1Gi
            requests:
              cpu: 50m
              ephemeral-storage: 200Mi
              memory: 384Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: false
            runAsGroup: 0
            runAsNonRoot: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/www/html
            name: wordpress-core
          - mountPath: /var/www/html/wp-content
            name: wordpress-content
          - mountPath: /var/www/html/wp-config-custom
            name: wordpress-config
          - mountPath: /var/www/html/wp-content/mu-plugins
            name: cache-plugin
          - mountPath: /var/cache/wordpress
            name: wordpress-cache
          - mountPath: /tmp
            name: tmp-dir
          - mountPath: /var/log/apache2
            name: apache-logs
          - mountPath: /var/lib/php/sessions
            name: php-sessions
          - mountPath: /usr/local/etc/php/conf.d/uploads.ini
            name: php-config
            subPath: uploads.ini
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
          runAsGroup: 0
          runAsNonRoot: false
          runAsUser: 0
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: wordpress
        serviceAccountName: wordpress
        terminationGracePeriodSeconds: 30
        volumes:
        - name: wordpress-core
          persistentVolumeClaim:
            claimName: wordpress-core
        - name: wordpress-content
          persistentVolumeClaim:
            claimName: wordpress-content
        - name: wordpress-config
          persistentVolumeClaim:
            claimName: wordpress-config
        - name: wordpress-cache
          persistentVolumeClaim:
            claimName: wordpress-cache
        - configMap:
            defaultMode: 420
            name: wordpress-cache-plugin
          name: cache-plugin
        - configMap:
            defaultMode: 420
            name: wordpress-php-config
          name: php-config
        - emptyDir:
            sizeLimit: 1Gi
          name: tmp-dir
        - emptyDir:
            sizeLimit: 500Mi
          name: apache-logs
        - emptyDir:
            sizeLimit: 100Mi
          name: php-sessions
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: wordpress
      meta.helm.sh/release-namespace: wordpress
    creationTimestamp: "2025-12-18T10:57:13Z"
    generation: 2
    labels:
      app.kubernetes.io/component: wordpress
      app.kubernetes.io/instance: wordpress
      app.kubernetes.io/name: wordpress
      pod-template-hash: 65c8f998f9
    name: wordpress-65c8f998f9
    namespace: wordpress
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: wordpress
      uid: a471dd15-4497-4ad8-8536-90dfb42962f8
    resourceVersion: "231127"
    uid: f9d76989-a07b-4f34-8cd6-e703b4720235
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: wordpress
        app.kubernetes.io/instance: wordpress
        app.kubernetes.io/name: wordpress
        pod-template-hash: 65c8f998f9
    template:
      metadata:
        annotations:
          checksum/configmap: 8857430bd90ff4c7db5f66a280f63dfe200672d889e4415439592bfaa834ac31
          checksum/secret: 2358267409e6597c93a73eb24d37c9d4ef418ab7485c9c55e9325ac5d9adeada
        labels:
          app.kubernetes.io/component: wordpress
          app.kubernetes.io/instance: wordpress
          app.kubernetes.io/name: wordpress
          pod-template-hash: 65c8f998f9
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: wordpress
                    app.kubernetes.io/instance: wordpress
                    app.kubernetes.io/name: wordpress
                topologyKey: kubernetes.io/hostname
              weight: 100
        automountServiceAccountToken: false
        containers:
        - env:
          - name: WORDPRESS_DB_HOST
            value: wordpress-mariadb
          - name: WORDPRESS_DB_PORT
            value: "3306"
          - name: WORDPRESS_DB_NAME
            value: wordpress
          - name: WORDPRESS_DB_USER
            value: wordpress
          - name: WORDPRESS_DB_PASSWORD
            valueFrom:
              secretKeyRef:
                key: mariadb-password
                name: wordpress
          - name: WORDPRESS_USERNAME
          - name: WORDPRESS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: wordpress-password
                name: wordpress
          - name: WORDPRESS_EMAIL
          - name: WORDPRESS_FIRST_NAME
            value: Admin
          - name: WORDPRESS_LAST_NAME
            value: User
          - name: WORDPRESS_BLOG_NAME
            value: Enterprise WordPress Site
          - name: WORDPRESS_SCHEME
            value: https
          - name: WORDPRESS_TABLE_PREFIX
            value: wp_
          - name: WORDPRESS_HTACCESS_OVERRIDE_NONE
            value: "no"
          - name: WORDPRESS_ENABLE_HTACCESS_PERSISTENCE
            value: "yes"
          - name: WORDPRESS_AUTO_UPDATE_LEVEL
            value: minor
          - name: WORDPRESS_ENABLE_REVERSE_PROXY
            value: "yes"
          - name: WORDPRESS_CONFIG_EXTRA
            value: |
              define('WP_MEMORY_LIMIT', '256M');
              define('WP_MAX_MEMORY_LIMIT', '512M');

              // WordPress configuration
              define('WP_DEBUG', false);
              define('WP_DEBUG_LOG', false);
              define('WP_DEBUG_DISPLAY', false);
              define('AUTOMATIC_UPDATER_DISABLED', false);
              define('WP_AUTO_UPDATE_CORE', 'minor');
              define('DISALLOW_FILE_EDIT', true);
              define('FORCE_SSL_ADMIN', true);
              define('WP_MEMORY_LIMIT', '256M');

              // Disable WP-Cron (handled by Kubernetes CronJob)
              define('DISABLE_WP_CRON', true);

              // Performance optimizations
              define('WP_CACHE', true);
              define('WP_POST_REVISIONS', 3);
              define('AUTOSAVE_INTERVAL', 300);
              define('EMPTY_TRASH_DAYS', 7);

              // Security hardening
              ini_set('session.cookie_httponly', true);
              ini_set('session.cookie_secure', true);
              ini_set('session.use_only_cookies', true);

              // Enable object caching
              define('WP_CACHE_KEY_SALT', 'wordpress_cache_');

              // SMTP Configuration for reliable email delivery
              define('SMTP_FROM_EMAIL', 'dhruv@weown.email');
              define('SMTP_FROM_NAME', 'YOUR_NAME');

              // Enable WordPress mail debug logging
              define('WP_MAIL_DEBUG', false);


              // WordPress security keys (auto-generated)
              define('AUTH_KEY', '');
              define('SECURE_AUTH_KEY', '');
              define('LOGGED_IN_KEY', '');
              define('NONCE_KEY', '');
              define('AUTH_SALT', '');
              define('SECURE_AUTH_SALT', '');
              define('LOGGED_IN_SALT', '');
              define('NONCE_SALT', '');
          - name: WORDPRESS_ENABLE_REDIS
            value: "yes"
          - name: REDIS_HOST
            value: wordpress-redis-master
          - name: REDIS_PORT
            value: "6379"
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis-password
                name: wordpress
          image: docker.io/wordpress:6.8.3-php8.3-apache
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 120
            periodSeconds: 30
            successThreshold: 1
            tcpSocket:
              port: 80
            timeoutSeconds: 10
          name: wordpress
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 10
            initialDelaySeconds: 60
            periodSeconds: 15
            successThreshold: 1
            tcpSocket:
              port: 80
            timeoutSeconds: 10
          resources:
            limits:
              cpu: 800m
              ephemeral-storage: 800Mi
              memory: 1Gi
            requests:
              cpu: 50m
              ephemeral-storage: 200Mi
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: false
            runAsGroup: 0
            runAsNonRoot: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/www/html
            name: wordpress-core
          - mountPath: /var/www/html/wp-content
            name: wordpress-content
          - mountPath: /var/www/html/wp-config-custom
            name: wordpress-config
          - mountPath: /var/www/html/wp-content/mu-plugins
            name: cache-plugin
          - mountPath: /var/cache/wordpress
            name: wordpress-cache
          - mountPath: /tmp
            name: tmp-dir
          - mountPath: /var/log/apache2
            name: apache-logs
          - mountPath: /var/lib/php/sessions
            name: php-sessions
          - mountPath: /usr/local/etc/php/conf.d/uploads.ini
            name: php-config
            subPath: uploads.ini
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 0
          runAsGroup: 0
          runAsNonRoot: false
          runAsUser: 0
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: wordpress
        serviceAccountName: wordpress
        terminationGracePeriodSeconds: 30
        volumes:
        - name: wordpress-core
          persistentVolumeClaim:
            claimName: wordpress-core
        - name: wordpress-content
          persistentVolumeClaim:
            claimName: wordpress-content
        - name: wordpress-config
          persistentVolumeClaim:
            claimName: wordpress-config
        - name: wordpress-cache
          persistentVolumeClaim:
            claimName: wordpress-cache
        - configMap:
            defaultMode: 420
            name: wordpress-cache-plugin
          name: cache-plugin
        - configMap:
            defaultMode: 420
            name: wordpress-php-config
          name: php-config
        - emptyDir:
            sizeLimit: 1Gi
          name: tmp-dir
        - emptyDir:
            sizeLimit: 500Mi
          name: apache-logs
        - emptyDir:
            sizeLimit: 100Mi
          name: php-sessions
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: matomo
      meta.helm.sh/release-namespace: matomo
    creationTimestamp: "2025-12-18T11:18:11Z"
    generation: 2
    labels:
      app.kubernetes.io/component: mariadb
      app.kubernetes.io/instance: matomo
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: matomo
      app.kubernetes.io/version: 5.5.1
      helm.sh/chart: matomo-2.0.6
    name: matomo-mariadb
    namespace: matomo
    resourceVersion: "43781"
    uid: 8d9a3e4a-b1b8-48c8-8dab-3b7741c200a5
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: mariadb
        app.kubernetes.io/instance: matomo
        app.kubernetes.io/name: matomo
    serviceName: matomo-mariadb
    template:
      metadata:
        labels:
          app.kubernetes.io/component: mariadb
          app.kubernetes.io/instance: matomo
          app.kubernetes.io/name: matomo
      spec:
        containers:
        - args:
          - --max-allowed-packet=128M
          - --innodb-buffer-pool-size=256M
          - --innodb-log-file-size=64M
          env:
          - name: MYSQL_ROOT_PASSWORD
            valueFrom:
              secretKeyRef:
                key: mariadb-root-password
                name: matomo-mariadb
          - name: MYSQL_DATABASE
            value: matomo
          - name: MYSQL_USER
            value: matomo
          - name: MYSQL_PASSWORD
            valueFrom:
              secretKeyRef:
                key: mariadb-password
                name: matomo-mariadb
          image: mariadb:12.0.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 3306
            timeoutSeconds: 3
          name: mariadb
          ports:
          - containerPort: 3306
            name: mysql
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 15
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: 3306
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsGroup: 999
            runAsNonRoot: true
            runAsUser: 999
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/mysql
            name: data
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 999
          runAsGroup: 999
          runAsNonRoot: true
          runAsUser: 999
          seccompProfile:
            type: RuntimeDefault
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
        storageClassName: do-block-storage
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: matomo-mariadb-66868f54b8
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updateRevision: matomo-mariadb-66868f54b8
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: wordpress
      meta.helm.sh/release-namespace: wordpress
    creationTimestamp: "2025-12-18T10:57:14Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: wordpress
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: mariadb
    name: wordpress-mariadb
    namespace: wordpress
    resourceVersion: "38368"
    uid: b74eda07-58a9-4e73-bc25-6187e189c7a5
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: wordpress
        app.kubernetes.io/name: mariadb
    serviceName: wordpress-mariadb
    template:
      metadata:
        labels:
          app.kubernetes.io/instance: wordpress
          app.kubernetes.io/name: mariadb
      spec:
        containers:
        - env:
          - name: MYSQL_ROOT_PASSWORD
            valueFrom:
              secretKeyRef:
                key: mariadb-root-password
                name: wordpress-mariadb
          - name: MYSQL_DATABASE
            value: wordpress
          - name: MYSQL_USER
            value: wordpress
          - name: MYSQL_PASSWORD
            valueFrom:
              secretKeyRef:
                key: mariadb-password
                name: wordpress-mariadb
          image: mariadb:12.0.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 3306
            timeoutSeconds: 5
          name: mariadb
          ports:
          - containerPort: 3306
            name: mysql
            protocol: TCP
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: 3306
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 250m
              ephemeral-storage: 1Gi
              memory: 512Mi
            requests:
              cpu: 50m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsGroup: 999
            runAsNonRoot: true
            runAsUser: 999
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/mysql
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 999
          runAsGroup: 999
          runAsNonRoot: true
          runAsUser: 999
          seccompProfile:
            type: RuntimeDefault
        terminationGracePeriodSeconds: 30
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        labels:
          app.kubernetes.io/instance: wordpress
          app.kubernetes.io/name: mariadb
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 5Gi
        storageClassName: do-block-storage
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: wordpress-mariadb-5c5989f9c6
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: wordpress-mariadb-5c5989f9c6
    updatedReplicas: 1
- apiVersion: batch/v1
  kind: CronJob
  metadata:
    annotations:
      meta.helm.sh/release-name: anythingllm
      meta.helm.sh/release-namespace: anything-llm
    creationTimestamp: "2025-12-18T11:03:09Z"
    generation: 1
    labels:
      app.kubernetes.io/component: backup
      app.kubernetes.io/instance: anythingllm
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: anythingllm
      app.kubernetes.io/part-of: weown-mvp
      app.kubernetes.io/version: 1.9.0
      helm.sh/chart: anythingllm-2.0.2
    name: anythingllm-backup
    namespace: anything-llm
    resourceVersion: "19353496"
    uid: 54fc21fa-6e7d-47ea-a247-ad2ca06d185e
  spec:
    concurrencyPolicy: Forbid
    failedJobsHistoryLimit: 1
    jobTemplate:
      metadata: {}
      spec:
        activeDeadlineSeconds: 3600
        backoffLimit: 2
        template:
          metadata:
            labels:
              app.kubernetes.io/component: backup
              app.kubernetes.io/instance: anythingllm
              app.kubernetes.io/name: anythingllm
          spec:
            containers:
            - command:
              - /bin/sh
              - -c
              - "set -e\n\n# Install kubectl\napk add --no-cache curl\ncurl -LO \"https://dl.k8s.io/release/$(curl
                -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nchmod
                +x kubectl\nmv kubectl /usr/local/bin/\n\n# Create backup directory
                with timestamp\nBACKUP_TIMESTAMP=$(date +%Y%m%d_%H%M%S)\nBACKUP_DIR=\"/backup/$BACKUP_TIMESTAMP\"\nmkdir
                -p \"$BACKUP_DIR\"\n\necho \"=== AnythingLLM Auto-Backup Started at
                $(date) ===\"\n\n# Get AnythingLLM pod name\nANYTHINGLLM_POD=$(kubectl
                get pods -n anything-llm -l app.kubernetes.io/name=anythingllm -o
                jsonpath='{.items[0].metadata.name}')\n\nif [ -z \"$ANYTHINGLLM_POD\"
                ]; then\n  echo \"ERROR: AnythingLLM pod not found!\"\n  exit 1\nfi\n\necho
                \"Found AnythingLLM pod: $ANYTHINGLLM_POD\"\n\n# Backup database using
                kubectl exec and cp (zero downtime)\necho \"Backing up database...\"\nkubectl
                exec -n anything-llm \"$ANYTHINGLLM_POD\" -- tar czf /tmp/backup-db.tar.gz
                -C /app/server/storage anythingllm.db anythingllm.db-journal 2>/dev/null
                || true\nkubectl cp anything-llm/\"$ANYTHINGLLM_POD\":/tmp/backup-db.tar.gz
                \"$BACKUP_DIR/anythingllm.db.tar.gz\"\n\n# Backup storage files\necho
                \"Backing up storage files...\"\nkubectl exec -n anything-llm \"$ANYTHINGLLM_POD\"
                -- tar czf /tmp/backup-storage.tar.gz -C /app/server/storage --exclude=\"*.db\"
                --exclude=\"*.db-journal\" . 2>/dev/null || true\nkubectl cp anything-llm/\"$ANYTHINGLLM_POD\":/tmp/backup-storage.tar.gz
                \"$BACKUP_DIR/storage.tar.gz\"\n\n# Create backup metadata\ncat >
                \"$BACKUP_DIR/backup_info.txt\" << EOF\nBackup Timestamp: $BACKUP_TIMESTAMP\nBackup
                Date: $(date)\nPod Name: $ANYTHINGLLM_POD\nNamespace: anything-llm\nHelm
                Release: anythingllm\nChart Version: 2.0.2\nAuto-Backup: Enabled (Daily
                at 2 AM)\nRetention: 30 days\nEOF\n\n# Cleanup temp files from pod\nkubectl
                exec -n anything-llm \"$ANYTHINGLLM_POD\" -- rm -f /tmp/backup-db.tar.gz
                /tmp/backup-storage.tar.gz 2>/dev/null || true\n\n# Get backup size\nBACKUP_SIZE=$(du
                -sh \"$BACKUP_DIR\" | cut -f1)\necho \"Γ£à Auto-backup completed successfully!\"\necho
                \"\U0001F4C1 Backup location: $BACKUP_DIR\"\necho \"\U0001F4CA Backup
                size: $BACKUP_SIZE\"\n\n# Cleanup old backups (configurable retention)\necho
                \"\U0001F9F9 Cleaning up backups older than 30 days...\"\nDELETED_COUNT=$(find
                /backup -type d -name \"[0-9]*_[0-9]*\" -mtime +30 2>/dev/null | wc
                -l)\nfind /backup -type d -name \"[0-9]*_[0-9]*\" -mtime +30 -exec
                rm -rf {} + 2>/dev/null || true\necho \"Deleted $DELETED_COUNT old
                backup(s)\"\n\necho \"=== Backup completed successfully at $(date)
                ===\"\necho \"Backup location: $BACKUP_DIR\"\necho \"Available backups:
                $(ls -1 /backup | wc -l)\"\necho \"Database size: $(stat -c%s \"$BACKUP_DIR/anythingllm.db\")
                bytes\"\n"
              image: alpine:3.18
              imagePullPolicy: IfNotPresent
              name: backup
              resources:
                limits:
                  cpu: 500m
                  memory: 512Mi
                requests:
                  cpu: 100m
                  memory: 128Mi
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              volumeMounts:
              - mountPath: /backup
                name: backup-storage
            dnsPolicy: ClusterFirst
            restartPolicy: OnFailure
            schedulerName: default-scheduler
            securityContext:
              fsGroup: 1000
              runAsGroup: 1000
              runAsNonRoot: true
              runAsUser: 1000
            serviceAccount: anythingllm-backup
            serviceAccountName: anythingllm-backup
            terminationGracePeriodSeconds: 30
            volumes:
            - name: backup-storage
              persistentVolumeClaim:
                claimName: anythingllm-backup
    schedule: 0 2 * * *
    successfulJobsHistoryLimit: 1
    suspend: false
  status:
    lastScheduleTime: "2026-02-18T02:00:00Z"
- apiVersion: batch/v1
  kind: CronJob
  metadata:
    annotations:
      meta.helm.sh/release-name: matomo
      meta.helm.sh/release-namespace: matomo
    creationTimestamp: "2025-12-18T11:18:11Z"
    generation: 2
    labels:
      app.kubernetes.io/component: archive
      app.kubernetes.io/instance: matomo
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: matomo
      app.kubernetes.io/version: 5.5.1
      helm.sh/chart: matomo-2.0.6
    name: matomo-archive
    namespace: matomo
    resourceVersion: "19602469"
    uid: ec9f688e-aa8e-43d5-8a11-76afd7cf9582
  spec:
    concurrencyPolicy: Forbid
    failedJobsHistoryLimit: 1
    jobTemplate:
      metadata: {}
      spec:
        template:
          metadata:
            labels:
              app.kubernetes.io/component: archive
              app.kubernetes.io/instance: matomo
              app.kubernetes.io/name: matomo
          spec:
            automountServiceAccountToken: false
            containers:
            - command:
              - /bin/sh
              - -c
              - |
                echo "[$(date)] Starting Matomo archive processing via HTTP..."

                # Wait for Matomo service to be ready
                echo "Checking Matomo service availability..."
                for i in $(seq 1 30); do
                  if curl -f -s "http://matomo.matomo.svc.cluster.local" > /dev/null 2>&1; then
                    echo "Γ£ô Matomo service is ready"
                    break
                  fi
                  echo "Waiting for Matomo service... attempt $i/30"
                  sleep 10
                done

                # Trigger archive processing via HTTP (no volume mount needed)
                echo "Triggering archive via HTTP endpoint..."
                ARCHIVE_URL="http://matomo.matomo.svc.cluster.local/misc/cron/archive.php?token=${ARCHIVE_TOKEN}"

                if curl -f -s "$ARCHIVE_URL" -o /tmp/archive_output.txt; then
                  echo "Γ£ô Archive processing completed successfully"
                  cat /tmp/archive_output.txt
                  echo ""
                  echo "[$(date)] Archive processing completed"
                  exit 0
                else
                  echo "ΓÜá∩╕Å Archive processing failed"
                  cat /tmp/archive_output.txt 2>/dev/null || echo "No output"
                  exit 1
                fi
              env:
              - name: MATOMO_DATABASE_HOST
                value: matomo-mariadb
              - name: MATOMO_DATABASE_USERNAME
                value: matomo
              - name: MATOMO_DATABASE_PASSWORD
                valueFrom:
                  secretKeyRef:
                    key: mariadb-password
                    name: matomo-mariadb
              - name: MATOMO_DATABASE_DBNAME
                value: matomo
              - name: ARCHIVE_TOKEN
                valueFrom:
                  secretKeyRef:
                    key: archive-token
                    name: matomo
              image: docker.io/matomo:5.5.1-apache
              imagePullPolicy: IfNotPresent
              name: archive
              resources:
                limits:
                  cpu: 200m
                  memory: 256Mi
                requests:
                  cpu: 50m
                  memory: 128Mi
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              volumeMounts:
              - mountPath: /tmp
                name: tmp
            dnsPolicy: ClusterFirst
            restartPolicy: OnFailure
            schedulerName: default-scheduler
            securityContext:
              fsGroup: 33
              runAsGroup: 33
              runAsNonRoot: true
              runAsUser: 33
              seccompProfile:
                type: RuntimeDefault
            serviceAccount: matomo
            serviceAccountName: matomo
            terminationGracePeriodSeconds: 30
            volumes:
            - emptyDir: {}
              name: tmp
    schedule: 5 * * * *
    successfulJobsHistoryLimit: 1
    suspend: false
  status:
    lastScheduleTime: "2026-02-18T21:05:00Z"
    lastSuccessfulTime: "2026-02-18T21:05:04Z"
- apiVersion: batch/v1
  kind: CronJob
  metadata:
    annotations:
      meta.helm.sh/release-name: matomo
      meta.helm.sh/release-namespace: matomo
    creationTimestamp: "2025-12-18T11:18:11Z"
    generation: 1
    labels:
      app.kubernetes.io/component: backup
      app.kubernetes.io/instance: matomo
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: matomo
      app.kubernetes.io/version: 5.5.1
      helm.sh/chart: matomo-2.0.6
    name: matomo-backup
    namespace: matomo
    resourceVersion: "19366623"
    uid: 00c6259b-7e3c-4933-820d-ea266b6ac8b7
  spec:
    concurrencyPolicy: Forbid
    failedJobsHistoryLimit: 1
    jobTemplate:
      metadata: {}
      spec:
        activeDeadlineSeconds: 3600
        backoffLimit: 2
        template:
          metadata:
            labels:
              app.kubernetes.io/component: backup
              app.kubernetes.io/instance: matomo
              app.kubernetes.io/name: matomo
          spec:
            containers:
            - command:
              - /bin/bash
              - -c
              - |
                set -euo pipefail

                echo "========================================="
                echo "Matomo Analytics Backup Started at $(date)"
                echo "========================================="

                # Backup configuration
                TIMESTAMP=$(date +%Y%m%d_%H%M%S)
                BACKUP_DIR="/backup/${TIMESTAMP}"
                BACKUP_PREFIX="matomo"
                RETENTION_DAYS=30

                # Database connection parameters
                MARIADB_HOST="matomo-mariadb"
                MARIADB_PORT="3306"
                MARIADB_DATABASE="matomo"
                MARIADB_USER="$MARIADB_USERNAME"

                # Create backup directory
                mkdir -p "$BACKUP_DIR"

                echo "Backing up Matomo analytics database..."
                echo "Database: $MARIADB_DATABASE"
                echo "Host: $MARIADB_HOST:$MARIADB_PORT"

                # Perform database backup with compression
                # Options explained:
                # --single-transaction: Consistent backup without locking tables
                # --routines: Include stored procedures
                # --triggers: Include triggers
                # --events: Include scheduled events
                # --lock-tables=false: Don't lock tables (InnoDB)
                # --add-drop-table: Add DROP TABLE before CREATE TABLE
                # --extended-insert: Use multi-row INSERT syntax (faster restore)
                # --quick: Retrieve rows one at a time (memory efficient)
                # --compress: Compress data between server and client

                echo "Starting database dump..."
                timeout 1800 mariadb-dump \
                  --host="$MARIADB_HOST" \
                  --port="$MARIADB_PORT" \
                  --user="$MARIADB_USER" \
                  --password="$MARIADB_PASSWORD" \
                  --single-transaction \
                  --routines \
                  --triggers \
                  --events \
                  --lock-tables=false \
                  --add-drop-table \
                  --extended-insert \
                  --quick \
                  --compress \
                  "$MARIADB_DATABASE" | gzip -9 > "$BACKUP_DIR/${BACKUP_PREFIX}_${TIMESTAMP}.sql.gz"

                # Verify backup file was created
                if [ ! -f "$BACKUP_DIR/${BACKUP_PREFIX}_${TIMESTAMP}.sql.gz" ]; then
                  echo "ERROR: Backup file was not created!"
                  exit 1
                fi

                # Get backup file size
                BACKUP_SIZE=$(stat -c%s "$BACKUP_DIR/${BACKUP_PREFIX}_${TIMESTAMP}.sql.gz")
                BACKUP_SIZE_MB=$(awk "BEGIN {printf \"%.2f\", $BACKUP_SIZE / 1024 / 1024}")

                echo "Γ£ô Database backup completed"
                echo "  File: ${BACKUP_PREFIX}_${TIMESTAMP}.sql.gz"
                echo "  Size: ${BACKUP_SIZE_MB} MB (compressed)"

                # Test backup integrity
                echo "Verifying backup integrity..."
                if gunzip -t "$BACKUP_DIR/${BACKUP_PREFIX}_${TIMESTAMP}.sql.gz" 2>/dev/null; then
                  echo "Γ£ô Backup file integrity verified"
                else
                  echo "ERROR: Backup file is corrupted!"
                  exit 1
                fi

                # Create backup manifest
                echo "Creating backup manifest..."
                cat > "$BACKUP_DIR/backup_manifest.txt" << EOF
                ========================================
                Matomo Analytics Backup Manifest
                ========================================
                Backup Date: $(date -Iseconds)
                Backup Timestamp: ${TIMESTAMP}
                Kubernetes Namespace: matomo
                Helm Release: matomo
                Chart Version: 2.0.6
                Matomo Version: 5.5.1-apache

                Database Information:
                ====================
                Host: $MARIADB_HOST
                Port: $MARIADB_PORT
                Database: $MARIADB_DATABASE
                User: $MARIADB_USER

                Backup Files:
                =============
                ${BACKUP_PREFIX}_${TIMESTAMP}.sql.gz (${BACKUP_SIZE_MB} MB compressed)

                Backup Details:
                ===============
                Compression: gzip level 9
                Backup Type: Full database dump
                Transaction Consistent: Yes
                Includes: Tables, Routines, Triggers, Events

                Retention Policy: ${RETENTION_DAYS} days

                Restoration Command:
                ====================
                gunzip < ${BACKUP_PREFIX}_${TIMESTAMP}.sql.gz | \\
                  mysql -h \$MARIADB_HOST -u \$MARIADB_USER -p \$MARIADB_DATABASE

                Notes:
                ======
                - This backup contains all Matomo analytics data
                - Includes visitor data, reports, configurations, and user accounts
                - Test restoration regularly to ensure backup integrity
                - Store backups securely with appropriate access controls
                EOF

                # Calculate statistics
                echo "Calculating backup statistics..."
                UNCOMPRESSED_SIZE=$(gunzip -c "$BACKUP_DIR/${BACKUP_PREFIX}_${TIMESTAMP}.sql.gz" | wc -c)
                UNCOMPRESSED_SIZE_MB=$(awk "BEGIN {printf \"%.2f\", $UNCOMPRESSED_SIZE / 1024 / 1024}")
                COMPRESSION_RATIO=$(awk "BEGIN {printf \"%.1f\", ($UNCOMPRESSED_SIZE - $BACKUP_SIZE) * 100 / $UNCOMPRESSED_SIZE}")

                echo "" >> "$BACKUP_DIR/backup_manifest.txt"
                echo "Compression Statistics:" >> "$BACKUP_DIR/backup_manifest.txt"
                echo "=====================" >> "$BACKUP_DIR/backup_manifest.txt"
                echo "Uncompressed: ${UNCOMPRESSED_SIZE_MB} MB" >> "$BACKUP_DIR/backup_manifest.txt"
                echo "Compressed: ${BACKUP_SIZE_MB} MB" >> "$BACKUP_DIR/backup_manifest.txt"
                echo "Compression Ratio: ${COMPRESSION_RATIO}%" >> "$BACKUP_DIR/backup_manifest.txt"

                # Cleanup old backups
                echo ""
                echo "Cleaning up backups older than ${RETENTION_DAYS} days..."
                BEFORE_COUNT=$(find /backup -maxdepth 1 -type d -name "[0-9]*" | wc -l)
                find /backup -maxdepth 1 -type d -name "[0-9]*" -mtime +${RETENTION_DAYS} -exec rm -rf {} \; 2>/dev/null || true
                AFTER_COUNT=$(find /backup -maxdepth 1 -type d -name "[0-9]*" | wc -l)
                DELETED=$((BEFORE_COUNT - AFTER_COUNT))

                if [ $DELETED -gt 0 ]; then
                  echo "Γ£ô Deleted $DELETED old backup(s)"
                fi

                # Final status
                echo ""
                echo "========================================="
                echo "Γ£à Matomo Backup Completed Successfully!"
                echo "========================================="
                echo "Backup location: $BACKUP_DIR"
                echo "Compressed size: ${BACKUP_SIZE_MB} MB"
                echo "Uncompressed size: ${UNCOMPRESSED_SIZE_MB} MB"
                echo "Space saved: ${COMPRESSION_RATIO}%"
                echo "Total backups retained: $AFTER_COUNT"
                echo "Completed at: $(date)"
                echo "========================================="

                # Log to backup log file
                echo "[$(date -Iseconds)] Backup completed: $BACKUP_DIR (${BACKUP_SIZE_MB}MB compressed, ${UNCOMPRESSED_SIZE_MB}MB uncompressed)" >> /backup/backup.log
              env:
              - name: MARIADB_USERNAME
                valueFrom:
                  secretKeyRef:
                    key: mariadb-username
                    name: matomo-mariadb
              - name: MARIADB_PASSWORD
                valueFrom:
                  secretKeyRef:
                    key: mariadb-password
                    name: matomo-mariadb
              image: mariadb:11.2
              imagePullPolicy: IfNotPresent
              name: matomo-backup
              resources:
                limits:
                  cpu: 200m
                  memory: 256Mi
                requests:
                  cpu: 50m
                  memory: 128Mi
              securityContext:
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                  - ALL
                readOnlyRootFilesystem: true
                runAsGroup: 1001
                runAsNonRoot: true
                runAsUser: 1001
                seccompProfile:
                  type: RuntimeDefault
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              volumeMounts:
              - mountPath: /backup
                name: backup-storage
              - mountPath: /tmp
                name: tmp-dir
            dnsPolicy: ClusterFirst
            restartPolicy: OnFailure
            schedulerName: default-scheduler
            securityContext:
              fsGroup: 1001
              runAsGroup: 1001
              runAsNonRoot: true
              runAsUser: 1001
              seccompProfile:
                type: RuntimeDefault
            serviceAccount: matomo-backup
            serviceAccountName: matomo-backup
            terminationGracePeriodSeconds: 30
            volumes:
            - name: backup-storage
              persistentVolumeClaim:
                claimName: matomo-backup
            - emptyDir:
                sizeLimit: 1Gi
              name: tmp-dir
    schedule: 0 3 * * *
    successfulJobsHistoryLimit: 1
    suspend: false
  status:
    lastScheduleTime: "2026-02-18T03:00:00Z"
- apiVersion: batch/v1
  kind: CronJob
  metadata:
    annotations:
      meta.helm.sh/release-name: n8n
      meta.helm.sh/release-namespace: n8n
    creationTimestamp: "2026-01-01T22:28:52Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: n8n
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: n8n
      app.kubernetes.io/part-of: n8n
      app.kubernetes.io/version: 2.1.4
      component: backup
      helm.sh/chart: n8n-2.8.1
      security.weown.xyz/compliance: SOC2-ISO42001
      security.weown.xyz/network-policy: zero-trust
    name: n8n-backup
    namespace: n8n
    resourceVersion: "19353394"
    uid: 65eefee7-8e59-4b07-9f08-f32ac3706015
  spec:
    concurrencyPolicy: Allow
    failedJobsHistoryLimit: 1
    jobTemplate:
      metadata: {}
      spec:
        template:
          metadata:
            labels:
              app.kubernetes.io/instance: n8n
              app.kubernetes.io/name: n8n
              component: backup
          spec:
            affinity:
              podAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchExpressions:
                    - key: app.kubernetes.io/name
                      operator: In
                      values:
                      - n8n
                    - key: component
                      operator: In
                      values:
                      - main
                  topologyKey: kubernetes.io/hostname
            containers:
            - command:
              - /bin/sh
              - -c
              - |
                echo "Starting n8n backup at $(date)"

                # Create backup directory
                mkdir -p /backup/$(date +%Y%m%d_%H%M%S)

                # Copy n8n data
                if [ -d "/data" ]; then
                  cp -r /data/* /backup/$(date +%Y%m%d_%H%M%S)/ || true
                  echo "Backup completed successfully at $(date)"
                else
                  echo "No data directory found"
                fi

                # Cleanup old backups (keep last 7 days)
                find /backup -type d -name "20*" -mtime +7 -exec rm -rf {} + || true

                echo "Backup job finished at $(date)"
              image: busybox:1.36
              imagePullPolicy: IfNotPresent
              name: backup
              resources:
                limits:
                  cpu: 100m
                  memory: 128Mi
                requests:
                  cpu: 10m
                  memory: 32Mi
              securityContext:
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                  - ALL
                readOnlyRootFilesystem: true
                runAsNonRoot: true
                runAsUser: 1000
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              volumeMounts:
              - mountPath: /data
                name: n8n-data
              - mountPath: /backup
                name: backup-storage
            dnsPolicy: ClusterFirst
            restartPolicy: OnFailure
            schedulerName: default-scheduler
            securityContext:
              fsGroup: 1000
              runAsGroup: 1000
              runAsNonRoot: true
              runAsUser: 1000
              seccompProfile:
                type: RuntimeDefault
            terminationGracePeriodSeconds: 30
            volumes:
            - name: n8n-data
              persistentVolumeClaim:
                claimName: n8n-data
            - name: backup-storage
              persistentVolumeClaim:
                claimName: n8n-backup
    schedule: 0 2 * * *
    successfulJobsHistoryLimit: 3
    suspend: false
  status:
    lastScheduleTime: "2026-02-18T02:00:00Z"
    lastSuccessfulTime: "2026-02-18T02:00:13Z"
- apiVersion: batch/v1
  kind: CronJob
  metadata:
    annotations:
      meta.helm.sh/release-name: wordpress
      meta.helm.sh/release-namespace: wordpress
    creationTimestamp: "2025-12-18T10:57:15Z"
    generation: 1
    labels:
      app.kubernetes.io/component: backup
      app.kubernetes.io/instance: wordpress
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: wordpress
      app.kubernetes.io/part-of: wordpress-platform
      app.kubernetes.io/version: 6.8.3
      helm.sh/chart: wordpress-3.2.6
    name: wordpress-backup
    namespace: wordpress
    resourceVersion: "19353531"
    uid: 663a4908-4b44-4f9b-a2cd-e5c90505cc3d
  spec:
    concurrencyPolicy: Forbid
    failedJobsHistoryLimit: 1
    jobTemplate:
      metadata: {}
      spec:
        activeDeadlineSeconds: 3600
        backoffLimit: 2
        template:
          metadata:
            labels:
              app.kubernetes.io/component: backup
              app.kubernetes.io/instance: wordpress
              app.kubernetes.io/name: wordpress
          spec:
            containers:
            - command:
              - /bin/bash
              - -c
              - |
                set -euo pipefail

                # Source backup configuration
                source /etc/backup/backup-config.sh

                echo "Starting WordPress backup at $(date)"

                # Create backup directory
                mkdir -p "$BACKUP_DIR"

                # Database backup with compression and timeout
                echo "Backing up database..."
                timeout 600 mariadb-dump \
                  --host="$MARIADB_HOST" \
                  --user="$MARIADB_USER" \
                  --password="$MARIADB_PASSWORD" \
                  --single-transaction \
                  --routines \
                  --triggers \
                  --lock-tables=false \
                  --add-drop-table \
                  --extended-insert \
                  --quick \
                  --compress \
                  "$MARIADB_DB" | gzip > "$BACKUP_DIR/${BACKUP_PREFIX}_db_${TIMESTAMP}.sql.gz"

                # WordPress files backup - Database only for now to avoid volume conflicts
                echo "Backing up WordPress database only (files on persistent volume)"

                # Create backup manifest
                echo "Creating backup manifest..."
                cat > "$BACKUP_DIR/${BACKUP_PREFIX}_manifest_${TIMESTAMP}.txt" << EOF
                WordPress Backup Manifest
                Backup Date: $(date -Iseconds)
                Kubernetes Namespace: wordpress
                WordPress Version: 6.8.3-php8.3-apache
                Database Host: $MARIADB_HOST
                Database Name: $MARIADB_DB

                Files:
                - ${BACKUP_PREFIX}_db_${TIMESTAMP}.sql.gz ($(stat -f%z "$BACKUP_DIR/${BACKUP_PREFIX}_db_${TIMESTAMP}.sql.gz" 2>/dev/null || stat -c%s "$BACKUP_DIR/${BACKUP_PREFIX}_db_${TIMESTAMP}.sql.gz") bytes)

                Note: WordPress files are stored on persistent volumes and automatically backed up by DigitalOcean.
                EOF

                # Cleanup old backups
                echo "Cleaning up old backups..."
                find "$BACKUP_DIR" -name "${BACKUP_PREFIX}_*" -mtime +$RETENTION_DAYS -delete

                # Log completion
                echo "Backup completed successfully at $(date)" | tee -a "$LOG_FILE"
                echo "Backup files created:"
                ls -lah "$BACKUP_DIR/${BACKUP_PREFIX}_"*"_${TIMESTAMP}."*
              env:
              - name: MARIADB_HOST
                value: wordpress-mariadb
              - name: MARIADB_USER
                value: wordpress
              - name: MARIADB_PASSWORD
                valueFrom:
                  secretKeyRef:
                    key: mariadb-password
                    name: wordpress
              - name: MARIADB_DB
                value: wordpress
              image: mariadb:11.1
              imagePullPolicy: IfNotPresent
              name: wordpress-backup
              resources:
                limits:
                  cpu: 200m
                  memory: 256Mi
                requests:
                  cpu: 50m
                  memory: 128Mi
              securityContext:
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                  - ALL
                readOnlyRootFilesystem: true
                runAsGroup: 1000
                runAsNonRoot: true
                runAsUser: 1000
                seccompProfile:
                  type: RuntimeDefault
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              volumeMounts:
              - mountPath: /var/backups/wordpress
                name: backup-storage
              - mountPath: /etc/backup
                name: backup-config
              - mountPath: /tmp
                name: tmp-dir
            dnsPolicy: ClusterFirst
            restartPolicy: OnFailure
            schedulerName: default-scheduler
            securityContext:
              fsGroup: 1000
              runAsGroup: 1000
              runAsNonRoot: true
              runAsUser: 1000
              seccompProfile:
                type: RuntimeDefault
            serviceAccount: wordpress
            serviceAccountName: wordpress
            terminationGracePeriodSeconds: 30
            volumes:
            - name: backup-storage
              persistentVolumeClaim:
                claimName: wordpress-backup
            - configMap:
                defaultMode: 420
                items:
                - key: backup-config.sh
                  mode: 493
                  path: backup-config.sh
                name: wordpress-config
              name: backup-config
            - emptyDir:
                sizeLimit: 1Gi
              name: tmp-dir
    schedule: 0 2 * * *
    successfulJobsHistoryLimit: 1
    suspend: false
  status:
    lastScheduleTime: "2026-02-18T02:00:00Z"
- apiVersion: batch/v1
  kind: CronJob
  metadata:
    annotations:
      meta.helm.sh/release-name: wordpress
      meta.helm.sh/release-namespace: wordpress
    creationTimestamp: "2025-12-18T10:57:15Z"
    generation: 1
    labels:
      app.kubernetes.io/component: cron
      app.kubernetes.io/instance: wordpress
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: wordpress
      app.kubernetes.io/part-of: wordpress-platform
      app.kubernetes.io/version: 6.8.3
      helm.sh/chart: wordpress-3.2.6
    name: wordpress-cron
    namespace: wordpress
    resourceVersion: "19611161"
    uid: 90f565a3-7554-4244-a78f-ff53ef3ae0e3
  spec:
    concurrencyPolicy: Forbid
    failedJobsHistoryLimit: 1
    jobTemplate:
      metadata: {}
      spec:
        template:
          metadata:
            labels:
              app.kubernetes.io/component: cron
              app.kubernetes.io/instance: wordpress
              app.kubernetes.io/managed-by: Helm
              app.kubernetes.io/name: wordpress
              app.kubernetes.io/part-of: wordpress-platform
              app.kubernetes.io/version: 6.8.3
              helm.sh/chart: wordpress-3.2.6
          spec:
            containers:
            - command:
              - /bin/sh
              - -c
              - |
                echo "Running WordPress cron at $(date)"
                curl -f -s -m 30 "http://wordpress/wp-cron.php?doing_wp_cron" || echo "Cron request failed"
                echo "WordPress cron completed at $(date)"
              image: curlimages/curl:latest
              imagePullPolicy: Always
              name: wordpress-cron
              resources:
                limits:
                  cpu: 50m
                  memory: 32Mi
                requests:
                  cpu: 10m
                  memory: 16Mi
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
            dnsPolicy: ClusterFirst
            restartPolicy: OnFailure
            schedulerName: default-scheduler
            securityContext:
              fsGroup: 1000
              runAsGroup: 1000
              runAsNonRoot: true
              runAsUser: 1000
            terminationGracePeriodSeconds: 30
    schedule: '*/15 * * * *'
    successfulJobsHistoryLimit: 1
    suspend: false
  status:
    lastScheduleTime: "2026-02-18T21:45:00Z"
    lastSuccessfulTime: "2026-02-18T21:45:06Z"
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2026-02-18T02:00:00Z"
    creationTimestamp: "2026-02-18T02:00:00Z"
    generation: 1
    labels:
      app.kubernetes.io/component: backup
      app.kubernetes.io/instance: anythingllm
      app.kubernetes.io/name: anythingllm
      batch.kubernetes.io/controller-uid: e18c8b50-711d-4874-ab8e-e28bba14bc8b
      batch.kubernetes.io/job-name: anythingllm-backup-29523000
      controller-uid: e18c8b50-711d-4874-ab8e-e28bba14bc8b
      job-name: anythingllm-backup-29523000
    name: anythingllm-backup-29523000
    namespace: anything-llm
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: anythingllm-backup
      uid: 54fc21fa-6e7d-47ea-a247-ad2ca06d185e
    resourceVersion: "19353492"
    uid: e18c8b50-711d-4874-ab8e-e28bba14bc8b
  spec:
    activeDeadlineSeconds: 3600
    backoffLimit: 2
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: e18c8b50-711d-4874-ab8e-e28bba14bc8b
    suspend: false
    template:
      metadata:
        labels:
          app.kubernetes.io/component: backup
          app.kubernetes.io/instance: anythingllm
          app.kubernetes.io/name: anythingllm
          batch.kubernetes.io/controller-uid: e18c8b50-711d-4874-ab8e-e28bba14bc8b
          batch.kubernetes.io/job-name: anythingllm-backup-29523000
          controller-uid: e18c8b50-711d-4874-ab8e-e28bba14bc8b
          job-name: anythingllm-backup-29523000
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - "set -e\n\n# Install kubectl\napk add --no-cache curl\ncurl -LO \"https://dl.k8s.io/release/$(curl
            -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nchmod
            +x kubectl\nmv kubectl /usr/local/bin/\n\n# Create backup directory with
            timestamp\nBACKUP_TIMESTAMP=$(date +%Y%m%d_%H%M%S)\nBACKUP_DIR=\"/backup/$BACKUP_TIMESTAMP\"\nmkdir
            -p \"$BACKUP_DIR\"\n\necho \"=== AnythingLLM Auto-Backup Started at $(date)
            ===\"\n\n# Get AnythingLLM pod name\nANYTHINGLLM_POD=$(kubectl get pods
            -n anything-llm -l app.kubernetes.io/name=anythingllm -o jsonpath='{.items[0].metadata.name}')\n\nif
            [ -z \"$ANYTHINGLLM_POD\" ]; then\n  echo \"ERROR: AnythingLLM pod not
            found!\"\n  exit 1\nfi\n\necho \"Found AnythingLLM pod: $ANYTHINGLLM_POD\"\n\n#
            Backup database using kubectl exec and cp (zero downtime)\necho \"Backing
            up database...\"\nkubectl exec -n anything-llm \"$ANYTHINGLLM_POD\" --
            tar czf /tmp/backup-db.tar.gz -C /app/server/storage anythingllm.db anythingllm.db-journal
            2>/dev/null || true\nkubectl cp anything-llm/\"$ANYTHINGLLM_POD\":/tmp/backup-db.tar.gz
            \"$BACKUP_DIR/anythingllm.db.tar.gz\"\n\n# Backup storage files\necho
            \"Backing up storage files...\"\nkubectl exec -n anything-llm \"$ANYTHINGLLM_POD\"
            -- tar czf /tmp/backup-storage.tar.gz -C /app/server/storage --exclude=\"*.db\"
            --exclude=\"*.db-journal\" . 2>/dev/null || true\nkubectl cp anything-llm/\"$ANYTHINGLLM_POD\":/tmp/backup-storage.tar.gz
            \"$BACKUP_DIR/storage.tar.gz\"\n\n# Create backup metadata\ncat > \"$BACKUP_DIR/backup_info.txt\"
            << EOF\nBackup Timestamp: $BACKUP_TIMESTAMP\nBackup Date: $(date)\nPod
            Name: $ANYTHINGLLM_POD\nNamespace: anything-llm\nHelm Release: anythingllm\nChart
            Version: 2.0.2\nAuto-Backup: Enabled (Daily at 2 AM)\nRetention: 30 days\nEOF\n\n#
            Cleanup temp files from pod\nkubectl exec -n anything-llm \"$ANYTHINGLLM_POD\"
            -- rm -f /tmp/backup-db.tar.gz /tmp/backup-storage.tar.gz 2>/dev/null
            || true\n\n# Get backup size\nBACKUP_SIZE=$(du -sh \"$BACKUP_DIR\" | cut
            -f1)\necho \"Γ£à Auto-backup completed successfully!\"\necho \"\U0001F4C1
            Backup location: $BACKUP_DIR\"\necho \"\U0001F4CA Backup size: $BACKUP_SIZE\"\n\n#
            Cleanup old backups (configurable retention)\necho \"\U0001F9F9 Cleaning
            up backups older than 30 days...\"\nDELETED_COUNT=$(find /backup -type
            d -name \"[0-9]*_[0-9]*\" -mtime +30 2>/dev/null | wc -l)\nfind /backup
            -type d -name \"[0-9]*_[0-9]*\" -mtime +30 -exec rm -rf {} + 2>/dev/null
            || true\necho \"Deleted $DELETED_COUNT old backup(s)\"\n\necho \"=== Backup
            completed successfully at $(date) ===\"\necho \"Backup location: $BACKUP_DIR\"\necho
            \"Available backups: $(ls -1 /backup | wc -l)\"\necho \"Database size:
            $(stat -c%s \"$BACKUP_DIR/anythingllm.db\") bytes\"\n"
          image: alpine:3.18
          imagePullPolicy: IfNotPresent
          name: backup
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /backup
            name: backup-storage
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: anythingllm-backup
        serviceAccountName: anythingllm-backup
        terminationGracePeriodSeconds: 30
        volumes:
        - name: backup-storage
          persistentVolumeClaim:
            claimName: anythingllm-backup
  status:
    conditions:
    - lastProbeTime: "2026-02-18T02:00:28Z"
      lastTransitionTime: "2026-02-18T02:00:28Z"
      message: Job has reached the specified backoff limit
      reason: BackoffLimitExceeded
      status: "True"
      type: FailureTarget
    - lastProbeTime: "2026-02-18T02:00:29Z"
      lastTransitionTime: "2026-02-18T02:00:29Z"
      message: Job has reached the specified backoff limit
      reason: BackoffLimitExceeded
      status: "True"
      type: Failed
    failed: 1
    ready: 0
    startTime: "2026-02-18T02:00:00Z"
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2026-02-18T21:05:00Z"
    creationTimestamp: "2026-02-18T21:05:00Z"
    generation: 1
    labels:
      app.kubernetes.io/component: archive
      app.kubernetes.io/instance: matomo
      app.kubernetes.io/name: matomo
      batch.kubernetes.io/controller-uid: 4a5c42b2-a0a7-4228-a25f-85bb4069951b
      batch.kubernetes.io/job-name: matomo-archive-29524145
      controller-uid: 4a5c42b2-a0a7-4228-a25f-85bb4069951b
      job-name: matomo-archive-29524145
    name: matomo-archive-29524145
    namespace: matomo
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: matomo-archive
      uid: ec9f688e-aa8e-43d5-8a11-76afd7cf9582
    resourceVersion: "19602465"
    uid: 4a5c42b2-a0a7-4228-a25f-85bb4069951b
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 4a5c42b2-a0a7-4228-a25f-85bb4069951b
    suspend: false
    template:
      metadata:
        labels:
          app.kubernetes.io/component: archive
          app.kubernetes.io/instance: matomo
          app.kubernetes.io/name: matomo
          batch.kubernetes.io/controller-uid: 4a5c42b2-a0a7-4228-a25f-85bb4069951b
          batch.kubernetes.io/job-name: matomo-archive-29524145
          controller-uid: 4a5c42b2-a0a7-4228-a25f-85bb4069951b
          job-name: matomo-archive-29524145
      spec:
        automountServiceAccountToken: false
        containers:
        - command:
          - /bin/sh
          - -c
          - |
            echo "[$(date)] Starting Matomo archive processing via HTTP..."

            # Wait for Matomo service to be ready
            echo "Checking Matomo service availability..."
            for i in $(seq 1 30); do
              if curl -f -s "http://matomo.matomo.svc.cluster.local" > /dev/null 2>&1; then
                echo "Γ£ô Matomo service is ready"
                break
              fi
              echo "Waiting for Matomo service... attempt $i/30"
              sleep 10
            done

            # Trigger archive processing via HTTP (no volume mount needed)
            echo "Triggering archive via HTTP endpoint..."
            ARCHIVE_URL="http://matomo.matomo.svc.cluster.local/misc/cron/archive.php?token=${ARCHIVE_TOKEN}"

            if curl -f -s "$ARCHIVE_URL" -o /tmp/archive_output.txt; then
              echo "Γ£ô Archive processing completed successfully"
              cat /tmp/archive_output.txt
              echo ""
              echo "[$(date)] Archive processing completed"
              exit 0
            else
              echo "ΓÜá∩╕Å Archive processing failed"
              cat /tmp/archive_output.txt 2>/dev/null || echo "No output"
              exit 1
            fi
          env:
          - name: MATOMO_DATABASE_HOST
            value: matomo-mariadb
          - name: MATOMO_DATABASE_USERNAME
            value: matomo
          - name: MATOMO_DATABASE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: mariadb-password
                name: matomo-mariadb
          - name: MATOMO_DATABASE_DBNAME
            value: matomo
          - name: ARCHIVE_TOKEN
            valueFrom:
              secretKeyRef:
                key: archive-token
                name: matomo
          image: docker.io/matomo:5.5.1-apache
          imagePullPolicy: IfNotPresent
          name: archive
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 33
          runAsGroup: 33
          runAsNonRoot: true
          runAsUser: 33
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: matomo
        serviceAccountName: matomo
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    completionTime: "2026-02-18T21:05:04Z"
    conditions:
    - lastProbeTime: "2026-02-18T21:05:04Z"
      lastTransitionTime: "2026-02-18T21:05:04Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2026-02-18T21:05:04Z"
      lastTransitionTime: "2026-02-18T21:05:04Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2026-02-18T21:05:00Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2026-02-18T03:00:00Z"
    creationTimestamp: "2026-02-18T03:00:00Z"
    generation: 1
    labels:
      app.kubernetes.io/component: backup
      app.kubernetes.io/instance: matomo
      app.kubernetes.io/name: matomo
      batch.kubernetes.io/controller-uid: 23a8c378-20aa-41e6-99d8-8113c23bb8c3
      batch.kubernetes.io/job-name: matomo-backup-29523060
      controller-uid: 23a8c378-20aa-41e6-99d8-8113c23bb8c3
      job-name: matomo-backup-29523060
    name: matomo-backup-29523060
    namespace: matomo
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: matomo-backup
      uid: 00c6259b-7e3c-4933-820d-ea266b6ac8b7
    resourceVersion: "19366619"
    uid: 23a8c378-20aa-41e6-99d8-8113c23bb8c3
  spec:
    activeDeadlineSeconds: 3600
    backoffLimit: 2
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 23a8c378-20aa-41e6-99d8-8113c23bb8c3
    suspend: false
    template:
      metadata:
        labels:
          app.kubernetes.io/component: backup
          app.kubernetes.io/instance: matomo
          app.kubernetes.io/name: matomo
          batch.kubernetes.io/controller-uid: 23a8c378-20aa-41e6-99d8-8113c23bb8c3
          batch.kubernetes.io/job-name: matomo-backup-29523060
          controller-uid: 23a8c378-20aa-41e6-99d8-8113c23bb8c3
          job-name: matomo-backup-29523060
      spec:
        containers:
        - command:
          - /bin/bash
          - -c
          - |
            set -euo pipefail

            echo "========================================="
            echo "Matomo Analytics Backup Started at $(date)"
            echo "========================================="

            # Backup configuration
            TIMESTAMP=$(date +%Y%m%d_%H%M%S)
            BACKUP_DIR="/backup/${TIMESTAMP}"
            BACKUP_PREFIX="matomo"
            RETENTION_DAYS=30

            # Database connection parameters
            MARIADB_HOST="matomo-mariadb"
            MARIADB_PORT="3306"
            MARIADB_DATABASE="matomo"
            MARIADB_USER="$MARIADB_USERNAME"

            # Create backup directory
            mkdir -p "$BACKUP_DIR"

            echo "Backing up Matomo analytics database..."
            echo "Database: $MARIADB_DATABASE"
            echo "Host: $MARIADB_HOST:$MARIADB_PORT"

            # Perform database backup with compression
            # Options explained:
            # --single-transaction: Consistent backup without locking tables
            # --routines: Include stored procedures
            # --triggers: Include triggers
            # --events: Include scheduled events
            # --lock-tables=false: Don't lock tables (InnoDB)
            # --add-drop-table: Add DROP TABLE before CREATE TABLE
            # --extended-insert: Use multi-row INSERT syntax (faster restore)
            # --quick: Retrieve rows one at a time (memory efficient)
            # --compress: Compress data between server and client

            echo "Starting database dump..."
            timeout 1800 mariadb-dump \
              --host="$MARIADB_HOST" \
              --port="$MARIADB_PORT" \
              --user="$MARIADB_USER" \
              --password="$MARIADB_PASSWORD" \
              --single-transaction \
              --routines \
              --triggers \
              --events \
              --lock-tables=false \
              --add-drop-table \
              --extended-insert \
              --quick \
              --compress \
              "$MARIADB_DATABASE" | gzip -9 > "$BACKUP_DIR/${BACKUP_PREFIX}_${TIMESTAMP}.sql.gz"

            # Verify backup file was created
            if [ ! -f "$BACKUP_DIR/${BACKUP_PREFIX}_${TIMESTAMP}.sql.gz" ]; then
              echo "ERROR: Backup file was not created!"
              exit 1
            fi

            # Get backup file size
            BACKUP_SIZE=$(stat -c%s "$BACKUP_DIR/${BACKUP_PREFIX}_${TIMESTAMP}.sql.gz")
            BACKUP_SIZE_MB=$(awk "BEGIN {printf \"%.2f\", $BACKUP_SIZE / 1024 / 1024}")

            echo "Γ£ô Database backup completed"
            echo "  File: ${BACKUP_PREFIX}_${TIMESTAMP}.sql.gz"
            echo "  Size: ${BACKUP_SIZE_MB} MB (compressed)"

            # Test backup integrity
            echo "Verifying backup integrity..."
            if gunzip -t "$BACKUP_DIR/${BACKUP_PREFIX}_${TIMESTAMP}.sql.gz" 2>/dev/null; then
              echo "Γ£ô Backup file integrity verified"
            else
              echo "ERROR: Backup file is corrupted!"
              exit 1
            fi

            # Create backup manifest
            echo "Creating backup manifest..."
            cat > "$BACKUP_DIR/backup_manifest.txt" << EOF
            ========================================
            Matomo Analytics Backup Manifest
            ========================================
            Backup Date: $(date -Iseconds)
            Backup Timestamp: ${TIMESTAMP}
            Kubernetes Namespace: matomo
            Helm Release: matomo
            Chart Version: 2.0.6
            Matomo Version: 5.5.1-apache

            Database Information:
            ====================
            Host: $MARIADB_HOST
            Port: $MARIADB_PORT
            Database: $MARIADB_DATABASE
            User: $MARIADB_USER

            Backup Files:
            =============
            ${BACKUP_PREFIX}_${TIMESTAMP}.sql.gz (${BACKUP_SIZE_MB} MB compressed)

            Backup Details:
            ===============
            Compression: gzip level 9
            Backup Type: Full database dump
            Transaction Consistent: Yes
            Includes: Tables, Routines, Triggers, Events

            Retention Policy: ${RETENTION_DAYS} days

            Restoration Command:
            ====================
            gunzip < ${BACKUP_PREFIX}_${TIMESTAMP}.sql.gz | \\
              mysql -h \$MARIADB_HOST -u \$MARIADB_USER -p \$MARIADB_DATABASE

            Notes:
            ======
            - This backup contains all Matomo analytics data
            - Includes visitor data, reports, configurations, and user accounts
            - Test restoration regularly to ensure backup integrity
            - Store backups securely with appropriate access controls
            EOF

            # Calculate statistics
            echo "Calculating backup statistics..."
            UNCOMPRESSED_SIZE=$(gunzip -c "$BACKUP_DIR/${BACKUP_PREFIX}_${TIMESTAMP}.sql.gz" | wc -c)
            UNCOMPRESSED_SIZE_MB=$(awk "BEGIN {printf \"%.2f\", $UNCOMPRESSED_SIZE / 1024 / 1024}")
            COMPRESSION_RATIO=$(awk "BEGIN {printf \"%.1f\", ($UNCOMPRESSED_SIZE - $BACKUP_SIZE) * 100 / $UNCOMPRESSED_SIZE}")

            echo "" >> "$BACKUP_DIR/backup_manifest.txt"
            echo "Compression Statistics:" >> "$BACKUP_DIR/backup_manifest.txt"
            echo "=====================" >> "$BACKUP_DIR/backup_manifest.txt"
            echo "Uncompressed: ${UNCOMPRESSED_SIZE_MB} MB" >> "$BACKUP_DIR/backup_manifest.txt"
            echo "Compressed: ${BACKUP_SIZE_MB} MB" >> "$BACKUP_DIR/backup_manifest.txt"
            echo "Compression Ratio: ${COMPRESSION_RATIO}%" >> "$BACKUP_DIR/backup_manifest.txt"

            # Cleanup old backups
            echo ""
            echo "Cleaning up backups older than ${RETENTION_DAYS} days..."
            BEFORE_COUNT=$(find /backup -maxdepth 1 -type d -name "[0-9]*" | wc -l)
            find /backup -maxdepth 1 -type d -name "[0-9]*" -mtime +${RETENTION_DAYS} -exec rm -rf {} \; 2>/dev/null || true
            AFTER_COUNT=$(find /backup -maxdepth 1 -type d -name "[0-9]*" | wc -l)
            DELETED=$((BEFORE_COUNT - AFTER_COUNT))

            if [ $DELETED -gt 0 ]; then
              echo "Γ£ô Deleted $DELETED old backup(s)"
            fi

            # Final status
            echo ""
            echo "========================================="
            echo "Γ£à Matomo Backup Completed Successfully!"
            echo "========================================="
            echo "Backup location: $BACKUP_DIR"
            echo "Compressed size: ${BACKUP_SIZE_MB} MB"
            echo "Uncompressed size: ${UNCOMPRESSED_SIZE_MB} MB"
            echo "Space saved: ${COMPRESSION_RATIO}%"
            echo "Total backups retained: $AFTER_COUNT"
            echo "Completed at: $(date)"
            echo "========================================="

            # Log to backup log file
            echo "[$(date -Iseconds)] Backup completed: $BACKUP_DIR (${BACKUP_SIZE_MB}MB compressed, ${UNCOMPRESSED_SIZE_MB}MB uncompressed)" >> /backup/backup.log
          env:
          - name: MARIADB_USERNAME
            valueFrom:
              secretKeyRef:
                key: mariadb-username
                name: matomo-mariadb
          - name: MARIADB_PASSWORD
            valueFrom:
              secretKeyRef:
                key: mariadb-password
                name: matomo-mariadb
          image: mariadb:11.2
          imagePullPolicy: IfNotPresent
          name: matomo-backup
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /backup
            name: backup-storage
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: matomo-backup
        serviceAccountName: matomo-backup
        terminationGracePeriodSeconds: 30
        volumes:
        - name: backup-storage
          persistentVolumeClaim:
            claimName: matomo-backup
        - emptyDir:
            sizeLimit: 1Gi
          name: tmp-dir
  status:
    conditions:
    - lastProbeTime: "2026-02-18T03:00:28Z"
      lastTransitionTime: "2026-02-18T03:00:28Z"
      message: Job has reached the specified backoff limit
      reason: BackoffLimitExceeded
      status: "True"
      type: FailureTarget
    - lastProbeTime: "2026-02-18T03:00:29Z"
      lastTransitionTime: "2026-02-18T03:00:29Z"
      message: Job has reached the specified backoff limit
      reason: BackoffLimitExceeded
      status: "True"
      type: Failed
    failed: 1
    ready: 0
    startTime: "2026-02-18T03:00:00Z"
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2026-02-16T02:00:00Z"
    creationTimestamp: "2026-02-16T02:00:00Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: n8n
      app.kubernetes.io/name: n8n
      batch.kubernetes.io/controller-uid: 2ebb48a1-13ca-41ff-842a-8ea9dc7e4d9c
      batch.kubernetes.io/job-name: n8n-backup-29520120
      component: backup
      controller-uid: 2ebb48a1-13ca-41ff-842a-8ea9dc7e4d9c
      job-name: n8n-backup-29520120
    name: n8n-backup-29520120
    namespace: n8n
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: n8n-backup
      uid: 65eefee7-8e59-4b07-9f08-f32ac3706015
    resourceVersion: "18726446"
    uid: 2ebb48a1-13ca-41ff-842a-8ea9dc7e4d9c
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 2ebb48a1-13ca-41ff-842a-8ea9dc7e4d9c
    suspend: false
    template:
      metadata:
        labels:
          app.kubernetes.io/instance: n8n
          app.kubernetes.io/name: n8n
          batch.kubernetes.io/controller-uid: 2ebb48a1-13ca-41ff-842a-8ea9dc7e4d9c
          batch.kubernetes.io/job-name: n8n-backup-29520120
          component: backup
          controller-uid: 2ebb48a1-13ca-41ff-842a-8ea9dc7e4d9c
          job-name: n8n-backup-29520120
      spec:
        affinity:
          podAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - n8n
                - key: component
                  operator: In
                  values:
                  - main
              topologyKey: kubernetes.io/hostname
        containers:
        - command:
          - /bin/sh
          - -c
          - |
            echo "Starting n8n backup at $(date)"

            # Create backup directory
            mkdir -p /backup/$(date +%Y%m%d_%H%M%S)

            # Copy n8n data
            if [ -d "/data" ]; then
              cp -r /data/* /backup/$(date +%Y%m%d_%H%M%S)/ || true
              echo "Backup completed successfully at $(date)"
            else
              echo "No data directory found"
            fi

            # Cleanup old backups (keep last 7 days)
            find /backup -type d -name "20*" -mtime +7 -exec rm -rf {} + || true

            echo "Backup job finished at $(date)"
          image: busybox:1.36
          imagePullPolicy: IfNotPresent
          name: backup
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 10m
              memory: 32Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: n8n-data
          - mountPath: /backup
            name: backup-storage
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        terminationGracePeriodSeconds: 30
        volumes:
        - name: n8n-data
          persistentVolumeClaim:
            claimName: n8n-data
        - name: backup-storage
          persistentVolumeClaim:
            claimName: n8n-backup
  status:
    completionTime: "2026-02-16T02:00:16Z"
    conditions:
    - lastProbeTime: "2026-02-16T02:00:16Z"
      lastTransitionTime: "2026-02-16T02:00:16Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2026-02-16T02:00:16Z"
      lastTransitionTime: "2026-02-16T02:00:16Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2026-02-16T02:00:00Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2026-02-17T02:00:00Z"
    creationTimestamp: "2026-02-17T02:00:00Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: n8n
      app.kubernetes.io/name: n8n
      batch.kubernetes.io/controller-uid: 2edbd3f4-fe8c-4dfe-8837-335465ae1c71
      batch.kubernetes.io/job-name: n8n-backup-29521560
      component: backup
      controller-uid: 2edbd3f4-fe8c-4dfe-8837-335465ae1c71
      job-name: n8n-backup-29521560
    name: n8n-backup-29521560
    namespace: n8n
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: n8n-backup
      uid: 65eefee7-8e59-4b07-9f08-f32ac3706015
    resourceVersion: "19040073"
    uid: 2edbd3f4-fe8c-4dfe-8837-335465ae1c71
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 2edbd3f4-fe8c-4dfe-8837-335465ae1c71
    suspend: false
    template:
      metadata:
        labels:
          app.kubernetes.io/instance: n8n
          app.kubernetes.io/name: n8n
          batch.kubernetes.io/controller-uid: 2edbd3f4-fe8c-4dfe-8837-335465ae1c71
          batch.kubernetes.io/job-name: n8n-backup-29521560
          component: backup
          controller-uid: 2edbd3f4-fe8c-4dfe-8837-335465ae1c71
          job-name: n8n-backup-29521560
      spec:
        affinity:
          podAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - n8n
                - key: component
                  operator: In
                  values:
                  - main
              topologyKey: kubernetes.io/hostname
        containers:
        - command:
          - /bin/sh
          - -c
          - |
            echo "Starting n8n backup at $(date)"

            # Create backup directory
            mkdir -p /backup/$(date +%Y%m%d_%H%M%S)

            # Copy n8n data
            if [ -d "/data" ]; then
              cp -r /data/* /backup/$(date +%Y%m%d_%H%M%S)/ || true
              echo "Backup completed successfully at $(date)"
            else
              echo "No data directory found"
            fi

            # Cleanup old backups (keep last 7 days)
            find /backup -type d -name "20*" -mtime +7 -exec rm -rf {} + || true

            echo "Backup job finished at $(date)"
          image: busybox:1.36
          imagePullPolicy: IfNotPresent
          name: backup
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 10m
              memory: 32Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: n8n-data
          - mountPath: /backup
            name: backup-storage
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        terminationGracePeriodSeconds: 30
        volumes:
        - name: n8n-data
          persistentVolumeClaim:
            claimName: n8n-data
        - name: backup-storage
          persistentVolumeClaim:
            claimName: n8n-backup
  status:
    completionTime: "2026-02-17T02:00:15Z"
    conditions:
    - lastProbeTime: "2026-02-17T02:00:15Z"
      lastTransitionTime: "2026-02-17T02:00:15Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2026-02-17T02:00:15Z"
      lastTransitionTime: "2026-02-17T02:00:15Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2026-02-17T02:00:00Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2026-02-18T02:00:00Z"
    creationTimestamp: "2026-02-18T02:00:00Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: n8n
      app.kubernetes.io/name: n8n
      batch.kubernetes.io/controller-uid: c490d15f-3577-4b81-a38e-0ed21c99d103
      batch.kubernetes.io/job-name: n8n-backup-29523000
      component: backup
      controller-uid: c490d15f-3577-4b81-a38e-0ed21c99d103
      job-name: n8n-backup-29523000
    name: n8n-backup-29523000
    namespace: n8n
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: n8n-backup
      uid: 65eefee7-8e59-4b07-9f08-f32ac3706015
    resourceVersion: "19353390"
    uid: c490d15f-3577-4b81-a38e-0ed21c99d103
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: c490d15f-3577-4b81-a38e-0ed21c99d103
    suspend: false
    template:
      metadata:
        labels:
          app.kubernetes.io/instance: n8n
          app.kubernetes.io/name: n8n
          batch.kubernetes.io/controller-uid: c490d15f-3577-4b81-a38e-0ed21c99d103
          batch.kubernetes.io/job-name: n8n-backup-29523000
          component: backup
          controller-uid: c490d15f-3577-4b81-a38e-0ed21c99d103
          job-name: n8n-backup-29523000
      spec:
        affinity:
          podAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - n8n
                - key: component
                  operator: In
                  values:
                  - main
              topologyKey: kubernetes.io/hostname
        containers:
        - command:
          - /bin/sh
          - -c
          - |
            echo "Starting n8n backup at $(date)"

            # Create backup directory
            mkdir -p /backup/$(date +%Y%m%d_%H%M%S)

            # Copy n8n data
            if [ -d "/data" ]; then
              cp -r /data/* /backup/$(date +%Y%m%d_%H%M%S)/ || true
              echo "Backup completed successfully at $(date)"
            else
              echo "No data directory found"
            fi

            # Cleanup old backups (keep last 7 days)
            find /backup -type d -name "20*" -mtime +7 -exec rm -rf {} + || true

            echo "Backup job finished at $(date)"
          image: busybox:1.36
          imagePullPolicy: IfNotPresent
          name: backup
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 10m
              memory: 32Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: n8n-data
          - mountPath: /backup
            name: backup-storage
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        terminationGracePeriodSeconds: 30
        volumes:
        - name: n8n-data
          persistentVolumeClaim:
            claimName: n8n-data
        - name: backup-storage
          persistentVolumeClaim:
            claimName: n8n-backup
  status:
    completionTime: "2026-02-18T02:00:13Z"
    conditions:
    - lastProbeTime: "2026-02-18T02:00:13Z"
      lastTransitionTime: "2026-02-18T02:00:13Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2026-02-18T02:00:13Z"
      lastTransitionTime: "2026-02-18T02:00:13Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2026-02-18T02:00:00Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2026-02-18T02:00:00Z"
    creationTimestamp: "2026-02-18T02:00:00Z"
    generation: 1
    labels:
      app.kubernetes.io/component: backup
      app.kubernetes.io/instance: wordpress
      app.kubernetes.io/name: wordpress
      batch.kubernetes.io/controller-uid: 12a7d590-9ec9-49a0-b978-e8e669ce02cc
      batch.kubernetes.io/job-name: wordpress-backup-29523000
      controller-uid: 12a7d590-9ec9-49a0-b978-e8e669ce02cc
      job-name: wordpress-backup-29523000
    name: wordpress-backup-29523000
    namespace: wordpress
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: wordpress-backup
      uid: 663a4908-4b44-4f9b-a2cd-e5c90505cc3d
    resourceVersion: "19353527"
    uid: 12a7d590-9ec9-49a0-b978-e8e669ce02cc
  spec:
    activeDeadlineSeconds: 3600
    backoffLimit: 2
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 12a7d590-9ec9-49a0-b978-e8e669ce02cc
    suspend: false
    template:
      metadata:
        labels:
          app.kubernetes.io/component: backup
          app.kubernetes.io/instance: wordpress
          app.kubernetes.io/name: wordpress
          batch.kubernetes.io/controller-uid: 12a7d590-9ec9-49a0-b978-e8e669ce02cc
          batch.kubernetes.io/job-name: wordpress-backup-29523000
          controller-uid: 12a7d590-9ec9-49a0-b978-e8e669ce02cc
          job-name: wordpress-backup-29523000
      spec:
        containers:
        - command:
          - /bin/bash
          - -c
          - |
            set -euo pipefail

            # Source backup configuration
            source /etc/backup/backup-config.sh

            echo "Starting WordPress backup at $(date)"

            # Create backup directory
            mkdir -p "$BACKUP_DIR"

            # Database backup with compression and timeout
            echo "Backing up database..."
            timeout 600 mariadb-dump \
              --host="$MARIADB_HOST" \
              --user="$MARIADB_USER" \
              --password="$MARIADB_PASSWORD" \
              --single-transaction \
              --routines \
              --triggers \
              --lock-tables=false \
              --add-drop-table \
              --extended-insert \
              --quick \
              --compress \
              "$MARIADB_DB" | gzip > "$BACKUP_DIR/${BACKUP_PREFIX}_db_${TIMESTAMP}.sql.gz"

            # WordPress files backup - Database only for now to avoid volume conflicts
            echo "Backing up WordPress database only (files on persistent volume)"

            # Create backup manifest
            echo "Creating backup manifest..."
            cat > "$BACKUP_DIR/${BACKUP_PREFIX}_manifest_${TIMESTAMP}.txt" << EOF
            WordPress Backup Manifest
            Backup Date: $(date -Iseconds)
            Kubernetes Namespace: wordpress
            WordPress Version: 6.8.3-php8.3-apache
            Database Host: $MARIADB_HOST
            Database Name: $MARIADB_DB

            Files:
            - ${BACKUP_PREFIX}_db_${TIMESTAMP}.sql.gz ($(stat -f%z "$BACKUP_DIR/${BACKUP_PREFIX}_db_${TIMESTAMP}.sql.gz" 2>/dev/null || stat -c%s "$BACKUP_DIR/${BACKUP_PREFIX}_db_${TIMESTAMP}.sql.gz") bytes)

            Note: WordPress files are stored on persistent volumes and automatically backed up by DigitalOcean.
            EOF

            # Cleanup old backups
            echo "Cleaning up old backups..."
            find "$BACKUP_DIR" -name "${BACKUP_PREFIX}_*" -mtime +$RETENTION_DAYS -delete

            # Log completion
            echo "Backup completed successfully at $(date)" | tee -a "$LOG_FILE"
            echo "Backup files created:"
            ls -lah "$BACKUP_DIR/${BACKUP_PREFIX}_"*"_${TIMESTAMP}."*
          env:
          - name: MARIADB_HOST
            value: wordpress-mariadb
          - name: MARIADB_USER
            value: wordpress
          - name: MARIADB_PASSWORD
            valueFrom:
              secretKeyRef:
                key: mariadb-password
                name: wordpress
          - name: MARIADB_DB
            value: wordpress
          image: mariadb:11.1
          imagePullPolicy: IfNotPresent
          name: wordpress-backup
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/backups/wordpress
            name: backup-storage
          - mountPath: /etc/backup
            name: backup-config
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: wordpress
        serviceAccountName: wordpress
        terminationGracePeriodSeconds: 30
        volumes:
        - name: backup-storage
          persistentVolumeClaim:
            claimName: wordpress-backup
        - configMap:
            defaultMode: 420
            items:
            - key: backup-config.sh
              mode: 493
              path: backup-config.sh
            name: wordpress-config
          name: backup-config
        - emptyDir:
            sizeLimit: 1Gi
          name: tmp-dir
  status:
    conditions:
    - lastProbeTime: "2026-02-18T02:00:31Z"
      lastTransitionTime: "2026-02-18T02:00:31Z"
      message: Job has reached the specified backoff limit
      reason: BackoffLimitExceeded
      status: "True"
      type: FailureTarget
    - lastProbeTime: "2026-02-18T02:00:32Z"
      lastTransitionTime: "2026-02-18T02:00:32Z"
      message: Job has reached the specified backoff limit
      reason: BackoffLimitExceeded
      status: "True"
      type: Failed
    failed: 1
    ready: 0
    startTime: "2026-02-18T02:00:00Z"
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2026-02-18T21:45:00Z"
    creationTimestamp: "2026-02-18T21:45:00Z"
    generation: 1
    labels:
      app.kubernetes.io/component: cron
      app.kubernetes.io/instance: wordpress
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: wordpress
      app.kubernetes.io/part-of: wordpress-platform
      app.kubernetes.io/version: 6.8.3
      batch.kubernetes.io/controller-uid: 1ecfa7ec-d602-41bc-94f7-fbc32a12d4b2
      batch.kubernetes.io/job-name: wordpress-cron-29524185
      controller-uid: 1ecfa7ec-d602-41bc-94f7-fbc32a12d4b2
      helm.sh/chart: wordpress-3.2.6
      job-name: wordpress-cron-29524185
    name: wordpress-cron-29524185
    namespace: wordpress
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: wordpress-cron
      uid: 90f565a3-7554-4244-a78f-ff53ef3ae0e3
    resourceVersion: "19611157"
    uid: 1ecfa7ec-d602-41bc-94f7-fbc32a12d4b2
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 1ecfa7ec-d602-41bc-94f7-fbc32a12d4b2
    suspend: false
    template:
      metadata:
        labels:
          app.kubernetes.io/component: cron
          app.kubernetes.io/instance: wordpress
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: wordpress
          app.kubernetes.io/part-of: wordpress-platform
          app.kubernetes.io/version: 6.8.3
          batch.kubernetes.io/controller-uid: 1ecfa7ec-d602-41bc-94f7-fbc32a12d4b2
          batch.kubernetes.io/job-name: wordpress-cron-29524185
          controller-uid: 1ecfa7ec-d602-41bc-94f7-fbc32a12d4b2
          helm.sh/chart: wordpress-3.2.6
          job-name: wordpress-cron-29524185
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - |
            echo "Running WordPress cron at $(date)"
            curl -f -s -m 30 "http://wordpress/wp-cron.php?doing_wp_cron" || echo "Cron request failed"
            echo "WordPress cron completed at $(date)"
          image: curlimages/curl:latest
          imagePullPolicy: Always
          name: wordpress-cron
          resources:
            limits:
              cpu: 50m
              memory: 32Mi
            requests:
              cpu: 10m
              memory: 16Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
  status:
    completionTime: "2026-02-18T21:45:06Z"
    conditions:
    - lastProbeTime: "2026-02-18T21:45:06Z"
      lastTransitionTime: "2026-02-18T21:45:06Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2026-02-18T21:45:06Z"
      lastTransitionTime: "2026-02-18T21:45:06Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2026-02-18T21:45:00Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
kind: List
metadata:
  resourceVersion: ""
